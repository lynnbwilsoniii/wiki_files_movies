\documentclass[letterpaper,fleqn,10pt]{article}

%%
%%  Graphics packages
%%
\pdfoutput=1
\usepackage[dvips,pdftex]{graphicx,color}  %%  allow colored text
%\usepackage[dvips,pdftex]{graphicx,color}  %%  allow colored text
\usepackage{floatrow}
\usepackage{wrapfig}
\usepackage[labelfont=bf,justification=raggedright,skip=1pt]{caption}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{array}
\usepackage{epstopdf}  %%  convert EPS --> PDF
\usepackage{float}
%%
%%  Math/Symbol packages
%%
\usepackage{amsmath}   %%  contains advanced math extensions
\usepackage{latexsym}  %%  adds other symbols in to be used in math mode
\usepackage{amssymb}   %%  adds new symbols in to be used in math mode
\usepackage{mathtools} %%  allows the use of the dcases environment
\allowdisplaybreaks
%%
%%  Bibliography packages
%%
%%  \citep{} --> [Wilson et al., YYYY]
%%  \citet{} --> Wilson et al., [YYYY]
\usepackage[square,authoryear,compress]{natbib}
%%
%%  Color packages
%%
\usepackage{color}
\usepackage[rgb,dvipsnames]{xcolor}
%%  Define the color grey
\definecolor{grey}{gray}{0.5}
%%
%%  Referencing packages
%%
\usepackage{hyperref}     %%  Allow internal and external references
\usepackage{lastpage}     %%  Allow user to know total # of pages (e.g. page 5 of 9)
%%  Setup hyper references
%%    colorlinks, urlcolor, citecolor, and linkcolor --> define appearance of links
%%    the rest --> define options for the PDF viewer when opened
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black, linkcolor=black, bookmarksopen=true, bookmarks=true, pdftoolbar=true, pdfmenubar=true, bookmarksopenlevel=\maxdimen, pdftex, bookmarksnumbered=true, bookmarkstype=toc}
%%
%%  Page Formatting packages
%%
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage[compact]{titlesec}
\usepackage[paperwidth=8.5in, paperheight=11.0in, top=1.00in, bottom=1.00in, left=1.00in, right=1.00in, textwidth=6.50in, textheight=9.0in, bindingoffset=0pt, showframe=false, includehead=false, includefoot=false, ignoreheadfoot=true, layoutsize={8.5in,11.0in}, layoutoffset={0pt,0pt}, hcentering=true]{geometry}
\usepackage{setspace}
\linespread{1.00}         %%  Set line spacing to single space
\usepackage{indentfirst}  %%  Force first paragraph to indent
\usepackage{fancyhdr}     %%  Allow the use of custom headers and footers
%%
%%  Extra Formatting packages
%%
\usepackage{tocloft}      %%  Allow the user to format the Table of Contents
\usepackage{lastpage}     %%  Allow user to know total # of pages (e.g. page 5 of 9)
%%  Change verbatim environment colors
\usepackage{newverbs}
\renewenvironment{verbatim}
{\semiverbatim\color{blue}}
{\endsemiverbatim}
\renewcommand{\verb}{\collectverb{\color{blue}}}
%%########################################################################################
%%  Length Formatting Settings
%%########################################################################################
%%  -> Define the margin sizes
\setlength{\topmargin}{-30pt}
\setlength{\marginparsep}{0pt}
\setlength{\marginparwidth}{0pt}
%%  -> Define the vertical offset of page
\setlength{\headheight}{18pt}
\setlength{\headsep}{10pt}
%%  Adjust vertical offset of center of top margin
\setlength{\voffset}{0pt}
%%  Adjust horizontal offset of center of left margin
\setlength{\hoffset}{0pt}
%%  -> Define the footnote and header sizes
%%  Adjust spacing between the baselines of the last line of text and the footnote
\setlength{\footskip}{20pt}
%%  Adjust spacing between the baseline of the last line of text and the top of footnote
%%    = footnotesep option in geometry package
\setlength{\skip\footins}{10pt}
%%  -> Define horizontal width of header&footer
\setlength{\headwidth}{1.00\textwidth}
%%  Space above and below captions -> 0
\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
%%  Adjust spacing between last top float or first bottom float and the text
\setlength{\textfloatsep}{0pt plus 2pt minus 2pt}
%%  Adjust spacing before/after an in-text float
\setlength{\intextsep}{0pt plus 2pt minus 2pt}
%%  Adjust spacing before/after each item in enumerate
\setlength{\itemsep}{0pt}
%%  Adjust spacing before/after a paragraph in each item in enumerate
\setlength{\parsep}{0pt}
%%  Adjust spacing before/after a list and paragraph in text
\setlength{\parskip}{0pt plus 2pt minus 2pt}
%%  Adjust spacing before an enumerate environment
\setlength{\topsep}{0pt}
%%  Adjust spacing between columns in a table [default = 6pt]
\setlength{\tabcolsep}{3pt}
%%  Adjust spacing for paragraph indentation
\setlength{\parindent}{15pt}
%%  -> Define width of line before equations
\setlength{\predisplaysize}{\textwidth}
%%  Adjust horizontal spacing before each item in enumerate
\setlist{leftmargin=10mm}
%%########################################################################################
%%  Table of Contents (ToC) Formatting Settings
%%########################################################################################
%%  Adjust space between lines in TOC
\setlength{\cftbeforesecskip}{-1pt}         %%  Section spacing
\setlength{\cftbeforesubsecskip}{-2pt}      %%  Subsection
\setlength{\cftbeforesubsubsecskip}{-2pt}   %%  Subsubsection
%%  Adjust font size in ToC
\renewcommand{\cftsecfont}{\normalsize\bfseries}
\renewcommand{\cftsubsecfont}{\small}
\renewcommand{\cftsubsubsecfont}{\footnotesize}
%%  Define # of levels labeled in TOCs
%%    1  =  show down to Sections
%%    2  =  show down to Subsections
%%    3  =  show down to Subsubsections
\setcounter{tocdepth}{3}
%%  Define new ToC section name
\renewcommand{\cftmarktoc}{{\bf \Large Table of Contents}}
%%  Define new List of Tables section name
\renewcommand{\cftmarklot}{{\bf \Large List of Tables}}
%%  Define new List of Figures section name
\renewcommand{\cftmarklof}{{\bf \Large List of Figures}}
%%########################################################################################
%%  Section Formatting options
%%########################################################################################
%%  \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before>}[<after>]
%%     (1 em = N points, where N = font size
%%       [e.g., 1 em = 10 pt for 10 pt font ])
\titleformat{\section}[hang]{\Large\bfseries}{\thesection}{10pt}{}  %% horizontal spacing
\titleformat{\subsection}[hang]{\large\bfseries}{\thesubsection}{7pt}{}
\titleformat{\subsubsection}[hang]{\normalsize\bfseries}{\thesubsubsection}{5pt}{}
%%########################################################################################
%%  -> Define some new commands
%%########################################################################################
%%  Define a date command
\newcommand{\thedate}{\today}
%%  Define my name
\newcommand{\myname}{Lynn B. Wilson III}
%%  -> Define the full title
\newcommand{\fulltitle}{General Math Tricks and Other Notes}
%%  -> Define the short title
\newcommand{\shorttitle}{Math and Notes}
%%  -> Define right-header as the section name (actual section name)
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}
\newcommand{\hdrnoter}{{\it \leftmark}}
%%  -> Define the footer notes
\newcommand{\ftrnotel}{\textcolor{grey}{lynn.b.wilsoniii@gmail.com} \\ \textcolor{grey}{lynn.b.wilson@nasa.gov}}
\newcommand{\ftrnotec}{\textcolor{grey}{\myname}}
\newcommand{\ftrnoter}{\thepage\ of \pageref{LastPage}}
%%  -> Define line thickness for header/footer lines
\renewcommand{\headrulewidth}{0.5mm}  %%  set to 0mm if no line desired
\renewcommand{\footrulewidth}{0.5mm}
%%  -> Change header/footer line color to teal
%%     Header Line
\renewcommand{\headrule}{{\color{teal}\hrule width\headwidth height\headrulewidth \vskip-\headrulewidth}}
%%     Footer Line
\renewcommand{\footrule}{{\color{teal}\vskip-\footruleskip\vskip-\footrulewidth
\hrule width\headwidth height\footrulewidth\vskip\footruleskip}}
%%########################################################################################
%%  Headers and Footers
%%########################################################################################
%%  -> Set the page style to use these custom headers/footers
\pagestyle{fancy}
%%  -> Define the headers [Left, Center, Right]
\fancyhead[L]{ \shorttitle }
\chead[]{}
\fancyhead[R]{ \hdrnoter }
%%  -> Define the footers [Left, Center, Right]
\fancyfoot[L]{ \ftrnotel }
\fancyfoot[C]{ \ftrnotec }
\fancyfoot[R]{ \ftrnoter }
%%########################################################################################
%%  Change Table and Figure Labels
%%########################################################################################
%%  -> Force figure labels to be alphanumeric
\renewcommand{\thefigure}{\Alph{figure}}
%%  Change the separator between Figure and the counters to :
\DeclareCaptionLabelSeparator{vline}{: }
%%########################################################################################
%%  Footnotes
%%########################################################################################
%%  Change footnote marks from symbol to numbers
\makeatletter  %%  Allow the use of @-symbol in the followin TeX definition
\let\@fnsymbol\@arabic
\makeatother   %%  Shut-off @-symbol functionality
%%  Change footnote marks
\renewcommand{\thefootnote}{\arabic{footnote}}
%%
%%  Reference Footnotes
%%
%%\usepackage{fmtcount}  %%  Allow user to keep track of counters/labels
%%\renewcommand{\fmtord}[1]{}  %%  remove st, nd, rd, th, etc. from ordinal suffixes
%%########################################################################################
%%  Math formatting options
%%########################################################################################
%%  -> Define horizontal spacing for equations
\setlength{\mathindent}{15pt}
%%########################################################################################
%%%%######################################################################################
%%%%%%####################################################################################
%%%%%%%%  Begin the document
%%%%%%####################################################################################
%%%%######################################################################################
%%########################################################################################
\begin{document}
%%  Define style of bibliography
\bibliographystyle{agufull08}  %%  agu08.bst is distributed by AGU
%%
%%       \titlespacing{ command }{ left }{ before }{ after }[ right ]
%%         before = X plus Y minus Z
%%           => This means that:
%%                X  =  value we would like
%%                Y  =  max value TeX can increase X
%%                Z  =  max value TeX can decrease X
%%
%%  -> Define spacing for floats
\titlespacing{\enumerate}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\figure}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\table}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\threeparttable}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\tablenotes}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\wrapfigure}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
%%  -> Define spacing for sections
\titlespacing{\section}{0pt}{3pt plus 2pt minus 2pt}{3pt plus 2pt minus 2pt}
\titlespacing{\subsection}{0pt}{2pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
\titlespacing{\subsubsection}{0pt}{1pt plus 2pt minus 2pt}{1pt plus 2pt minus 2pt}
%%
%%  Format Table/Figure captions
%%
%%  \captionsetup[figure]{figurename=Figure,skip=1pt,font=footnotesize,position=below,justification=RaggedRight,labelsep=vline}
%%  \captionsetup[wrapfigure]{figurename=Figure,labelfont=bf,skip=1pt,font=footnotesize,justification=RaggedRight,labelsep=vline}
%%----------------------------------------------------------------------------------------
%%  Title Page
%%----------------------------------------------------------------------------------------
%%  Page Format and Numbering
\pagestyle{plain}
\pagenumbering{arabic}
%%  Define Title
\title{\bf \fulltitle}
%%  Define Author
\author{\myname}
%%  Define current date
\date{\today}
\maketitle  %%  necessary to make LaTeX print title

%%  Shut off page numbering here
\thispagestyle{empty}
\clearpage
%%----------------------------------------------------------------------------------------
%%  Auto-generated Table of Contents (ToC)
%%----------------------------------------------------------------------------------------
%%  Shut off default ToC section name
\renewcommand{\contentsname}{}
\tableofcontents  %%  Output ToC

%%  Shut off page numbering here
\thispagestyle{empty}
\clearpage


%%  -> Set the page style to use these custom headers/footers
\pagestyle{fancy}
\pagenumbering{arabic}  %%  start page numbering here
\setcounter{page}{1}    %%  set page number to 1
%%  -> Set the equation numbers to number by section
\numberwithin{equation}{section}
%%----------------------------------------------------------------------------------------
%%  Section: Conversion Factors
%%----------------------------------------------------------------------------------------
\section{Conversion Factors}
\indent  The following are useful conversion factors:
\begin{subequations}
  \begin{align}
    1 \frac{ km \thickspace T }{ s } & = 10^{-3} \thickspace \frac{ mV }{ m } = 1 \frac{ \mu V }{ m } \\
    1 \frac{ eV }{ cm^{3} } & = 1.60217646 \times 10^{-4} \thickspace \frac{ J }{ km^{3} } \\
    1 \frac{ \mu W }{ m^{2} } & = 6.24150974 \times 10^{0} \thickspace \frac{ keV \thickspace km }{ s \thickspace cm^{3} } \\
    1 \frac{ keV \thickspace km }{ s \thickspace cm^{3} } & = 1.60217646 \times 10^{-7} \thickspace \frac{ erg }{ s \thickspace cm^{2} } \\
    1 \frac{ erg }{ s \thickspace cm^{2} } & = 10^{3} \thickspace \frac{ \mu W }{ m^{2} } \\
    1 \frac{ s^{3} }{ km^{3} \thickspace cm^{3} } & = 10^{-3} \thickspace \frac{ s^{3} }{ m^{6} } \\
    1 \frac{ \mu V }{ cm } & = 10^{-1} \thickspace \frac{ mV }{ m }
  \end{align}
\end{subequations}

\indent  Let us define q${\scriptstyle_{s}}$ $=$ Z e [$\equiv$ charge of particle species $s$], Z $\equiv$ number of unit charges, m${\scriptstyle_{e}}$(M${\scriptstyle_{s}}$) $\equiv$ mass of electron(ion species $s$), $\mu$ $\equiv$ M${\scriptstyle_{i}}$/M${\scriptstyle_{p}}$, n${\scriptstyle_{s}}$ $\equiv$ number density of particle species $s$, T${\scriptstyle_{s}}$ $\equiv$ average temperature of particle species $s$, and B${\scriptstyle_{o}}$ $\equiv$ magnitude of the quasi-static magnetic field.  In addition, let us define $\omega{\scriptstyle_{ps}}$ $=$ 2$\pi$f${\scriptstyle_{ps}}$ $=$ $\sqrt{ n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}/(m{\scriptstyle_{s}} \varepsilon{\scriptstyle_{o}}) }$ [$\equiv$ plasma frequency of particle species $s$], $\Omega{\scriptstyle_{cs}}$ $=$ 2$\pi$f${\scriptstyle_{cs}}$ $=$ q${\scriptstyle_{s}}$B${\scriptstyle_{o}}$/m${\scriptstyle_{s}}$ [$\equiv$ cyclotron frequency of particle species $s$], V${\scriptstyle_{Ts}}$ $=$ $\sqrt{ (2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})/m{\scriptstyle_{s}} }$ [$\equiv$ average thermal speed of particle species $s$], $\lambda{\scriptstyle_{s}}$ $=$ c/$\omega{\scriptstyle_{ps}}$ [$\equiv$ inertial length (or skin depth) of particle species $s$], $\lambda{\scriptstyle_{Ds}}$ $=$ $\sqrt{ (\varepsilon{\scriptstyle_{o}} k{\scriptstyle_{B}} T{\scriptstyle_{s}})/(n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}) }$ [$\equiv$ Debye length of particle species $s$], $\rho{\scriptstyle_{cs}}$ $=$ V${\scriptstyle_{Ts}}$/$\omega{\scriptstyle_{ps}}$ [$\equiv$ thermal gyroradius of particle species $s$], and V${\scriptstyle_{A}}$ $=$ $\sqrt{ B{\scriptstyle_{o}}^{2}/( \mu{\scriptstyle_{o}} M{\scriptstyle_{i}} n{\scriptstyle_{i}} ) }$ [$\equiv$ Alfv\'{e}n speed]\footnote{we also refer to an electron Alfv\'{e}n speed, V${\scriptstyle_{Ae}}$, on occasion, but it does not have the same physical significance as V${\scriptstyle_{A}}$}.

\indent  Below, the units are defined as follows:  all frequencies are in Hz; distances in meters; speeds in km/s; temperatures in eV; magnetic fields in nT; and densities in cm$^{-3}$.  The approximate factors are:

%%  plasma frequencies
\begin{equation}
  f{\scriptstyle_{pe}} \cong 8.9787 \times 10^{3} \thickspace \sqrt{ n{\scriptstyle_{e}} }
\end{equation}

\begin{equation}
  f{\scriptstyle_{pi}} \cong 209.5353 \thickspace \sqrt{ \frac{ Z^{2} n{\scriptstyle_{i}} }{ \mu } }
\end{equation}

%%  cyclotron frequencies
\begin{equation}
  f{\scriptstyle_{ce}} \cong 27.9925 \thickspace B{\scriptstyle_{o}}
\end{equation}

\begin{equation}
  f{\scriptstyle_{ci}} \cong 1.5245 \times 10^{-2} \thickspace \frac{ Z }{ \mu } B{\scriptstyle_{o}}
\end{equation}

%%  gyroradii
\begin{equation}
  \rho{\scriptstyle_{ce}} \cong 3.3721 \thickspace \frac{\sqrt{ T{\scriptstyle_{e}} }}{ B{\scriptstyle_{o}} }
\end{equation}

\begin{equation}
  \rho{\scriptstyle_{ci}} \cong 144.4970 \thickspace \frac{ \sqrt{ \mu T{\scriptstyle_{i}} } }{ Z B{\scriptstyle_{o}} }
\end{equation}

%%  inertial lengths
\begin{equation}
  \lambda{\scriptstyle_{e}} \cong 5.3141 \times 10^{3} \thickspace n{\scriptstyle_{e}}^{-1/2}
\end{equation}

\begin{equation}
  \lambda{\scriptstyle_{i}} \cong 2.2771 \times 10^{5} \thickspace \sqrt{ \frac{ \mu }{ Z^{2} n{\scriptstyle_{i}} } }
\end{equation}

%%  Debye lengths
\begin{equation}
  \tilde{\lambda}{\scriptstyle_{De}} = \frac{ V{\scriptstyle_{Te}} }{ \sqrt{2} \omega{\scriptstyle_{pe}} } \cong 7.4339 \thickspace \sqrt{ \frac{ T{\scriptstyle_{e}} }{ n{\scriptstyle_{e}} } }
\end{equation}

\begin{equation}
  \lambda{\scriptstyle_{Ds}} = \frac{ V{\scriptstyle_{Ts}} }{ \omega{\scriptstyle_{pe}} } \cong 10.5132 \thickspace \sqrt{ \frac{ T{\scriptstyle_{s}} }{ Z{\scriptstyle_{s}}^{2} n{\scriptstyle_{s}} } }
\end{equation}

%%  thermal speeds
\begin{equation}
  V{\scriptstyle_{Te}} \cong 593.0970 \thickspace \sqrt{ T{\scriptstyle_{e}} }
\end{equation}

\begin{equation}
  V{\scriptstyle_{Ti}} \cong 13.8411 \thickspace \sqrt{ \frac{ T{\scriptstyle_{i}} }{ \mu } }
\end{equation}

\begin{equation}
  \frac{\omega_{pe}}{\Omega_{ce}} \cong 3.21 \times 10^{2} \thickspace \frac{\sqrt{n_{e}(cm^{-3})}}{B(nT)}
\end{equation}

\begin{equation}
  \beta{\scriptstyle_{s}} \cong 0.403 \thickspace \frac{ n{\scriptstyle_{s}} T{\scriptstyle_{s}} }{ B{\scriptstyle_{o}}^{2} }
\end{equation}

\indent  The following are useful relationships:

\begin{subequations}
  \begin{align}
    \omega{\scriptstyle_{pe}} & = \left( \frac{ c }{ V{\scriptstyle_{Ae}} } \right) \Omega{\scriptstyle_{ce}}  \\
    \eta & = \frac{ \nu }{ \varepsilon{\scriptstyle_{o}} \omega{\scriptstyle_{pe}}^{2} }
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section:  General Mathematical Rules
%%----------------------------------------------------------------------------------------
\section{General Mathematical Rules}
%%----------------------------------------------------------------------------------------
%%  Subsection:  The Dirac Delta Function
%%----------------------------------------------------------------------------------------
\subsection{The Dirac Delta Function}  \label{subsec:diracdelta}
Definition = a mathematically \emph{improper} function having the properties :
\begin{enumerate}
  \item $\delta\bigl($x - a$\bigr)$ = 0 for x $\ne$ a
  \item $\smallint$ $\delta\bigl($x - a$\bigr)$ dx = 1 (\emph{if region includes x = a which we'll assume from here on, otherwise it is zero})
  \item $\smallint$ dx f(x) $\delta\bigl($x - a$\bigr)$ = f(a)
  \item $\smallint$ dx f(x) $\delta$'$\bigl($x - a$\bigr)$ = -f'(a)
  \item The delta function transforms according to the rule seen in Equation \ref{eq:dirac_delta1}, assuming f(x) only has simple zeros located at x = x$_{i}$.
  \item In more than one dimension, the delta function can be written as seen in Equation \ref{eq:dirac_delta2}
  \item The delta function has the inverse units of whatever the delta function happens to be a function of $\Rightarrow$ the delta function in Equation \ref{eq:dirac_delta2} has the units of an inverse volume
  \item One can expand a delta function in a Taylor series according to the rules defined in Equations (\ref{eq:delta_func_expan1} - \ref{eq:delta_func_expan4})
  \item Typically one assumes that $\nabla^{2}$(1/r) = 0, assuming r $\ne$ 0 and its volume integral is equal to -4$\pi$.  One can then use the properties of the delta function to say $\nabla^{2}$(1/r) = -4$\pi$ $\delta$(\textbf{x}).  A more general version can be seen in Equation \ref{eq:dirac_delta3}.
\end{enumerate}
\begin{equation}
  \label{eq:dirac_delta0}
    \delta\left(x' - x\right) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} dk e^{ik(x' - x)}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_a}
    \frac{1}{k}\delta\left(k - k'\right) =  \int_{0}^{\infty} d\rho \rho J_{\nu}\bigl(k \rho\bigr) J_{\nu}\bigl(k' \rho\bigr)
\end{equation}
where J$_{\nu}$ are \emph{Bessel Functions} and Re$\bigl\{\nu\bigr\}$ $>$ -1.
\begin{equation}
  \label{eq:dirac_delta0_1}
    \frac{d^{n} \delta\left(x' - x\right)}{dx^{n}} = \delta\left(x' - x\right) \frac{d^{n}}{dx^{n}}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_2}
    \delta\left(xa\right) = \frac{\delta\left(x\right)}{\lvert a \rvert}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_3}
    \delta ' \left(x' - x\right) = \frac{d}{dx} \delta\left(x' - x\right) = -\frac{d}{dx'} \delta\left(x' - x\right)
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_4}
    \delta\left(x' - x\right) = \frac{d}{dx} \Theta\left(x' - x\right)
\end{equation}
where $\Theta \bigl($x' - x$\bigr)$ is the \emph{Theta Function} which has the properties:
\begin{equation}
  \label{eq:dirac_delta0_5}
\Theta \bigl(x' - x\bigr) = \begin{cases}
  0 & \text{ if (x' - x) $<$ 0}, \\
  1 & \text{ if (x' - x) $>$ 0}.
\end{cases}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta1}
  \delta\bigl(f(x)\bigr) = \sum_{i} \frac{1}{\lvert \frac{df}{dx_{i}} \rvert} \delta\left(x - x_{i}\right) \text{ [x$_{i}$ are the zeros of f(x)]}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta2}
  \delta\left(\textbf{x} - \textbf{x}'\right) = \delta\left(x_{1} - x_{1}'\right) \delta\left(x_{2} - x_{2}'\right) \delta\left(x_{3} - x_{3}'\right) 
\end{equation}
\begin{equation}
  \label{eq:dirac_delta3}
  \nabla^{2} \Biggl(\frac{1}{\lvert \textbf{x} - \textbf{x}' \rvert}\Biggr) = -4\pi \delta \bigl(\textbf{x} - \textbf{x}'\bigr)
\end{equation}
\begin{subequations}
  \begin{align}
    \vec{r}_{i+\delta} & = \vec{r}_{i} + \vec{r}_{\delta i} \label{eq:delta_func_expan1} \\
    \frac{\lvert \vec{r}_{\delta i} \rvert}{\lvert \vec{r}_{i + \delta} \rvert} & \ll 1 \label{eq:delta_func_expan2} \\
    \delta\left(\vec{r} - \vec{r}_{i+\delta}\right) & \rightarrow \delta\left(\vec{r} - \vec{r}_{i} - \vec{r}_{\delta i}\right) \label{eq:delta_func_expan3} \\
    & \approx \delta\left(\vec{r} - \vec{r}_{i}\right) - \vec{r}_{\delta i} \cdot \nabla_{\vec{r}} \Bigl(\delta\left(\vec{r} - \vec{r}_{i}\right) \Bigr) \label{eq:delta_func_expan4} 
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Vector and Tensor Calculus
%%----------------------------------------------------------------------------------------
\subsection{Vector and Tensor Calculus}  \label{subsec:vectortensors}
\indent  If we have an arbitrary vector that is not coplanar with a plane that has a unit normal $\hat{\textbf{n}}$, we can define the vector along the normal (subscript n) and transverse to the normal (subscript t) as:
\begin{subequations}
  \begin{align}
    Q{\scriptstyle_{n}} & = \textbf{Q} \cdot \hat{\textbf{n}}  \label{eq:rhequations_1a}  \\
    \textbf{Q}{\scriptstyle_{t}} & = \left( \hat{\textbf{n}} \times \textbf{Q} \right) \times \hat{\textbf{n}}  \label{eq:rhequations_1b}  \\
    & = \textbf{Q} \cdot \left( \mathbb{I} - \hat{\textbf{n}} \hat{\textbf{n}} \right)  \label{eq:rhequations_1c}
  \end{align}
\end{subequations}

\indent  Note that advection is not necessarily the same as convection.  Convection is the sum of the advective and diffusive effects of a fluid flow.  Diffusion describes the spread of particles through random motion from regions of higher concentration to regions of lower concentration.  Mathematically, this can be shown by considering the advection term, $\nabla$ $\times$ \textbf{V}, separately from the convection term, \textbf{V} $\cdot$ ($\nabla$ \textbf{V}), because the convection term can be rewritten in the following form:
\begin{equation}
  \label{eq:cattell96a_1}
  \textbf{V} \cdot \left(\nabla \textbf{V}\right) = \left[ \nabla \frac{\mid \textbf{V} \mid^{2}}{2} - \left(\nabla \textbf{V}\right) \cdot \textbf{V} \right] + \left(\textbf{V} \cdot \nabla\right) \textbf{V}
\end{equation}
where we have used the vector identity:
\begin{equation}
  \label{eq:cattell96a_2}
  \textbf{A} \times \left( \nabla \times \textbf{B} \right) = \left(\nabla \textbf{B}\right) \cdot \textbf{A} - \left(\textbf{A} \cdot \nabla\right)\textbf{B} 
\end{equation}

\begin{enumerate}
  \item Assume that the vector \textbf{A} and the scalars, $\psi$ and $\phi$, are well behaved vector functions
  \item V $\equiv$ 3D volume with volume element d$^{3}$x
  \item S $\equiv$ is a closed 2D surface bounding volume \emph{V}, with area element \emph{da}
  \item \textbf{n} $\equiv$ unit \emph{outward} normal vector at surface element \emph{da}
\end{enumerate}
\begin{subequations}
  \begin{align}
  \int_{V} d^{3}x \thickspace \nabla \cdot \textbf{A} & = \int_{S} da \thickspace \textbf{n} \cdot \textbf{A} \label{eq:vec_calc1} \\
  \int_{V} d^{3}x \thickspace \nabla \psi & = \int_{S} da \thickspace \textbf{n} \psi \label{eq:vec_calc2} \\
  \int_{V} d^{3}x \thickspace \nabla \times \textbf{A} & = \int_{S} da \thickspace \textbf{n} \times \textbf{A} \label{eq:vec_calc3} \\
  \int_{V} d^{3}x \thickspace \Bigl[\textbf{A} \cdot \Bigl(\nabla \times \bigl(\nabla \times \textbf{B}\bigr) - \textbf{B} \cdot \Bigl(\nabla \times \bigl(\nabla \times \textbf{A}\bigr)\Bigr)\Bigr] & = \int_{S} da \thickspace \textbf{n} \cdot \Bigl[\textbf{B} \times \Bigl(\nabla \times \textbf{A}\Bigr) - \textbf{A} \times \Bigl(\nabla \times \textbf{B}\Bigr)\Bigr]  \label{eq:vec_calc3_2} \\
  \int_{V} d^{3}x \thickspace \Bigl(\phi \nabla^{2} \psi + \nabla \phi \cdot  \nabla \psi\Bigr) & = \int_{S} da \thickspace \phi \bigl(\textbf{n} \cdot  \nabla \psi\bigr) \text{  (Green's 1}^{st}\text{ Identity)} \label{eq:vec_calc4} \\
  \int_{V} d^{3}x \thickspace \Bigl(\phi \nabla^{2} \psi + \psi \nabla^{2} \phi\Bigr) & = \int_{S} da \thickspace \phi \bigl(\phi \nabla \psi - \psi \nabla \phi\bigr) \text{  (Green's Theorem)} \label{eq:vec_calc5}
  \end{align}
\end{subequations}
\begin{enumerate}
  \item In the following equations, we define \emph{S} $\equiv$ open surface
  \item \emph{C} $\equiv$ contour bounding the open surface S, with line element d\textbf{l}
  \item \textbf{n} $\equiv$ normal to the surface \emph{S} with the direction defined by the \emph{right-hand-screw rule} in relation to the direction of d\textbf{l} (i.e. the line integral around contour \emph{C})
\end{enumerate}
\begin{subequations}
  \begin{align}
  \int_{S} da \thickspace \Bigl(\nabla \times \textbf{A} \Bigr) \cdot \textbf{n} & = \oint_{C} \textbf{A} \cdot d\textbf{l} \text{  (Stokes's Theorem)} \label{eq:vec_calc6} \\
  \int_{S} da \thickspace \Bigl(\textbf{n} \times \nabla\Bigr) \psi & = \oint_{C} \psi d\textbf{l} \label{eq:vec_calc7} \\
  \int_{S} da \thickspace \Bigl(\textbf{n} \times \nabla\Bigr) \times \textbf{A} & = \oint_{C} \thickspace d\textbf{l} \times \textbf{A} \label{eq:vec_calc7_2} \\
  \int_{S} da \thickspace \textbf{n} \cdot \Bigl(\nabla f \times \nabla g\Bigr) & = \oint_{C} dg \thickspace f = - \oint_{C} df \thickspace g \label{eq:vec_calc7_3}
  \end{align}
\end{subequations}
\begin{enumerate}
  \item In the following equations, we define \textbf{x} $\equiv$ coordinate of some point with respect to some origin
  \item r $\equiv$ the magnitude of \textbf{x} (= $\lvert$\textbf{x}$\rvert$)
  \item \textbf{k} $\equiv$ \textbf{x}/r = unit radial vector
  \item f(r) $\equiv$ a well-behaved function of r
  \item \textbf{a} $\equiv$ an arbitrary vector
  \item \textbf{L} $\equiv$ the angular momentum operator defined in Equation \ref{eq:vec_calc14}
\end{enumerate}
\begin{subequations}
  \begin{align}
    \nabla \cdot \textbf{x} & = 3 \label{eq:vec_calc8} \\
    \nabla \times \textbf{x} & = 0 \label{eq:vec_calc9} \\
    \nabla \cdot \Bigl[\textbf{n} f(r) \Bigr] & = \frac{2}{r} f(r) + \frac{\partial f}{\partial r} \label{eq:vec_calc10} \\
    \nabla \times \Bigl[\textbf{n} f(r) \Bigr] & = 0 \label{eq:vec_calc11} \\
    \Bigl(\textbf{a} \cdot \nabla\Bigr) \textbf{n} f(r) & = \frac{f(r)}{r} \Bigl[\textbf{a} - \textbf{n}\Bigl(\textbf{a} \cdot \textbf{n} \Bigr) \Bigr] + \textbf{n}\Bigl(\textbf{a} \cdot \textbf{n} \Bigr) \frac{\partial f}{\partial r} \label{eq:vec_calc12} \\
    \nabla \Bigl(\textbf{x} \cdot \textbf{a} \Bigr) & = \textbf{a} + \textbf{x} \Bigl( \nabla \cdot \textbf{a}\Bigr) + i \Bigl(\textbf{L} \times \textbf{a}\Bigr) \label{eq:vec_calc13} \\
    \textbf{L} & = -i \Bigl(\textbf{x} \times \nabla \Bigr) \label{eq:vec_calc14}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \textbf{a} \cdot \bigl(\textbf{b} \times \textbf{c}\bigr) & = \textbf{b} \cdot \bigl(\textbf{c} \times \textbf{a}\bigr) = \textbf{c} \cdot \bigl(\textbf{a} \times \textbf{b}\bigr) \label{eq:vec_calc15} \\
    \textbf{a} \times \bigl(\textbf{b} \times \textbf{c}\bigr) & = \bigl(\textbf{a} \cdot \textbf{c}\bigr)\textbf{b} - \bigl(\textbf{a} \cdot \textbf{b}\bigr)\textbf{c} \label{eq:vec_calc16} \\
    \bigl(\textbf{a} \times \textbf{b}\bigr) \cdot \bigl(\textbf{c} \times \textbf{d}\bigr) & = \bigl(\textbf{a} \cdot \textbf{c}\bigr)\bigl(\textbf{b} \cdot \textbf{d}\bigr) - \bigl(\textbf{a} \cdot \textbf{d}\bigr)\bigl(\textbf{b} \cdot \textbf{c}\bigr) \label{eq:vec_calc17} \\
    \nabla \times \nabla \psi & = 0 \label{eq:vec_calc18} \\
    \nabla \cdot \bigl(\nabla \times \textbf{a}\bigr) & = 0 \label{eq:vec_calc19} \\
    \nabla \times \bigl(\nabla \times \textbf{a}\bigr) & = \nabla \bigl(\nabla \cdot \textbf{a} \bigr) - \nabla^{2}\textbf{a} \label{eq:vec_calc20} \\
    \nabla \cdot \bigl(\psi \textbf{a} \bigr) & = \textbf{a} \cdot \nabla \psi + \psi \nabla \cdot \textbf{a} \label{eq:vec_calc21} \\
    \nabla \times \bigl(\psi \textbf{a} \bigr) & = \nabla \psi \times \textbf{a} + \psi \nabla \times \textbf{a} \label{eq:vec_calc22} \\
    \nabla \bigl(\textbf{a} \cdot \textbf{b}\bigr) & = \bigl(\textbf{a} \cdot \nabla \bigr)\textbf{b} + \bigl(\textbf{b} \cdot \nabla \bigr)\textbf{a} + \textbf{a} \times \bigl(\nabla \times \textbf{b}\bigr) + \textbf{b} \times \bigl(\nabla \times \textbf{a}\bigr) \label{eq:vec_calc23} \\
    \nabla \cdot \bigl(\textbf{a} \times \textbf{b}\bigr) & = \textbf{b} \cdot \bigl(\nabla \times \textbf{a}\bigr) - \textbf{a} \cdot \bigl(\nabla \times \textbf{b}\bigr) \label{eq:vec_calc24} \\
    \nabla \times \bigl(\textbf{a} \times \textbf{b}\bigr) & = \textbf{a}\bigl(\nabla \cdot \textbf{b}\bigr) - \textbf{b}\bigl(\nabla \cdot \textbf{a}\bigr) + \bigl(\textbf{b} \cdot \nabla\bigr)\textbf{a} - \bigl(\textbf{a} \cdot \nabla\bigr)\textbf{b} \label{eq:vec_calc25} \\
    \bigl(\nabla \textbf{b}\bigr) \cdot \textbf{a} & = \textbf{a} \times \bigl(\nabla \times \textbf{b}\bigr) + \bigl(\textbf{a} \cdot \nabla\bigr) \textbf{b} \label{eq:vec_calc26} \\
    \nabla^{2}\textbf{a} & = \nabla \bigl( \nabla \cdot \textbf{a}\bigr) - \nabla \times \bigl(\nabla \times \textbf{a}\bigr) \label{eq:vec_calc27} \\
  \end{align}
\end{subequations}
\begin{equation}
  \label{eq:laplacian_general} 
    \nabla^{2} \equiv \frac{1}{h_{1} h_{2} h_{3}}\Biggl[\frac{\partial}{\partial u_{1}} \Bigl(\frac{h_{2}h_{3}}{h_{1}}\frac{\partial}{\partial u_{1}}\Bigr) +  \frac{\partial}{\partial u_{2}} \Bigl(\frac{h_{1}h_{3}}{h_{2}}\frac{\partial}{\partial u_{2}}\Bigr) + \frac{\partial}{\partial u_{3}} \Bigl(\frac{h_{1}h_{2}}{h_{3}}\frac{\partial}{\partial u_{3}}\Bigr) \Biggr]
\end{equation}
\begin{table}[htp]
  \begin{center}
  \caption{Scale factors of the Laplacian}
  \label{tab:laplacian}
  \begin{tabular}{| l | c | c | c | c | c | c |}
    \hline \hline
    Coord.        & u$_{1}$ & u$_{2}$  & u$_{3}$ & h$_{1}$ & h$_{2}$ &      h$_{3}$   \\
    \hline
    Cartesian     &    x    &    y     &    z    &    1    &    1    &         1      \\
    \hline
    Cylindrical   &    r    &  $\phi$  &    z    &    1    &    r    &         1      \\
    \hline
    Spherical     &    z    & $\theta$ & $\phi$  &    1    &    r    &  r$\sin{\phi}$ \\
    \hline \hline
    Oblate Sph.   &  $\xi$  &  $\eta$  & $\phi$  & a$\sqrt{\sinh^{2}{\xi} + \sin^{2}{\eta}}$ & a$\sqrt{\sinh^{2}{\xi} + \sin^{2}{\eta}}$ & a$\cosh{\xi}\cos{\eta}$ \\
    \hline \hline
    Elliptic Cyl. &    u    &  $\nu$   &    z    & a$\sqrt{\sinh^{2}{u} + \sin^{2}{\nu}}$ & a$\sqrt{\sinh^{2}{u} + \sin^{2}{\nu}}$ &         1      \\
    \hline
  \end{tabular}
  \end{center}
\end{table}
\begin{enumerate}
  \item In the following equations, \emph{dA} $\equiv$ unit surface area
  \item \emph{dV} $\equiv$ unit volume
  \item \emph{ds}$^{2}$ $\equiv$ 1$^{st}$ Fundamental Form of a Line Element or Geodesic Equation of Free Motion
  \item h$_{i}$ $\equiv$ scale factors in the coordinate system metric
  \item g$_{\mu \nu}$ $\equiv$ coordinate system metric
  \item $\Gamma^{\lambda}_{\mu \nu}$ $\equiv$ Christoffel Symbol of the Second Kind
\end{enumerate}
\begin{equation}
  \label{eq:metrics_1}
  h_{i} \equiv \sqrt{g_{ii}} = \sqrt{\sum_{k=1}^{n} \Biggl(\frac{\partial X_{k}}{\partial q_{i}} \Biggr)^{2}}
\end{equation}
\begin{equation}
  \label{eq:metrics_2}
  g_{ij} = g_{ii} \delta_{ij} \text{ (Diagonal Metric)}
\end{equation}
\begin{subequations}
  \begin{align}
    ds^{2} & = g_{11} dx_{1}^{2} + \dotsc + g_{nn} dx_{n}^{2} \label{eq:metrics_3} \\
    & = h_{1}^{2} dx_{1}^{2} + \dotsc + h_{n}^{2} dx_{n}^{2} \label{eq:metrics_4} 
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
  \nabla^{2} \phi & = g^{\mu \nu} \partial_{\mu} \partial_{\nu} \phi - \Gamma^{\mu} \partial_{\nu} \phi \label{eq:metrics_5} \\
  & = g^{\mu \nu} \frac{\partial}{\partial_{\mu}} \Bigl(\frac{\partial \phi}{\partial_{\nu}} \Bigr) - \Gamma^{\mu} \frac{\partial \phi}{\partial_{\nu}} \label{eq:metrics_6}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Gamma^{\lambda} \thinspace _{\mu \nu} & \equiv \frac{\partial^{2} \zeta^{\alpha}}{\partial x^{\mu} \partial x^{\nu}} \frac{\partial x^{\lambda}}{\partial \zeta^{\alpha}} \label{eq:metrics_7} \\
    & = \frac{1}{2} g^{\lambda \alpha} \Bigl[\partial_{\nu} g_{\alpha \mu} + \partial_{\mu} g_{\nu \alpha} - \partial_{\alpha} g_{\mu \nu} \Bigr] \label{eq:metrics_8} \\
    & = g^{\alpha \lambda} \Bigl[\mu \nu , \lambda\Bigr] \label{eq:metrics_9}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Gamma_{\lambda \mu \nu} & = 0 \text{ for } \lambda \ne \mu \ne \nu \label{eq:metrics_10} \\
    \Gamma_{\lambda \lambda \nu} & = -\frac{1}{2} \frac{\partial g_{\lambda \lambda}}{\partial x^{\nu}} \text{ for } \lambda \ne \nu \label{eq:metrics_11} \\
    \Gamma_{\lambda \mu \lambda} & = \Gamma_{\mu \lambda \lambda} = \frac{1}{2} \frac{\partial g_{\lambda \lambda}}{\partial x^{\mu}} \label{eq:metrics_12} \\
    \Gamma^{\nu} \thinspace _{\lambda \mu} & = 0 \text{ for } \lambda \ne \mu \ne \nu \label{eq:metrics_13} \\
    \Gamma^{\nu} \thinspace _{\lambda \lambda} & = -\frac{1}{2 g_{\nu \nu}} \frac{\partial g_{\lambda \lambda}}{\partial x^{\nu}}  \text{ for } \lambda \ne \nu \label{eq:metrics_14} \\
    \Gamma^{\lambda} \thinspace _{\lambda \mu} & = \Gamma^{\lambda} \thinspace _{\mu \lambda} = \frac{1}{2 g_{\lambda \lambda}} \frac{\partial g_{\lambda \lambda}}{\partial x^{\mu}} = \frac{1}{2} \frac{\partial \ln{g_{\lambda \lambda}}}{\partial x^{\mu}} \label{eq:metrics_15}
  \end{align}
\end{subequations}
\begin{equation}
  \label{eq:metrics_16}
  d\tau^{2} \equiv - g_{\mu \nu} dx^{\mu}dx^{\nu}
\end{equation}
\begin{equation}
  \label{eq:metrics_17}
  \nabla_{\mu} V^{\nu} \equiv \partial_{\mu} V^{\nu} + \Gamma^{\nu} \thinspace _{\mu \lambda} V^{\lambda}
\end{equation}
Let x$^{\mu}$ = x$^{\mu}$($\lambda$) then:
\begin{equation}
  \label{eq:metrics_18}
  \frac{d^{2} x^{\mu}}{d\lambda^{2}} + \Gamma^{\mu} \thinspace _{\rho \sigma} \frac{dx^{\rho}}{d\lambda} \frac{dx^{\sigma}}{d\lambda} = 0
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Mean, Variance, Covariance, and Correlation
%%----------------------------------------------------------------------------------------
\subsection{Mean, Variance, Covariance, and Correlation}  \label{subsec:meanvarcovar}

\indent  We will use $\langle$ $\rangle{\scriptstyle_{\alpha}}$ to denote the \emph{arithmetic mean} (or \emph{expectation value} or \emph{average}) with respect to the variable $\alpha$ (e.g., time or space).  These angle brackets act like an operator and can be defined by:
\begin{equation}
  \label{eq:mean_0a}
  \left\langle f \left( x \right) \right\rangle{\scriptstyle_{\alpha}} = \frac{ \int \thickspace d\alpha \thickspace f \left( x \right) }{ \int \thickspace d\alpha }
\end{equation}
for a continuous function\footnote{in general, the denominator is not present and the $\langle$ $\rangle{\scriptstyle_{\alpha}}$ is an unnormalized average} or
\begin{equation}
  \label{eq:mean_0b}
  \left\langle x \right\rangle = \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}}
\end{equation}
for discrete variates, $x{\scriptstyle_{i}}$.  We will use $\mu{\scriptstyle_{2}}$, $\sigma^{2}$, or $var()$ to denote the \emph{variance} and $cov(x,y)$ to denote the \emph{covariance}.  Finally, we will denote the \emph{correlation} as $cor(x,y)$.  \\
\indent  The $\mu{\scriptstyle_{n}}$ notation denotes the $n$-th moment of some probability distribution, P(x), for some function, f(x).  In general, the moments are called \emph{raw moments} ($\mu{\scriptstyle_{n}}$), as they are not centered on any significant value of $x$.  Herein, we will use \emph{central moments} ($\bar{\mu}{\scriptstyle_{n}}$), which are centered on the \emph{mean}.  The general form of the $n$-th central moment is:
\begin{subequations}
  \begin{align}
    \bar{\mu}{\scriptstyle_{n}} & \equiv \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{n} \right\rangle  \label{eq:mean_0c}  \\
    & = \int \thickspace dx \thickspace \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{n} \thickspace P\left( x \right)  \label{eq:mean_0d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

where the integral is changed to a summation for discrete f(x).  The various moments of a distribution are defined as:

\begin{subequations}
  \begin{align}
    \text{  normalization  } & \equiv \mu{\scriptstyle_{0}}  \label{eq:moms_0a}  \\
    & \equiv \text{  density for particle velocity distributions  }  \notag \\
    \text{  mean  } & \equiv \mu{\scriptstyle_{1}} = \left\langle f\left( x \right) \right\rangle  \label{eq:moms_0b}  \\
    & \equiv \text{  bulk flow velocity for particle velocity distributions  }  \notag \\
    \bar{\mu}{\scriptstyle_{1}} & = \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right] \right\rangle = \left\langle f\left( x \right) \right\rangle - \left\langle f\left( x \right) \right\rangle = 0  \label{eq:moms_0c}  \\
    \text{  variance  } & \equiv \bar{\mu}{\scriptstyle_{2}} = \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{2} \right\rangle  \label{eq:moms_0d}  \\
    & \equiv \text{  pressure tensor for particle velocity distributions  }  \notag \\
    \text{  skewness  } & \equiv \frac{ \bar{\mu}{\scriptstyle_{3}} }{ \bar{\mu}{\scriptstyle_{2}}^{3/2} } = \text{  measure of asymmetry of a distribution  }  \label{eq:moms_0e}  \\
    & \equiv \text{  heat flux tensor for particle velocity distributions  }  \notag \\
    \text{  kurtosis  } & \equiv \frac{ \bar{\mu}{\scriptstyle_{4}} }{ \bar{\mu}{\scriptstyle_{2}}^{2} } = \text{  degree of peakedness of a distribution  }  \label{eq:moms_0f}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Mean
%%----------------------------------------------------------------------------------------
\subsubsection{Mean}  \label{subsubsec:mean}
\noindent  The \emph{mean} satisfies the following relations:
\begin{subequations}
  \begin{align}
    \langle a x + b y \rangle & = a \langle x \rangle + b \langle y \rangle  \label{eq:mean_1a}  \\
    \langle f \left( x \right) + g \left( x \right) \rangle & = \langle f \left( x \right) \rangle + \langle g \left( x \right) \rangle  \label{eq:mean_1b}  \\
    \langle f \left( x \right) \cdot g \left( x \right) \rangle & = \langle f \left( x \right) \rangle \cdot \langle g \left( x \right) \rangle  \label{eq:mean_1c}  \\
    \langle a x + b \rangle & = a \langle x \rangle + b  \label{eq:mean_1d}  \\
    \langle \bar{x} \rangle & = \left\langle \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1e}  \\
    & = \frac{ 1 }{ N } \left\langle \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1f}  \\
    & = \frac{ 1 }{ N } \sum_{i = 1}^{N} \left\langle x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1g}  \\
    & = \frac{ 1 }{ N } \sum_{i = 1}^{N} \mu  \label{eq:mean_1h}  \\
    & = \frac{ 1 }{ N } \left( N \mu \right)  \label{eq:mean_1i}  \\
    & \equiv \mu  \label{eq:mean_1j}
%%    \langle  \rangle & = 
  \end{align}
\end{subequations}
where $\mu$ is the \emph{population mean} or the first moment of the central moments defined by:
\begin{equation}
  \label{eq:mean_2}
  \bar{\mu}{\scriptstyle_{n}} = \left\langle \left( x - \left\langle x \right\rangle \right)^{n} \right\rangle  \text{  .}
\end{equation}
\noindent  The \emph{mean} of a bivariate function satisfies the following relationship:
\begin{subequations}
  \begin{align}
    \left\langle \left( x - \mu{\scriptstyle_{x}} \right) \left( y - \mu{\scriptstyle_{y}} \right) \right\rangle & = \left\langle  x y + \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} - x \mu{\scriptstyle_{y}} - y \mu{\scriptstyle_{x}} \right\rangle  \label{eq:mean_3a}  \\
    & = \left\langle x y \right\rangle + \left\langle \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} \right\rangle - \left\langle x \mu{\scriptstyle_{y}} \right\rangle - \left\langle y \mu{\scriptstyle_{x}} \right\rangle  \label{eq:mean_3b}  \\
    & = \left\langle x y \right\rangle + \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} - \mu{\scriptstyle_{y}} \left\langle x \right\rangle - \mu{\scriptstyle_{x}} \left\langle y \right\rangle  \label{eq:mean_3c}  \\
    & = \left\langle x y \right\rangle - \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}}  \label{eq:mean_3d}  \\
    & = \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle  \text{  .}  \label{eq:mean_3e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Variance
%%----------------------------------------------------------------------------------------
\subsubsection{Variance}  \label{subsubsec:variance}
\noindent  The \emph{variance} is given by:
\begin{subequations}
  \begin{align}
    var\left( x \right) & = \left\langle \left( x - \left\langle x \right\rangle \right)^{2} \right\rangle  \label{eq:variance_0a}  \\
    & = \left\langle \left( x^{2} + \left\langle x \right\rangle^{2} - 2 x \left\langle x \right\rangle \right) \right\rangle  \label{eq:variance_0b}  \\
    & = \left\langle x^{2} \right\rangle + \left\langle \left\langle x \right\rangle^{2} \right\rangle - 2 \left\langle x \left\langle x \right\rangle \right\rangle  \label{eq:variance_0c}  \\
    & = \left\langle x^{2} \right\rangle - \left\langle x \right\rangle^{2}  \text{  .}  \label{eq:variance_0d}
%%    \left\langle  \right\rangle & = 
  \end{align}
\end{subequations}
The \emph{variance} satisfies the following relationship:
\begin{subequations}
  \begin{align}
    var\left( \bar{x} \right) & = var\left\{ \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\}  \label{eq:variance_1a}  \\
    & = \left\langle \left[ \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} - \left\langle \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1b}  \\
    & = \frac{ 1 }{ N^{2} } \left\langle \left[ \sum_{i = 1}^{N} x{\scriptstyle_{i}} - \left\langle \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1c}  \\
    & = \frac{ 1 }{ N^{2} } var\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\}  \label{eq:variance_1d}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} var\left\{ x{\scriptstyle_{i}} \right\}  \label{eq:variance_1e}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} \left\langle \left[ x{\scriptstyle_{i}} - \left\langle x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1f}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} \sigma^{2} = \frac{ \sigma^{2} }{ N }  \text{  .}  \label{eq:variance_1g}
%%    \left\langle  \right\rangle & = 
  \end{align}
\end{subequations}
The \emph{variance} also satisfies the following relationship:
\begin{subequations}
  \begin{align}
    var\left( a x + b \right) & = \left\langle \left[ \left( a x + b \right) - \left\langle a x + b \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2a}  \\
    & = \left\langle \left[ \left( a x \right) - a \left\langle x \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2b}  \\
    & = \left\langle a^{2} \left[ x - \left\langle x \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2c}  \\
    & = a^{2} var\left( x \right)  \text{  .}  \label{eq:variance_2d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
The \emph{variance} of two random variates is given by:
\begin{subequations}
  \begin{align}
    var\left( x + y \right) & = var\left( x \right) + var\left( y \right) + 2 cov\left( x, y \right)  \label{eq:variance_3a}  \\
    & = \sigma{\scriptstyle_{x}}^{2} + \sigma{\scriptstyle_{y}}^{2} + 2 cov\left( x, y \right)  \label{eq:variance_3b}  \\
    var\left( y - b x \right) & = var\left( y \right) + var\left( - b x \right) + 2 cov\left( y, - b x \right)  \label{eq:variance_3c}  \\
    & = var\left( y \right) + b^{2} var\left( x \right) - 2 b cov\left( y, x \right)  \label{eq:variance_3d}  \\
    & = \sigma{\scriptstyle_{y}}^{2} + b^{2} \sigma{\scriptstyle_{x}}^{2} - 2 b \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} cor\left( y, x \right)  \label{eq:variance_3e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
where we have used $\sigma{\scriptstyle_{x}}$ $\equiv$ $\sqrt{ var\left( x \right) }$ (also known as the \emph{standard deviation}), $cov\left( y, x \right)$ $\equiv$ the \emph{covariance} (see Section \ref{subsubsec:covariance} for more), and $cor\left( y, x \right)$ $\equiv$ the \emph{correlation} (see Section \ref{subsubsec:correlation} for more).
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Covariance
%%----------------------------------------------------------------------------------------
\subsubsection{Covariance}  \label{subsubsec:covariance}
\indent  The \emph{covariance} is given by:
\begin{equation}
  \label{eq:covariance_0a}
  cov\left( x, y \right) \equiv \left\langle \left( x - \mu{\scriptstyle_{x}} \right) \left( y - \mu{\scriptstyle_{y}} \right) \right\rangle = \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle 
\end{equation}
where we note that:
\begin{equation}
  \label{eq:covariance_1a}
  cov\left( x, x \right) = \left\langle x^{2} \right\rangle - \left\langle x \right\rangle^{2} = var\left( x \right)  \text{  .}
\end{equation}
\noindent  The \emph{covariance} can also be related to the \emph{variance} through:
\begin{subequations}
  \begin{align}
    var\left( \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right) & = cov\left( \sum_{i = 1}^{N} x{\scriptstyle_{i}}, \sum_{j = 1}^{N} x{\scriptstyle_{j}} \right)  \label{eq:covariance_2a}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2b}  \\
    & = \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j = i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right) + \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j \neq i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2c}  \\
    & = \sum_{i = 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{i}} \right) + \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j \neq i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2d}  \\
    & = \sum_{i = 1}^{N} var\left( x{\scriptstyle_{i}} \right) + 2 \sum_{i = 1}^{N} \sum_{j = i + 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \text{  .}  \label{eq:covariance_2e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
\noindent  The \emph{covariance} has the following property for linear sums:
\begin{subequations}
  \begin{align}
    var\left( \sum_{i = 1}^{N} a{\scriptstyle_{i}} x{\scriptstyle_{i}} \right) & = cov\left( \sum_{i = 1}^{N} a{\scriptstyle_{i}} x{\scriptstyle_{i}}, \sum_{j = 1}^{N} a{\scriptstyle_{j}} x{\scriptstyle_{j}} \right)  \label{eq:covariance_3a}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{N} a{\scriptstyle_{i}} a{\scriptstyle_{j}} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_3b}  \\
    & = \sum_{i = 1}^{N} a{\scriptstyle_{i}}^{2} var\left( x{\scriptstyle_{i}} \right) + 2 \sum_{i = 1}^{N} \sum_{j = i + 1}^{N} a{\scriptstyle_{i}} a{\scriptstyle_{j}} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \text{  .}  \label{eq:covariance_3c}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
We use the results for the \emph{variance} of two random variates is given by Equations \ref{eq:variance_3a} -- \ref{eq:variance_3e}, which allows us to calculate:
\begin{subequations}
  \begin{align}
    cov\left( x + z, y \right) & = \left\langle \left[ \left( x + z \right) - \left\langle x + z \right\rangle \right] \cdot \left[ \left( y \right) - \left\langle y \right\rangle \right] \right\rangle  \label{eq:covariance_5a}  \\
    & = \left\langle \left( x + z \right) y \right\rangle - \left\langle x + z \right\rangle \left\langle y \right\rangle  \label{eq:covariance_5b}  \\
    & = \left\langle x y + z y \right\rangle - \left[ \left\langle x \right\rangle + \left\langle z \right\rangle \right] \left\langle y \right\rangle  \label{eq:covariance_5c}  \\
    & = \left\langle x y \right\rangle + \left\langle z y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle - \left\langle z \right\rangle \left\langle y \right\rangle  \label{eq:covariance_5d}  \\
    & = \left[ \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle \right] + \left[ \left\langle z y \right\rangle - \left\langle z \right\rangle \left\langle y \right\rangle \right]  \label{eq:covariance_5e}  \\
    & = cov\left( x, y \right) + cov\left( z, y \right)  \text{  .}  \label{eq:covariance_5f}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
\noindent  Finally, the \emph{covariance} satisfies the following:
\begin{subequations}
  \begin{align}
    cov\left( x, a \right) & = 0  \label{eq:covariance_6a}  \\
    cov\left( a x, b y \right) & = a b \thickspace cov\left( x, y \right)  \label{eq:covariance_6b}  \\
    cov\left( x + a, y + b \right) & = cov\left( x, y \right)  \label{eq:covariance_6c}  \\
    cov\left( a x + b y, c w + d z \right) & = a c \thickspace cov\left( x, w \right) + a d \thickspace cov\left( x, z \right) + b c \thickspace cov\left( y, w \right) + b d \thickspace cov\left( y, z \right)  \label{eq:covariance_6d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
where the proofs are just a few lines of algebra following the rules for the \emph{variance} and the \emph{mean}.  The \emph{covariance matrix} is given by:
\begin{equation}
  \label{eq:covariance_7a}
  V{\scriptstyle_{ij}} = cor\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right) \equiv \left\langle \left( x{\scriptstyle_{i}} - \mu{\scriptstyle_{i}} \right) \cdot \left( x{\scriptstyle_{j}} - \mu{\scriptstyle_{j}} \right) \right\rangle
\end{equation}
where an individual matrix element, $V{\scriptstyle_{ij}}$, is called the covariance of $x{\scriptstyle_{i}}$ and $x{\scriptstyle_{j}}$.  \\
\indent  The \emph{covariance}, in the form of Equation \ref{eq:covariance_0a}, is similar, physically, to the \emph{pressure} in kinetic theory.  More generally, the \emph{covariance matrix} is analogous to the \emph{pressure tensor} in kinetic theory\footnote{just replace $x{\scriptstyle_{i}}$ with a component of the particle momentum(velocity) and $\mu{\scriptstyle_{i}}$ with the first moment, or bulk flow momentum(velocity)}.  Recall that the \emph{pressure tensor} is symmetric, which is the result of the following:
\begin{subequations}
  \begin{align}
    cov\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}}, y \right\} & = \sum_{i = 1}^{N} cov\left\{ x{\scriptstyle_{i}}, y \right\}  \label{eq:covariance_8a}  \\
    cov\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}}, \sum_{j = 1}^{M} y{\scriptstyle_{j}} \right\} & = \sum_{i = 1}^{N} cov\left\{ x{\scriptstyle_{i}}, \sum_{j = 1}^{M} y{\scriptstyle_{j}} \right\}  \label{eq:covariance_8b}  \\
    & = \sum_{i = 1}^{N} cov\left\{ \sum_{j = 1}^{M} y{\scriptstyle_{j}}, x{\scriptstyle_{i}} \right\}  \label{eq:covariance_8c}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{M} cov\left\{ y{\scriptstyle_{j}}, x{\scriptstyle_{i}} \right\}  \label{eq:covariance_8d}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{M} cov\left\{ x{\scriptstyle_{i}}, y{\scriptstyle_{j}} \right\}  \label{eq:covariance_8e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
%%    \left\{  \right\}
  \end{align}
\end{subequations}
which shows that $V{\scriptstyle_{ij}}$ $=$ $V{\scriptstyle_{ji}}$ $\Rightarrow$ the \emph{pressure tensor} is symmetric.
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Correlation
%%----------------------------------------------------------------------------------------
\subsubsection{Correlation}  \label{subsubsec:correlation}
\noindent  The \emph{correlation} is given by:
\begin{equation}
  \label{eq:correlation_0a}
  cor\left( x, y \right) \equiv \frac{ cov\left( x, y \right) }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} }
\end{equation}
where $\sigma{\scriptstyle_{x}}$ is defined as $\sqrt{ var\left( x \right) }$.  We can use a rule for the \emph{variance}, given by:
\begin{subequations}
  \begin{align}
    var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \pm \frac{ y }{ \sigma{\scriptstyle_{y}} } \right\} & = var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \right\} + var\left\{ \frac{ \pm y }{ \sigma{\scriptstyle_{y}} } \right\} + 2 var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} }, \frac{ \pm y }{ \sigma{\scriptstyle_{y}} } \right\}  \label{eq:correlation_1a}  \\
    & = \frac{ 1 }{ \sigma{\scriptstyle_{x}}^{2} } var\left( x \right) + \frac{ 1 }{ \sigma{\scriptstyle_{y}}^{2} } var\left( y \right) \pm \frac{ 2 }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} } cov\left( x, y \right)  \label{eq:correlation_1b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
combined with the knowledge that:
\begin{equation}
  \label{eq:correlation_2a}
  var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \pm \frac{ y }{ \sigma{\scriptstyle_{y}} } \right\} \geq 0
\end{equation}
to prove the following:
\begin{equation}
  \label{eq:correlation_3a}
  -1 \leq cor\left( x, y \right) \leq 1  \text{  .}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Linear Algebra
%%----------------------------------------------------------------------------------------
\subsection{Linear Algebra} \label{subsec:LA}

Let $\langle \mathbf{x} \rangle$ be defined as the \emph{sample mean}, which mathematically means:

\begin{equation}
  \label{eq:linalg_0}
  \langle \mathbf{x} \rangle \equiv \frac{1}{N} \sum_{i=1}^{N} \textbf{x}_{i}
\end{equation}

where N is the number of samples in your data set.  Let us define the following:

\begin{equation}
  \label{eq:linalg_1}
  \hat{\textbf{x}}_{k} \equiv \textbf{x}_{k} - \langle \mathbf{x} \rangle
\end{equation}

which leads us to a matrix whose columns have a zero sample mean, defined as:

\begin{equation}
  \label{eq:linalg_2}
  \textbf{B} = \Bigl[\hat{\textbf{x}}_{1} \hat{\textbf{x}}_{2} \dotsc \hat{\textbf{x}}_{n}\Bigr] \text{ .}
\end{equation}

The \emph{sample covariance matrix} is thus defined by:

\begin{equation}
  \label{eq:linalg_3}
  \textbf{S} \equiv \frac{\textbf{B} \textbf{B}^{T}}{N-1} \text{ .}
\end{equation}

If we now define a vector, \textbf{X}, which varies over the set of observed vectors and denote the coordinates by \emph{x}$_{j}$, then the diagonal entry, \emph{s}$_{jj}$ in \textbf{S} is called the variance of \emph{x}$_{j}$.  Thus, \emph{s}$_{jj}$ measures the \emph{spread} of the values of \emph{x}$_{j}$.  The \emph{total variance} is defined as:

\begin{equation}
  \label{eq:linalg_4}
  \Bigl\{Total Variance\Bigr\} \equiv Tr\Bigl[\textbf{S}\Bigr]
\end{equation}

The \emph{covariance}, \emph{s}$_{ij}$ for i $\neq$ j, is equal to zero when \emph{x}$_{i}$ and \emph{x}$_{j}$ are uncorrelated.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Principle Component Analysis
%%----------------------------------------------------------------------------------------
\subsection{Principle Component Analysis} \label{subsec:PCA}

The main goal here is to find an orthogonal \emph{n}$\times$\emph{n} matrix, \textbf{P} = [\textbf{u}$_{1}$ $\dotsc$ \textbf{u}$_{n}$], such that \textbf{X} = \textbf{P} \textbf{Y}, with the property that the components of \textbf{Y}, \emph{y}$_{j}$, are uncorrelated and arranged in order of decreasing variance.  This implies that each individual observed vector, \textbf{X}$_{k}$, goes to a new \emph{name}, \textbf{Y}$_{k}$.  This results in the following relationship:

\begin{equation}
  \label{eq:linalg_5}
  \textbf{Y}_{k} = \textbf{P}^{-1} \textbf{X}_{k} = \textbf{P}^{T} \textbf{X}_{k} \text{  for k = 1,$\dotsc$,N .}
\end{equation}

A direct result of this \emph{re-naming} is that the covariance matrix for \textbf{Y}$_{k}$ is:

\begin{equation}
  \label{eq:linalg_6}
  \textbf{S}_{2} = \textbf{P}^{T} \textbf{S} \textbf{P}
\end{equation}

which forces \textbf{S}$_{2}$ to be diagonal (since \textbf{P} is an orthogonal matrix).  Now if we allow \textbf{D} to be a diagonal matrix with eigenvalues of \textbf{S}, $\lambda_{k}$ on the diagonal arranged so that $\lambda_{1}$ $\geq$ $\lambda_{2}$ $\geq$ $\dotsc$ $\lambda_{n}$ $\geq$ 0, then if \textbf{P} is an orthogonal matrix of corresponding eigenvectors we have:

\begin{subequations}
  \begin{align}
    \textbf{S} & = \textbf{P} \textbf{D} \textbf{P}^{T}  \label{eq:linalg_7} \\
    \textbf{D} & = \textbf{P}^{T} \textbf{S} \textbf{P} \label{eq:linalg_8} \text{ .}
  \end{align}
\end{subequations}

The eigenvectors, \textbf{u}$_{i}$, of the covariance matrix, \textbf{S}, are called the \emph{principal components} of the data.  The \emph{first principal component}, \textbf{u}$_{1}$, is the eigenvector corresponding to the largest eigenvalue of \textbf{S} and the \emph{second principal component}, \textbf{u}$_{2}$, corresponds to the second largest eigenvalue and so on.  If we allow \emph{c}$_{i}$ to be entries of \textbf{u}$_{1}$, then \textbf{Y} $=$ \textbf{P}$^{T}$ \textbf{X} gives:

\begin{equation}
  \label{eq:linalg_9}
  y_{1} = \textbf{u}_{1}^{T} \textbf{X} = c_{1} x_{1} + c_{2} x_{2} + \dotsc + c_{n} x_{n}
\end{equation}

which means y$_{1}$ is a linear combination of the original variables x$_{1}$ $\dotsc$ x$_{n}$.  One thing to note, the orthogonal change of variables, \textbf{X} $=$ \textbf{P} \textbf{Y}, does NOT change the total variance of the data, or in other words:

\begin{subequations}
  \begin{align}
    \Bigl\{Total \: Variance \: of \: x_{i}\Bigr\} & = \Bigl\{Total \: Variance \: of \: y_{i}\Bigr\}\label{eq:linalg_10} \\
    \Bigl\{Total \: Variance \: of \: y_{i}\Bigr\} & = Tr\Bigl[\textbf{D}\Bigr] \label{eq:linalg_11} \\
    & = \lambda_{1} + \dotsc + \lambda_{n} \label{eq:linalg_12}
  \end{align}
\end{subequations}

$\Rightarrow$
where the variance of \emph{y}$_{i}$ is $\lambda_{i}$, and $\lambda_{i}$/Tr$ \bigl[$\textbf{S}$\bigr]$ measures the fraction of the total variace that is \emph{explained} or \emph{captured} by \emph{y}$_{i}$.  Thus if \textbf{u} satisfies, \emph{y} $=$ \textbf{u}$^{T}$ \textbf{X}, then the variance of the values of \emph{y} as \textbf{X} varies over the original data, \textbf{X}$_{i}$, is \textbf{u}$^{T}$\textbf{S}\textbf{u}.

\begin{enumerate}
  \item The maximum value of \textbf{u}$^{T}$\textbf{S}\textbf{u} occurs for $\lambda_{1}$ and \textbf{u}$_{1}$
  \item \emph{y}$_{2}$ has a maximum variance among all variables \emph{y} $=$ \textbf{u}$^{T}$\textbf{X} that are uncorrelated with \emph{y}$_{1}$
  \item Likewise, \emph{y}$_{3}$ has a maximum variance among all variables that are uncorrelated with BOTH \emph{y}$_{1}$ and \emph{y}$_{2}$.
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Minimum Variance Analysis
%%----------------------------------------------------------------------------------------
\subsection{Minimum Variance Analysis} \label{subsec:MVA}

Minimum variance analysis, or MVA, is the utilization of a property of plane polarized linear electromagnetic waves which allows one to assume that fluctuations in the electric ($\delta \mathbf{E}$) and magnetic ($\delta \mathbf{B}$) fields are are in a plane orthogonal to the direction of propagation ($\hat{\mathbf{k}}$) \citet{khrabrov98}.  If the wave is truly a plane polarized wave, then $\hat{\mathbf{k}}$ $\cdot$ $\delta \mathbf{B}$ $=$ 0, which is a linear approximation of the Maxwell equation, $\nabla$ $\cdot$ $\mathbf{B}$ $=$ 0.  The analysis is performed by minimizing the variance matrix of the magnetic field given by:

\begin{equation}
      \label{eq:appmv_1}
      \textbf{S}{\scriptstyle_{pq}} = \Bigl< \Bigl( B{\scriptstyle_{p}} - \bigl<B{\scriptstyle_{p}}\bigr> \Bigr) \Bigl( B{\scriptstyle_{q}} - \bigl<B{\scriptstyle_{q}}\bigr> \Bigr) \Bigr>
\end{equation}

where $\langle B{\scriptstyle_{p}} \rangle$ is the average of the $p{\scriptstyle^{th}}$ component of the magnetic field.  We assume $\mathbf{S}{\scriptstyle_{pq}}$ to be a non-degenerate matrix with three distinct eigenvalues, $\lambda{\scriptstyle_{3}}$ $<$ $\lambda{\scriptstyle_{2}}$ $<$ $\lambda{\scriptstyle_{1}}$, and three corresponding eigenvectors, $\mathbf{e}{\scriptstyle_{3}}$, $\mathbf{e}{\scriptstyle_{2}}$, $\mathbf{e}{\scriptstyle_{1}}$.  Thus the minimum variance eigenvalue and eigenvector are $\lambda{\scriptstyle_{3}}$ and $\mathbf{e}{\scriptstyle_{3}}$.  The propagation direction is said to be along $\hat{\textbf{e}} {\scriptstyle_{3}}$ if one assumes small isotropic noise and the condition $\lambda{\scriptstyle_{2}}$/$\lambda{\scriptstyle_{3}}$ $\geq$ 10 is satisfied.  Then the uncertainty in this direction is given by \citet{kawano95a}:

\begin{equation}
  \label{eq:appmv_2}
  \delta \hat{\textbf{k}} = \pm \Bigl(\hat{\textbf{e}}{\scriptstyle_{1}} \sqrt{\frac{\delta \lambda{\scriptstyle_{3}}}{\lambda{\scriptstyle_{1}} - \lambda{\scriptstyle_{3}}}} + \hat{\textbf{e}}{\scriptstyle_{2}} \sqrt{\frac{\delta \lambda{\scriptstyle_{3}}}{\lambda{\scriptstyle_{2}} - \lambda{\scriptstyle_{3}}}} \Bigr)
\end{equation}

where \emph{K} is the number vectors used and $\delta \lambda{\scriptstyle_{3}}$, the uncertainty in the $\lambda{\scriptstyle_{3}}$ eigenvalue, is given by:

\begin{equation}
  \label{eq:appmv_3}
  \delta \lambda{\scriptstyle_{3}} = \pm \lambda{\scriptstyle_{3}} \sqrt{ \frac{2}{(K - 1)} }  \text{  .}
\end{equation}

In general, the uncertainty of $\delta \lambda{\scriptstyle_{i}}$ is given by:

\begin{equation}
  \label{eq:appmv_4}
  \delta \lambda{\scriptstyle_{i}} = \pm \sqrt{\frac{2 \lambda{\scriptstyle_{3}} (\lambda{\scriptstyle_{i}} - \lambda{\scriptstyle_{3}}) }{(K - 1)}}  \text{  .}
\end{equation}

Another useful quantity to know is the angle between the local ambient magnetic field and the propagation direction, $\theta{\scriptstyle_{kB}}$.  This can be calculated in the typical manner, $\theta{\scriptstyle_{kB}} \equiv \cos^{-1}{\left( \hat{\textbf{k}} \cdot \hat{\textbf{b}} \right)}$, with associated uncertainties of:

\begin{equation}
  \label{eq:appmv_5}
  \delta \theta{\scriptstyle_{kB}} = \pm \sqrt{ \frac{ \lambda{\scriptstyle_{3}} \lambda{\scriptstyle_{2}} }{(K - 1) (\lambda{\scriptstyle_{2}} - \lambda{\scriptstyle_{3}})^{2} } }  \text{  .}
\end{equation}  

\citet{khrabrov98} found analytical estimates to the error analysis of statistical noise in a vector field (i.e., B-field) with the application of minimum/maximum variance analysis.  They consider two special cases of signal-to-noise ratios: 1) large and 2) small, for arbitrary noise distributions.

\begin{enumerate}
  \item \textbf{The Ideal Case} $\equiv$ small errors and isotropic Gaussian noise
  \item For the ideal case, one can determine \emph{uncertainty cones} with elliptic cross sections for all three eigenvectors: $\mathbf{x}_{1}$,$\mathbf{x}_{2}$, $\mathbf{x}_{3}$, and \emph{uncertainty intervals} for all three eigenvalues: $\lambda_{1}$, $\lambda_{2}$, $\lambda_{3}$
  \item Note: $\lambda_{3}$ $<$ $\lambda_{2}$ $<$ $\lambda_{1}$ by definition
  \item \textbf{Anisotropic Noise, No Signal:} 1) $\lambda_{3}$ $\approx$ $\lambda_{2}$ $\equiv$ \textbf{Linearly Polarized IF} $\lambda_{3}$ $\ll$ $\lambda_{1}$ \textbf{AND} the non-fluctuating part of the signal is negligible (i.e., only measuring noise due to wave packets which are broadband or spatially unresolved), 2) $\lambda_{1}$ $\approx$ $\lambda_{2}$ $\equiv$ \textbf{Circularly Polarized} IF $\lambda_{3}$ $\ll$ $\lambda_{2}$
  \item \textbf{Small Anisotropic Noise:}  If amplitude of noise $\ll$ amplitude of signal, then $\lambda_{3}$ can be said to be entirely due to noise
  \item \textbf{Isotropic Gaussian Noise:}  Equations \ref{eq:khrabrov_stdev2} and \ref{eq:khrabrov_vecuncert1} implicitly assume isotropic Gaussian noise
  \item In Equation \ref{eq:khrabrov_dphi1}, $\Delta \phi_{i,j}$ $\equiv$ the angular standard deviation (radians) of the $i^{th}$ vector's ($\vec{x}_{i}$) direction towards/away from the $j^{th}$ vector's ($\vec{x}_{j}$) direction
  \item The \emph{Variance} of any quantity is defined as in Equation \ref{eq:khrabrov_variance1}.  The use of Minimum Variance Analysis (MVA) on magnetic fields derives from the Maxwell Equation $\nabla \cdot \mathbf{B}$ = 0.  From this equation, one can convert the divergence into a dot product between a vector, \textbf{n}, and the B-field.  If this vector \textbf{n} exists, the field does not vary along it.  Thus we say, $B_{n}$ = $\mathbf{n} \cdot \mathbf{B}$ = constant!  So we vary the B-field in each of it's component directions and the variance is described by Equation \ref{eq:khrabrov_variance2}, where K $\equiv$ number of measurements/vectors.
  \item \emph{Rule of Thumb:}  for K $<$ 50, REQUIRE $\lambda_{2}$/$\lambda_{3}$ $\ge$ 10, UNLESS one knows \emph{a priori} that the noise is truly random, which then implies that 1/K is a relevant, small parameter
  \item The Variance Matrix:  see Equation \ref{eq:khrabrov_variance3}
  \item \textbf{Ensemble Average} $\equiv$ $\bigl<\bigl<$  $\bigr>\bigr>$ $\equiv$ average over the ensemble of all realizations of data
  \item \textbf{Average of Data} $\equiv$ $\bigl<$  $\bigr>$ $\equiv$ average of data in a given realization
\end{enumerate}

If we have a set of functions given by:

\begin{equation}
  \label{eq:khrabrov_fset1}
  \Bigl\{f_{j}\Bigr\} = \Bigl\{f\left(x_{j}\right)\Bigr\}
\end{equation}

and we let $x_{j}$ go to $\langle x \rangle + e_{j}$, then we have:

\begin{subequations}
 \begin{align}
  \bigl<f\bigr> & = \frac{1}{N} \sum_{j} f\left(x_{j}\right) \label{eq:khrabrov_fset2} \\
                & = \frac{1}{N} \sum_{j} f\left(\left<x\right> + e_{j}\right) \label{eq:khrabrov_fset3} \\
                & = f\left(\left<x\right>\right) + \frac{1}{N} f'\left(\left<x\right>\right) \sum_{j} e_{j} + \frac{1}{2N} f''\left(\left<x\right>\right) \sum_{j} \left(e_{j}\right)^{2} + \dotsc \label{eq:khrabrov_fset4} \\
                & = f\left(\left<x\right>\right) + \frac{\sigma^{2}}{2} f''\left(\left<x\right>\right) \label{eq:khrabrov_fset5}
 \end{align}
\end{subequations}

where $\sigma^{2}$ is defined by:

\begin{equation}
  \label{eq:khrabrov_variance1}
  \sigma^{2} \equiv \frac{1}{N} \sum_{i=1}^{N} \left(\textbf{x}_{i} - \langle \textbf{x}\rangle \right)^{2}
\end{equation}

thus, it can be shown that the fluctuations of two eigenvalues, treated as distinct and uncorrelated, have an the standard deviation of their difference, ($\lambda_{i}$ - $\lambda_{j}$), as: 

\begin{equation}
  \label{eq:khrabrov_stdev1}
  \sigma_{ij} = \sqrt{\left<\left< \bigl(\Delta \lambda_{i} \bigr)^{2} \right>\right> + \left<\left< \bigl(\Delta \lambda_{j} \bigr)^{2} \right>\right>}
\end{equation}

where, we have

\begin{equation}
\label{eq:khrabrov_stdev2}
  \left<\left< \bigl(\Delta \lambda_{i} \bigr)^{2} \right>\right> = \frac{2 \lambda_{3} \left(2 \lambda_{i} - \lambda_{3}\right)}{\left(K - 1\right)} \text{ .}
\end{equation}

The uncertainty in the vector, $\mathbf{x}_{1}$ (The maximum variance direction.), is then given by:

\begin{equation}
\label{eq:khrabrov_vecuncert1}
  \Delta \phi_{1j} = \sqrt{\frac{\lambda_{j} \lambda_{1}}{\left(K - 1\right) \left(\lambda_{1} - \lambda_{j}\right)} }
\end{equation}

if $\lambda_{2}$ $\ll$ $\lambda_{1}$ \textbf{AND} $\lambda_{3}$ $\ll$ $\lambda_{1}$.

\begin{equation}
\label{eq:khrabrov_variance2}
  Var(\textbf{B} \cdot \textbf{x}) \equiv \frac{1}{K} \sum_{k=1}^{K} \Bigl[ \left( \textbf{B}^{(k)} - \langle \textbf{B}\rangle \right) \cdot \textbf{x} \Bigr]^{2} \equiv \Bigl< \Bigl[ \left( \textbf{B}^{(k)} - \langle \textbf{B}\rangle \right) \cdot \textbf{x} \Bigr]^{2} \Bigr>
\end{equation}

\begin{equation}
\label{eq:khrabrov_variance3}
  M_{ij} = \Bigl< \Bigl( B_{i}^{(k)} - \langle B_{i}^{(k)} \rangle \Bigr) \Bigl( B_{j}^{(k)} - \langle B_{j}^{(k)} \rangle   \Bigr) \Bigr> \equiv \Bigl< \delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr>
\end{equation}

now replace $\mathbf{B}^{(k)}$ by $\mathbf{B}^{*(k)} + \delta \mathbf{b}^{(k)}$, where $\mathbf{B}^{*(k)}$ $\equiv$ signal and $\delta \mathbf{b}^{(k)}$ $\equiv$ noise.  One should note that $\mathbf{B}^{*(k)}$, k = 1, 2, $\dotsc$, K are the same in all realizations, while the K-offset noise components, $\delta \mathbf{b}^{(k)}$, contain $\bigl< \mathbf{b} \bigr>$, therefore are functions of all K noise vectors, $\mathbf{b}^{(k)}$, k = 1, 2, $\dotsc$,K, in the realization.  By definition, the latter has the property:

\begin{equation}
\label{eq:khrabrov_variance4}
  \Bigl<\Bigl< \textbf{b}^{(k)} \Bigr>\Bigr> \equiv 0  \Rightarrow \Bigl<\Bigl< \delta\textbf{b}^{(k)} \Bigr>\Bigr> \equiv 0
\end{equation}

which allows us to define the following:

\begin{equation}
\label{eq:khrabrov_variance5}
   \delta \textbf{B}^{(k)} = \delta \textbf{B}^{*(k)} +  \delta \textbf{b}^{(k)}
\end{equation}

where

\begin{subequations}
 \begin{align}
   \delta \textbf{B}^{*(k)} & \equiv \textbf{B}^{*(k)} - \bigl< \textbf{B}^{*} \bigr> \label{eq:khrabrov_definition1} \\
   \delta \textbf{b}^{(k)} & \equiv \textbf{b}^{(k)} - \bigl< \textbf{b} \bigr> \label{eq:khrabrov_definition2}
 \end{align}
\end{subequations}

 so that Equation \ref{eq:khrabrov_variance3} goes to:

\begin{subequations}
 \begin{align}
   \Bigl<\delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr> & = \Bigl<\bigl(\delta B_{i}^{*(k)} + \delta b_{i}^{(k)} \bigr) \bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> \label{eq:khrabrov_variance6} \\
   & = \Bigl<\delta B_{i}^{*(k)}\bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> + \Bigl<\delta b_{i}^{(k)}\bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> \label{eq:khrabrov_variance7} \\
   & = \Bigl<\delta B_{i}^{*(k)} \delta B_{j}^{*(k)} \Bigr> + \Bigl<\delta B_{i}^{*(k)} \delta b_{j}^{(k)} \Bigr> + \Bigl<\delta b_{i}^{(k)} \delta B_{j}^{*(k)} \Bigr> + \Bigl<\delta b_{i}^{(k)} \delta b_{j}^{(k)}\Bigr> = M_{ij} \label{eq:khrabrov_variance8} \text{ .}
 \end{align}
\end{subequations}

For the next step we have to realize that the following rule is valid:

\begin{equation}
\label{eq:khrabrov_definition3} 
   \Biggl< \Bigl[\bigl<\bigl< A \bigr>\bigr>\Bigr] \Biggr> = \Biggl<\Biggl< \Bigl[\bigl< A \bigr>\Bigr] \Biggr>\Biggr> \text{ .}
\end{equation}

If we take the \emph{ensemble average} of our variance matrix, we get:

\begin{subequations}
  \begin{align}
  \Biggl<\Biggl< \Bigl(\Delta M_{ij} \Bigr)^{2}\Biggr>\Biggr> & = \Biggl<\Biggl< \Bigl[M_{ij} + \bigl<\bigl<M_{ij}\bigr>\bigr> \Bigr]^{2}\Biggr>\Biggr> \label{eq:khrabrov_variance9} \\
  \begin{split}
  & = \Biggl<\Biggl< \Biggl\{ \frac{1}{K}\sum_{k} \Bigl( \delta B_{i}^{*(k)} + \delta b_{i}^{(k)} \Bigr) \Bigl( \delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \Bigr) - \\
  & \qquad \frac{1}{K}\sum_{m} \Bigl( \delta B_{i}^{*(m)}\delta B_{j}^{*(m)} + \bigl<\bigl<\delta b_{i} \delta b_{j} \bigr>\bigr> \Bigr) \Biggr\}^{2} \Biggr>\Biggr> \label{eq:khrabrov_variance10}
 \end{split}
  \end{align}
\end{subequations}

where the second term on the R.H.S. of Equation \ref{eq:khrabrov_variance9} is:

\begin{subequations}
  \begin{align}
    \Biggl<\Biggl< M_{ij}\Biggr>\Biggr> & = \Biggl<\Biggl< \Bigl[ \Bigl<\delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr> \Bigr]\Biggr>\Biggr> \label{eq:khrabrov_variance11} \\
    & = \Biggl<\Bigl[\Bigl<\Bigl< \delta B_{i}^{(k)} \delta B_{j}^{(k)}\Bigr>\Bigr>\Bigr]\Biggr> \label{eq:khrabrov_variance12} \\
    \begin{split}
    & = \Biggl<\Bigl[\Bigl<\Bigl<\delta B_{i}^{*(k)}\delta B_{j}^{*(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \Biggl<\Bigl[\Bigl<\Bigl< \delta B_{i}^{*(k)} \delta b_{j}^{(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \\
    & \qquad \Biggl<\Bigl[\Bigl<\Bigl< \delta b_{i}^{(k)} \delta B_{j}^{*(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \Biggl<\Bigl[\Bigl<\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Bigr>\Bigr]\Biggr>  \label{eq:khrabrov_variance13}
   \end{split}
  \end{align}
\end{subequations}

which is highly simplified by realizing that the middle two terms can be canceled when the ensemble average is taken due to the properties assumed in Equation \ref{eq:khrabrov_variance4}.  The first term on the R.H.S. is just defined as:

\begin{equation}
  \label{eq:khrabrov_variance14}
  M_{ij}^{*} \equiv \Biggl<\Biggl\{\Bigl<\Bigl<\delta B_{i}^{*(k)}\delta B_{j}^{*(k)} \Bigr>\Bigr>\Biggr\}\Biggr>
\end{equation}

which is the variance matrix of the nonfluctuating part of the field.  The final result is written as:

\begin{equation}
  \label{eq:khrabrov_variance15}
   \Biggl<\Biggl< M_{ij}\Biggr>\Biggr> = M_{ij}^{*} + \Biggl<\Biggl<\Biggl\{\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Biggr\}\Biggr>\Biggr> \text{ .}
\end{equation}

The second term on the R.H.S. of Equation \ref{eq:khrabrov_variance15} can be dealt with in the following manner:

\begin{subequations}
  \begin{align}
    \Biggl<\Biggl<\Biggl\{\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Biggr\}\Biggr>\Biggr> & = \Biggl<\Biggl<\Biggl\{\Bigl< \Bigl(b_{i}^{(k)} - \bigl<b_{i}^{(k)}\bigr> \Bigr) \Bigl(b_{j}^{(k)} - \bigl<b_{j}^{(k)}\bigr> \Bigr) \Bigr>\Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance16} \\
    & = \Biggl<\Biggl<\Biggl\{\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr> - \Bigl<b_{i}^{(k)} \bigl<b_{j}^{(k)}\bigr>\Bigr> - \Bigl<\bigl<b_{i}^{(k)}\bigr> b_{j}^{(k)}\Bigr> + \Bigl<\bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr>\Bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance17} \\
    & = \Biggl<\Biggl<\Biggl\{\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr> - \bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance18} \\
    & = \Biggl<\Biggl\{\Bigl<\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr>\Bigr>\Biggr\}\Biggr> - \Biggl<\Biggl<\Biggl\{ \bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance19} 
  \end{align}
\end{subequations}

The uncertainty in the direction between any two eigenvectors is given by:

\begin{equation}
\label{eq:khrabrov_dphi1}
  \Delta \phi_{i,j} = \pm \sqrt{ \left(\frac{\lambda_{3} (\lambda_{i} + \lambda_{j} - \lambda_{3})}{(K - 1)(\lambda_{i} - \lambda_{j})^{2}} \right)}
\end{equation}

with an uncertainty in eigenvalues given by:

\begin{equation}
\label{eq:khrabrov_dlam1}
  \Delta \lambda_{i} = \pm \sqrt{ \left(\frac{2 \lambda_{3} (2 \lambda_{i} - \lambda_{3})}{(K - 1)} \right)}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Trigonometric Identities
%%----------------------------------------------------------------------------------------
\subsection{Trigonometric Identities}  \label{subsec:trigidentities}
\noindent  The following are trigonometric identities for complex functions of $x$:
\begin{subequations}
  \begin{align}
    sinh \thickspace \left( \pm i x \right) & = \pm i \thickspace sin \thickspace x  \label{eq:trigidentities_0a}  \\
    cosh \thickspace \left( \pm i x \right) & = cos \thickspace x  \label{eq:trigidentities_0b}  \\
    tanh \thickspace \left( \pm i x \right) & = \pm i \thickspace tan \thickspace x  \label{eq:trigidentities_0c}  \\
    sinh \thickspace \left( y \pm i x \right) & = \pm i \thickspace cosh \left( y \right) \thickspace sin \left( x \right) + cos \left( x \right) \thickspace sinh \left( y \right)  \label{eq:trigidentities_0d}  \\
    cosh \thickspace \left( y \pm i x \right) & = cos \left( x \right) \thickspace cosh \left( y \right) \pm i \thickspace sin \left( x \right) \thickspace sinh \left( y \right)  \label{eq:trigidentities_0e}  \\
    tanh \thickspace \left( y \pm i x \right) & = \frac{ \pm i \thickspace cosh \left( y \right) \thickspace sin \left( x \right) + cos \left( x \right) \thickspace sinh \left( y \right) }{ cos \left( x \right) \thickspace cosh \left( y \right) \pm i \thickspace sin \left( x \right) \thickspace sinh \left( y \right) }  \label{eq:trigidentities_0f}
  \end{align}
\end{subequations}
where we can note that letting $i x$ $\rightarrow$ $z$ gives:
\begin{subequations}
  \begin{align}
    sinh \thickspace \left( y \pm z \right) & = cosh \left( z \right) \thickspace sinh \left( y \right) \pm cosh \left( y \right) \thickspace sinh \left( z \right)  \label{eq:trigidentities_1a}  \\
    cosh \thickspace \left( y \pm z \right) & = cosh \left( y \right) \thickspace cosh \left( z \right) \pm sinh \left( y \right) \thickspace sinh \left( z \right)  \label{eq:trigidentities_1b}  \\
    tanh \thickspace \left( y \pm z \right) & = \frac{ cosh \left( z \right) \thickspace sinh \left( y \right) \pm cosh \left( y \right) \thickspace sinh \left( z \right) }{ cosh \left( y \right) \thickspace cosh \left( z \right) \pm sinh \left( y \right) \thickspace sinh \left( z \right) }  \text{  .}  \label{eq:trigidentities_1c}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Taylor Series
%%----------------------------------------------------------------------------------------
\subsection{Taylor Series}  \label{subsec:taylorseries}
\noindent  The following are Taylor series expansions for general functions of $x$:
\begin{subequations}
  \begin{align}
    \sqrt{\frac{1}{1 + x^{2}}} & \approx 1 - \frac{x^{2}}{2} + \frac{3 x^{4}}{8} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0a}  \\
    \sqrt{\frac{1}{1 + \left(x/a\right)^{2}}} & \approx 1 - \frac{x^{2}}{2 a^{2}} + \frac{3 x^{4}}{8 a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0b}  \\
    \sqrt{1 + x^{2}} & \approx 1 + \frac{x^{2}}{2} - \frac{x^{4}}{8} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0c}  \\
    \sqrt{1 + \left(x/a\right)^{2}} & \approx 1 - \frac{x^{2}}{2 a^{2}} + \frac{x^{4}}{8 a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0d}  \\
    \sqrt{\frac{x^{2}}{1 + x^{2}}} & \approx x - \frac{x^{3}}{2} + \frac{3 x^{5}}{8} + \mathcal{O}\left( x^{7} \right)  \label{eq:series_0e}  \\
    \sqrt{\frac{1 + x^{2}}{x^{2}}} & \approx \frac{1}{x} + \frac{x}{2} - \frac{x^{3}}{8} + \mathcal{O}\left( x^{5} \right)  \label{eq:series_0f}  \\
    \frac{1}{1 + x^{2}} & \approx 1 - x^{2} + x^{4} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0g}  \\
    \frac{1}{1 + \left(x/a\right)^{2} } & \approx 1 - \frac{x^{2}}{a^{2}} + \frac{x^{4}}{a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0h}
  \end{align}
\end{subequations}
\noindent  The following are Taylor series expansions for exponential functions of $x$:
\begin{subequations}
  \begin{align}
    e^{ ^{\displaystyle \pm x } } & \approx 1 \pm x + \frac{ x^{2} }{ 2! } \pm \frac{ x^{3} }{ 3! } + \frac{ x^{4} }{ 4! } \pm \frac{ x^{5} }{ 5! } + \mathcal{O}\left( x^{6} \right)  \label{eq:series_1a}  \\
    e^{ ^{\displaystyle \pm i x } } & \approx 1 \pm \left( i x \right) - \frac{ x^{2} }{ 2! } \mp \frac{ i x^{3} }{ 3! } + \frac{ x^{4} }{ 4! } \pm \frac{ i x^{5} }{ 5! } + \mathcal{O}\left( x^{6} \right)  \label{eq:series_1b}  \\
    e^{ ^{\displaystyle \pm x^{2} } } & \approx 1 \pm x^{2} + \frac{ x^{4} }{ 2! } \pm \frac{ x^{6} }{ 3! } + \frac{ x^{8} }{ 4! } + \mathcal{O}\left( x^{10} \right)  \label{eq:series_1c}
  \end{align}
\end{subequations}
where we know that:
\begin{equation}
  \label{eq:series_1d}
  e^{ ^{\displaystyle \pm i x } } = \cos{x} \pm i \sin{x}  \text{  .}
\end{equation}
\noindent  The following are Taylor series expansions for trigonometric functions of $x$:
\begin{subequations}
  \begin{align}
    \sin{x} & \approx x - \frac{ x^{3} }{ 3! } + \frac{ x^{5} }{ 5! } - \frac{ x^{7} }{ 7! } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_2a}  \\
    \cos{x} & \approx 1 - \frac{ x^{2} }{ 2! } + \frac{ x^{4} }{ 4! } - \frac{ x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_2b}  \\
    \tan{x} & \approx x + \frac{ x^{3} }{ 3 } + \frac{ 2 x^{5} }{ 15 } + \frac{ 17 x^{7} }{ 315 } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_2c}  \\
    \sec{x} & \approx 1 + \frac{ x^{2} }{ 2! } + \frac{ 5 x^{4} }{ 4! } + \frac{ 61 x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_2d}  \\
    \csc{x} & \approx \frac{ 1 }{ x } + \frac{ x }{ 6 } + \frac{ 7 x^{3} }{ 360 } + \frac{ 31 x^{5} }{ 15120 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_2e}  \\
    \cot{x} & \approx \frac{ 1 }{ x } - \frac{ x }{ 3 } - \frac{ x^{3} }{ 45 } - \frac{ 2 x^{5} }{ 945 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_2f}
  \end{align}
\end{subequations}
\noindent  The following are Taylor series expansions for hyperbolic functions of $x$:
\begin{subequations}
  \begin{align}
    sinh \thickspace x & \approx x + \frac{ x^{3} }{ 3! } + \frac{ x^{5} }{ 5! } + \frac{ x^{7} }{ 7! } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_3a}  \\
    cosh \thickspace x & \approx 1 + \frac{ x^{2} }{ 2! } + \frac{ x^{4} }{ 4! } + \frac{ x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_3b}  \\
    tanh \thickspace x & \approx x - \frac{ x^{3} }{ 3 } + \frac{ 2 x^{5} }{ 15 } - \frac{ 17 x^{7} }{ 315 } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_3c}  \\
    sech \thickspace x & \approx 1 - \frac{ x^{2} }{ 2! } + \frac{ 5 x^{4} }{ 4! } - \frac{ 61 x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_3d}  \\
    csch \thickspace x & \approx \frac{ 1 }{ x } - \frac{ x }{ 6 } + \frac{ 7 x^{3} }{ 360 } - \frac{ 31 x^{5} }{ 15120 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_3e}  \\
    coth \thickspace x & \approx \frac{ 1 }{ x } + \frac{ x }{ 3 } - \frac{ x^{3} }{ 45 } + \frac{ 2 x^{5} }{ 945 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_3f}
  \end{align}
\end{subequations}


\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Variational Principle
%%----------------------------------------------------------------------------------------
\subsection{Variational Principle}  \label{subsec:VariationalPrinciple}
\indent  Recall that for an arbitrary function, $\mathcal{F}$ $=$ $\mathcal{F} \left( t, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, ..., x{\scriptstyle_{n - 1}}, x{\scriptstyle_{n}} \right)$, the \emph{exact derivative} or \emph{total derivative} is given by:
\begin{equation}
  \label{eq:varprincp_0a}
  \frac{ d \mathcal{F} }{ dt } = \frac{ \partial \mathcal{F} }{ \partial t } + \sum_{i = 1}^{n} \frac{ \partial \mathcal{F} }{ \partial x{\scriptstyle_{i}} } \thickspace \frac{ d x{\scriptstyle_{i}} }{ dt }  \text{   .}
\end{equation}
If we have $\mathcal{W}$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \textbf{x}, t \right)$, then the variation is given by:
\begin{equation}
  \label{eq:varprincp_1a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \omega, \textbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \textbf{x} } \cdot \delta \textbf{x} + \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \omega + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa}
\end{equation}
\indent  Now let us consider the dispersion relation, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \textbf{x}, t \right)$, then variation can be shown to be:
\begin{equation}
  \label{eq:varprincp_2a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \textbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \textbf{x} } \cdot \delta \textbf{x} + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Bessel Functions
%%----------------------------------------------------------------------------------------
\subsection{Bessel Functions}  \label{subsec:BesselFunctions}
\indent  If we define $\mathit{J}{\scriptstyle_{n}}$ and $\mathit{Y}{\scriptstyle_{n}}$ as Bessel functions of the first and second kind, respectively, and we let $\mathit{I}{\scriptstyle_{n}}$ and $\mathit{K}{\scriptstyle_{n}}$ be the modified Bessel functions of the first and second kind, respectively, then:
\begin{subequations}
  \begin{align}
    \mathit{J}{\scriptstyle_{n}}(x) & = \sum_{j=0}^{\infty} \frac{ \left( -1 \right)^{j} }{ j \! \left( n + j \right) \! } \left(\frac{x}{2}\right)^{2 j + n} \label{eq:stix62a_9} \\
    \mathit{Y}{\scriptstyle_{n}}(x) & = \frac{ \mathit{J}{\scriptstyle_{n}}(x) \cos\left( n \pi \right) - \mathit{J}{\scriptstyle_{-n}}(x) }{\sin\left( n \pi \right)}  \label{eq:stix62a_10} \\
    \mathit{I}{\scriptstyle_{n}}(x) & = \left( i \right)^{-n} \mathit{J}{\scriptstyle_{n}}(i x)  \label{eq:stix62a_11a} \\
    & = e^{-i n \pi/2} \mathit{J}{\scriptstyle_{n}}\left( x e^{i \pi/2} \right) \label{eq:stix62a_11b} \\
    & = \sum_{m=0}^{\infty} \frac{1}{ m! \left( m + \mid n \mid \right)! } \left( \frac{x}{2} \right)^{ 2 m + \mid n \mid } \label{eq:stix62a_11c}  \\
    \mathit{K}{\scriptstyle_{n}}(x) & = \frac{\pi}{2} \frac{\mathit{I}{\scriptstyle_{-n}}(x) - \mathit{I}{\scriptstyle_{n}}(x)}{\sin\left( n \pi \right)} \label{eq:stix62a_12}
  \end{align}
\end{subequations}

\indent  Some Bessel function relationships are:
\begin{subequations}
  \begin{align}
    \int_{0}^{\infty} \thickspace du \thickspace J{\scriptstyle_{n}}^{2}\left( au \right) \thickspace u \thickspace e^{ - \beta u^{2} } & = \frac{1}{ 2 \beta } e^{ -a^{2}/2 \beta } I{\scriptstyle_{n}}\left( \frac{ a^{2} }{ 2 \beta } \right)  \label{eq:bessel_0a}  \\
    e^{ i \left[ \alpha{\scriptstyle_{s}} \phi + \beta{\scriptstyle_{s}} \sin{\phi} \right] } & = \sum_{m=0}^{\infty} \thickspace J{\scriptstyle_{m}}\left( \beta{\scriptstyle_{s}} \right) e^{ i \left[ \alpha{\scriptstyle_{s}} + m \right] \phi } \label{eq:bessel_0d}  \\
    J{\scriptstyle_{n+1}}\left( x \right) + J{\scriptstyle_{n-1}}\left( x \right) & = \frac{ 2 n }{ x } J{\scriptstyle_{n}}\left( x \right) \label{eq:bessel_0e}  \\
    J{\scriptstyle_{n+1}}\left( x \right) - J{\scriptstyle_{n-1}}\left( x \right) & = -2 \frac{ d J{\scriptstyle_{n}}\left(x\right) }{ dx } \label{eq:bessel_0f}  \\
    \int_{\phi} \thickspace d\phi' \thickspace e^{-i \left[ \alpha{\scriptstyle_{s}} \phi' + \beta{\scriptstyle_{s}} \sin{\phi'} \right] } & = \sum_{n=0}^{\infty} \thickspace J{\scriptstyle_{n}}\left( \beta{\scriptstyle_{s}} \right) \int_{\phi} \thickspace d\phi' \thickspace e^{ -i \left[ \alpha{\scriptstyle_{s}} + n \right] \phi' } \label{eq:bessel_0g}  \\
    & = i \sum_{n=0}^{\infty} \thickspace \frac{ J{\scriptstyle_{n}}\left( \beta{\scriptstyle_{s}} \right) }{ \alpha{\scriptstyle_{s}} + n } e^{ -i \left[ \alpha{\scriptstyle_{s}} + n \right] \phi' } \label{eq:bessel_0h}  \\
    e^{ i x  \sin{\theta} } & = \sum_{ n = -\infty }^{\infty} \thickspace J{\scriptstyle_{n}}\left( x \right) e^{ i n \theta } \label{eq:bessel_0i}
  \end{align}
\end{subequations}
and we can also note that:
\begin{subequations}
  \begin{align}
    \int_{0}^{2 \pi} d\phi e^{i (m - n) \phi} & = 2 \pi \delta{\scriptstyle_{m,n}}  \label{eq:bessel_1a}  \\
    \lim_{x \rightarrow 0} n J{\scriptstyle_{n}}^{2}\left(x\right) & = 0  \label{eq:bessel_1b}
  \end{align}
\end{subequations}

\indent  Bessel function identities:
\begin{subequations}
  \begin{align}
    \sum_{ n = -\infty }^{\infty} \thickspace J{\scriptstyle_{n}}^{2}\left( x \right) & = 1  \label{eq:bessel_2a}  \\
    \sin{\phi} \sum_{n} \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi} & = - i \thickspace \sum_{n} \thickspace J'{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi}  \label{eq:bessel_2b}  \\
    \cos{\phi} \sum_{n} \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi} & = \sum_{n} \thickspace \frac{ n \Omega }{ k{\scriptstyle_{\perp}} V{\scriptstyle_{\perp}}  } \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi}  \label{eq:bessel_2c}  \\
    \sum_{n} \thickspace J{\scriptstyle_{n}}\left( x \right) \thickspace J'{\scriptstyle_{n}}\left( x \right) & = 0  \label{eq:bessel_2d}  \\
    \sum_{n} \thickspace n^{2} \thickspace J{\scriptstyle_{n}}^{2}\left( x \right) & = \frac{ x^{2} }{ 2 }  \label{eq:bessel_2e}
%%    J{\scriptstyle_{n}}\left( x \right) & = \frac{ 1 }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ \left( x \sin{ \theta } - n \theta \right) }  \label{eq:bessel_2f}  \\
%%    & = \frac{ i^{-n} }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ n \theta } \thickspace e^{ i x \cos{ \theta } }  \label{eq:bessel_2g}  \\
%%    I{\scriptstyle_{n}}\left( x \right) & = \frac{ 1 }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ n \theta } \thickspace e^{ x \cos{ \theta } }  \label{eq:bessel_2h}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  The Plasma Dispersion Function
%%----------------------------------------------------------------------------------------
\subsection{The Plasma Dispersion Function}  \label{subsec:plasmadispersion}
\indent  In this section, we will discuss the plasma dispersion function \citep[e.g.,][]{gurnett05}.

\noindent  If we let $\zeta{\scriptstyle_{s}}$ $=$ $\sqrt{m{\scriptstyle_{s}}/(2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})}$ (i p/k), then we define:

\begin{equation}
  \label{eq:pdispf_0}
  Z\left( \zeta \right) = \frac{ 1 }{ \sqrt{\pi} } \int_{C} \thickspace dz \thickspace \frac{ e^{ -z^{2} } }{ z - \zeta }
\end{equation}

\noindent  where the contour C is understood to be along the real z-axis, passing under the pole at z $=$ $\zeta$.  To alter this, we need to consider the Plemelj relation given by:

\begin{equation}
  \label{eq:pdispf_1}
  \lim_{\epsilon \rightarrow 0} \int_{-\infty}^{\infty} \thickspace dx \thickspace \frac{ f\left( x \right) }{ x - \left( x{\scriptstyle_{o}} \pm i \varepsilon \right) } = P \int_{-\infty}^{\infty} \thickspace dx \thickspace \frac{ f\left( x \right) }{ x - x{\scriptstyle_{o}} } \pm i\pi f\left( x{\scriptstyle_{o}} \right)
\end{equation}

\noindent  where $\epsilon$ $>$ 0 and the P refers to the principal value integral defined by:

\begin{equation}
  \label{eq:pdispf_2}
  P \int_{-\infty}^{\infty} ... dx = \lim_{\delta \rightarrow 0}[\int_{-\infty}^{x_{o} - \delta} ...dx + \int_{x_{o} + \delta}^{\infty} ...dx] .
\end{equation}

\noindent  We can define the derivative of the plasma dispersion function as:

\begin{equation}
  \label{eq:pdispf_3}
  \frac{ d Z }{ d \zeta } \equiv Z'\left( \zeta \right) = - 2 \left[ 1 + \zeta Z\left( \zeta \right) \right]
\end{equation}

\noindent  We can expand $Z\left( \zeta \right)$ and $Z'\left( \zeta \right)$ in the limits $\lvert \zeta \rvert$ $\gg$ 1 and $\lvert \zeta \rvert$ $\ll$ 1, which are given by:

\begin{subequations}
  \begin{align}
  Z\left( \zeta \right) & = i \sqrt{\pi} \frac{ k }{ \mid k \mid } e^{ - \zeta^{2} } - \left[ \frac{1}{ \zeta } + \frac{1}{ 2 \zeta^{3} } + \frac{3}{ 4 \zeta^{5} } + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\gg$ 1)}  \label{eq:pdispf_4a} \\
  Z\left( \zeta \right) & = i \sqrt{\pi} \frac{ k }{ \mid k \mid } e^{ - \zeta^{2} } - \left[ 2 \zeta - \frac{4}{3} \zeta^{3} + \frac{8}{15} \zeta^{5} + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\ll$ 1)} \label{eq:pdispf_4b} \\
  Z'\left( \zeta \right) & = -2 i \sqrt{\pi} \frac{ k }{ \mid k \mid } \zeta e^{ - \zeta^{2} } +  \left[ \frac{1}{ \zeta^{2} } + \frac{3}{ 2 \zeta^{4} } + \frac{15}{ 4 \zeta^{6} } + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\gg$ 1)} \label{eq:pdispf_4c} \\
  Z'\left( \zeta \right) & = -2 i \sqrt{\pi} \frac{ k }{ \mid k \mid } \zeta e^{ - \zeta^{2} } - \left[ 2 - 4 \zeta^{2} + \frac{8}{3} \zeta^{4} + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\ll$ 1)}  \label{eq:pdispf_4d}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Deriving the Quadratic Equation
%%----------------------------------------------------------------------------------------
\subsection{Deriving the Quadratic Equation} \label{subsec:BasicAlgebra}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: Basic Algebra
%%----------------------------------------------------------------------------------------
\subsubsection{Basic Algebra} \label{subsubsec:BasicAlgebra}

\indent  So before we get too ahead of ourselves, let us review a few important properties of algebraic manipulation.  The first is multiplication by one, which can be seen as:

\begin{subequations}
  \begin{align}
    a & = a \left(\frac{b}{b}\right) \label{eq:multone0}  \\
    \frac{1}{(x - y)} & = \frac{1}{\frac{x}{x}(x - y)} \label{eq:multone10}  \\
    & = \frac{1}{x (1 - \frac{y}{x})} \label{eq:multone11}  \\
    \frac{1}{(x - y)^{n}} & = \frac{1}{(\frac{x}{x})^{n}(x - y)^{n}} \label{eq:multone20}  \\
    & = \frac{1}{x^{n} (1 - \frac{y}{x})^{n}} \label{eq:multone21}
  \end{align}
\end{subequations}

\noindent  and so on and so forth.  The point is, there are a multitude of ways to multiply any factor by the number one.  Note that in the above examples, I am simply doing everything with arbitrary variables and for the anal retentive mathematicians, we'll assume that \textbf{ALL} of those variables are definite real numbers not equal to zero.

\indent  Now let's review a few properties of quotients.  The two most important ones to remember, at least in my mind, are the following:

\begin{subequations}
  \begin{align}
    \left(\frac{a}{\frac{b}{c}}\right) & = \left(\frac{a c}{b}\right)  \label{eq:quotient0}  \\
    \left(\frac{\frac{a}{b}}{c}\right) & =\left(\frac{a}{b c}\right)  \label{eq:quotient1}
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: Quadratic Equation
%%----------------------------------------------------------------------------------------
\subsubsection{Quadratic Equation} \label{subsubsec:QuadraticEquation}

\indent  If we let the following variables be defined as constants, \textit{a}, \textit{b}, \textit{c}, where \textit{a} $\neq$ 0 and \textit{b} and \textit{c} are $\in$ $\Re$\footnote{Note that $\in$ is one of the fancy mathematician ways of saying \textit{element of...} while $\Re$ pertains to the set of numbers known as \textit{Reals}.  Also, I should be careful to point out that the ONLY real requirement on any of the constants is that \textit{a} $\neq$ 0, but we'll throw in the real number thing to avoid imaginaries, which tend to obfuscate things.}.  Thus we start with a general second-order\footnote{the highest power of our undetermined variable, \textit{x}, is 2} polynomial equation of the form:

\begin{equation}
  \label{eq:quadratic0}
    a x^{2} + b x + c = 0
\end{equation}

\noindent  where our undetermined variable, \textit{x}, is the unknown we seek to solve for.  Now when one is faced with a general second-order polynomial that cannot be factored, it is typically useful to do something called \textit{completing the square}.  To do so, we first divide both sides of Equation \ref{eq:quadratic0} by \textit{a}\footnote{Were \textit{a} allowed even the slightest possibility to be $=$ 0, mathematicians would have their panties all in a bunch over this step...}:

\begin{subequations}
  \begin{align}
    a x^{2} + b x + c & = 0  \label{eq:quadratic1}  \\
    x^{2} + \left(\frac{b}{a}\right) x + \left(\frac{c}{a}\right) & = 0 \label{eq:quadratic2}
  \end{align}
\end{subequations}

\indent  Now the next step in completing the square requires that we move \textbf{ALL} terms with \textbf{ONLY} constants in them to the opposite side of the equation from that of our unknown variable, \textit{x}.  This changes Equation \ref{eq:quadratic2} to:

\begin{equation}
  \label{eq:quadratic3}
    x^{2} + \left(\frac{b}{a}\right) x = -\left(\frac{c}{a}\right)
\end{equation}

\noindent  and now are trying to change our original equation to something of the form of:  \\

\begin{equation}
  \label{eq:quadratic4}
    \left(x + B \right)^{2} = C
\end{equation}

\noindent  where \textit{B} and \textit{C} are \textbf{NOT} the same as their lower case counterparts.  To see this, we expand the left hand side of Equation \ref{eq:quadratic4} to find:

\begin{equation}
  \label{eq:quadratic5}
    \left(x + B \right)^{2} = x^{2} + 2 B x + B^{2}
\end{equation}

\noindent  where we see that there is a factor of 2 which must be taken into account.  So let's expand the following:

\begin{equation}
  \label{eq:quadratic6}
    \left(x + \left(\frac{b}{a}\right) \right)^{2} = x^{2} + 2 \left(\frac{b}{a}\right) x + \left(\frac{b}{a}\right)^{2}
\end{equation}

\noindent  which still leaves that pesky factor of 2 in our equation, so let's try a different approach.  Instead of Equation \ref{eq:quadratic6}, let's try the following:

\begin{equation}
  \label{eq:quadratic7}
    \left(x + \left(\frac{b}{2 a}\right) \right)^{2} = \left[x^{2} + \left(\frac{b}{a}\right) x \right] + \left(\frac{b}{2 a}\right)^{2}
\end{equation}

\noindent  where we can see the the terms in [ ] are the same as those on the left hand side of Equation \ref{eq:quadratic3}.  Thus, we rearrange Equation \ref{eq:quadratic7} in the following manner to find:

\begin{subequations}
  \begin{align}
    \left\{x + \left(\frac{b}{2 a}\right) \right\}^{2} - \left(\frac{b}{2 a}\right)^{2} & = \left[x^{2} + \left(\frac{b}{a}\right) x \right]  \label{eq:quadratic8}  \\
    & = -\left(\frac{c}{a}\right) \label{eq:quadratic9}
  \end{align}
\end{subequations}

\noindent  Thus we have the following equation of the following form:

\begin{equation}
  \label{eq:quadratic10}
    \left\{x + \left(\frac{b}{2 a}\right) \right\}^{2} - \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right] = 0
\end{equation}

\noindent  which allows us to see that \textit{B} and \textit{C} in Equation \ref{eq:quadratic4} are:

\begin{subequations}
  \begin{align}
    B & \equiv \left(\frac{b}{2 a}\right)  \label{eq:quadratic11} \\
    C & \equiv \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]  \label{eq:quadratic12}
  \end{align}
\end{subequations}

\noindent  Now we return to Equation \ref{eq:quadratic4} and solve for \textit{x} by first taking the square-root of both sides finding:

\begin{subequations}
  \begin{align}
    \left(x + B \right) & = \pm \sqrt{C}  \label{eq:quadratic13} \\
    x & = - B \pm \sqrt{C}  \label{eq:quadratic14}
  \end{align}
\end{subequations}

\noindent  and now substitute in our definitions of \textit{B} and \textit{C} from Equations \ref{eq:quadratic11} and \ref{eq:quadratic12} to find:

\begin{subequations}
  \begin{align}
    x & = - \left(\frac{b}{2 a}\right) \pm \sqrt{\left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]}  \label{eq:quadratic15}  \\
      & = - \left(\frac{b}{2 a}\right) \pm \sqrt{ \left(\frac{2 a}{2 a}\right)^{2} \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]} \label{eq:quadratic16}  \\
      & = - \left(\frac{b}{2 a}\right) \pm \left(\frac{1}{2 a}\right) \sqrt{b^{2} - \left(\frac{4 a^{2} c}{a}\right)} \label{eq:quadratic17}  \\
    x & = \frac{-b \pm \sqrt{b^{2} - 4 a c}}{2 a} \label{eq:quadratic18}
  \end{align}
\end{subequations}

\noindent  where Equation \ref{eq:quadratic18} is the commonly seen form of the general solution to any second-order polynomial of the form of Equation \ref{eq:quadratic0}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Numerical Integration
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Numerical Integration} \label{sec:NumericalIntegration}

\indent  In the following, we will discuss a specific numerical integration technique known as \emph{Simpson's 1/3 Rule}\footnote{The ``1/3'' part comes from a factor of 1/3 in the resulting expression.  There is also another method called \emph{Simpson's 3/8 Rule} that starts with a factor of, you guessed it, 3/8.}.  It can be derived in multiple ways.  Recall that the trapezoidal rule is based upon approximating a curve by a first order polynomial then integrating the polynomial over some interval $\left[ a, b \right]$.  Similarly, Simpson's 1/3 Rule is an extension of the trapezoidal rule, but here the integrand is approximated as a second order polynomial (e.g., $f\left( x \right)$ $\approx$ $a$ $+$ $b \ x$ $+$ $c \ x^{2}$).  Thus, one can implement the trapezoidal rule and solve for the unknown coefficients using the end points and the mid-point of the interval.  One can also derive Simpson's 1/3 Rule by starting with a second order polynomial approximated using Newton's divided difference polynomials.  Further, one could also derive Simpson's 1/3 Rule by using the \emph{Lagrange polynomial}, i.e., any method of three-point quadratic interpolation will work.  Finally, one can also use the \emph{method of coefficients} to derive Simpson's 1/3 Rule.  Some simple examples can be found at: \\

\noindent  \href{https://en.wikipedia.org/wiki/Simpson\%27s_rule}{https://en.wikipedia.org/wiki/Simpson\%27s\_rule} \\

\indent  Technically, what follows will be a discussion of the composite Simpson's 1/3 Rule that depends upon $N$ discrete, independent data points, where $N$ must be an odd number.

%%----------------------------------------------------------------------------------------
%%  Subsection:  Simpson's 1/3 Rule:  Regularly Gridded Data
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Simpson's 1/3 Rule:  Regularly Gridded Data} \label{subsec:Simpsons}

\noindent  \textbf{1D Integration:}  Suppose we have a set of $N$ discrete, independent data points given by $\mathbf{x}$ $=$ $\{ x{\scriptstyle_{0}}, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, \dotsc , x{\scriptstyle_{N - 1}} \}$ that are the abscissa of the function $f\left( \mathbf{x} \right)$, where we define $f{\scriptstyle_{j}}$ $\equiv$ $f\left( x{\scriptstyle_{j}} \right)$.  If the abscissa are evenly spaced, the quadratic interpolation approxation through any three points greatly simplifies and we have a numerical approximation for the integral of $f\left( \mathbf{x} \right)$ on the interval $\left[ a, b \right]$ given by:

\begin{equation}
  \label{eq:Simpsons_0}
  \int_{a}^{b} \ dx f\left( x \right) = \lim_{N \rightarrow \infty} S{\scriptstyle_{N}}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where we define $S{\scriptstyle_{N}}$ as:

\begin{equation}
  \label{eq:Simpsons_1}
  S{\scriptstyle_{N}} = \frac{ \Delta x }{ 3 } \sum_{j=0}^{T} \ s{\scriptstyle_{j}} \ f{\scriptstyle_{j}}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where $\Delta x$ $\equiv$ $\tfrac{ b - a }{ T }$, $T$ $\equiv$ $\left( N - 1 \right)$, and $s{\scriptstyle_{j}}$ are scale factors that satisfy:

\begin{subequations}
  \begin{align}
    s{\scriptstyle_{0}} & = 1 \label{eq:Simpsons_2a} \\
    s{\scriptstyle_{T}} & = 1 \label{eq:Simpsons_2b} \\
    s{\scriptstyle_{2 j + 1}} & = 4 \label{eq:Simpsons_2c} \\
    s{\scriptstyle_{2 j + 2}} & = 2 \label{eq:Simpsons_2d}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  This can be rewritten as follows:

\begin{equation}
  \label{eq:Simpsons_3}
  S{\scriptstyle_{N}} = \frac{ \Delta x }{ 3 } \left[ f{\scriptstyle_{0}} + 4 f{\scriptstyle_{1}} + 2 f{\scriptstyle_{2}} + 4 f{\scriptstyle_{3}} + 2 f{\scriptstyle_{4}} + \dotsc + 4 f{\scriptstyle_{T-1}} + f{\scriptstyle_{T}} \right]
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\indent  Fortunately, there is an analytical expression for the maximum possible error that expresses the difference between the continuous integral and the discrete sum approximation given by:

\begin{equation}
  \label{eq:Simpsons_4}
  E{\scriptstyle_{S}} \leq \frac{ M{\scriptstyle_{S}} \left( b - a \right)^{5} }{ 180 \ T^{4} }
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where $M{\scriptstyle_{S}}$ is the maximum value of $\lvert f^{\left( 4 \right)}\left( \mathbf{x} \right) \rvert$ and $f^{\left( n \right)}\left( x \right)$ $\equiv$ $\tfrac{ d^{n} }{ dx^{n} } f\left( x \right)$.  As an aside, in practical applications on real data, there are situations where the magnitude of $f\left( \mathbf{x} \right)$ can be such that small variations due to statistical fluctuations result in extremely large values of $M{\scriptstyle_{S}}$, i.e., noise dominated results.  Therefore, it is useful to remove outliers in $\lvert f^{\left( 4 \right)}\left( \mathbf{x} \right) \rvert$ prior to calculating $M{\scriptstyle_{S}}$.  The total error can be approximated by using an average instead of the maximum as well.

\noindent  \textbf{2D Integration:}  Things are similar in quadrature, or in two dimensions, where we now have two sets of $N$ discrete, independent data points given by $\mathbf{x}$ $=$ $\{ x{\scriptstyle_{0}}, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, \dotsc , x{\scriptstyle_{N - 1}} \}$ and $\mathbf{y}$ $=$ $\{ y{\scriptstyle_{0}}, y{\scriptstyle_{1}}, y{\scriptstyle_{2}}, \dotsc , y{\scriptstyle_{N - 1}} \}$ that are the abscissa of the function $f\left( \mathbf{x}, \mathbf{y} \right)$.  We define $f{\scriptstyle_{i,j}}$ $\equiv$ $f\left( x{\scriptstyle_{i}}, y{\scriptstyle_{j}} \right)$ and we want to integrate on the intervals $\left[ a, b \right]$ for $\mathbf{x}$ and $\left[ c, d \right]$ for $\mathbf{y}$.  Similar to the 1D case, we define $\Delta x$ $\equiv$ $\tfrac{ b - a }{ T }$ and further define $\Delta y$ $\equiv$ $\tfrac{ d - c }{ T }$.  Construction of the coefficients, $s{\scriptstyle_{i,j}}$, is similar, but in 2D, of course, where we have:

\begin{subequations}
  \begin{align}
    s{\scriptstyle_{0,0}} & = 1 \label{eq:Simpsons_5a} \\
    s{\scriptstyle_{T,T}} & = 1 \label{eq:Simpsons_5b} \\
    s{\scriptstyle_{i, 2 j + 1}} & = 4 * s{\scriptstyle_{i}} \label{eq:Simpsons_5c} \\
    s{\scriptstyle_{2 i + 1, j}} & = 4 * s{\scriptstyle_{j}} \label{eq:Simpsons_5d} \\
    s{\scriptstyle_{i, 2 j + 2}} & = 2 * s{\scriptstyle_{i}} \label{eq:Simpsons_5e} \\
    s{\scriptstyle_{2 i + 2, j}} & = 2 * s{\scriptstyle_{j}} \label{eq:Simpsons_5f}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  where we have used the $s{\scriptstyle_{k}}$ notation to refer to the 1D version of the coefficients defined in Equations \ref{eq:Simpsons_2a}--\ref{eq:Simpsons_2d}.  Note that the following is satisfied for these coefficients 1 $\leq$ $s{\scriptstyle_{i, k}}$ $\leq$ 16.  We can now define $S{\scriptstyle_{N}}$ in 2D as:

\begin{equation}
  \label{eq:Simpsons_6}
  S{\scriptstyle_{N}} = \frac{ \Delta x }{ 3 } \frac{ \Delta y }{ 3 } \sum_{i=0}^{T} \ \sum_{j=0}^{T} \ s{\scriptstyle_{i,j}} \ f{\scriptstyle_{i,j}}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  One can see that 4 $\leq$ $s{\scriptstyle_{i, k}}$ $\leq$ 16 for all $i$ and $k$ satisfying $\left( k \mod 2 \right) \neq 0$ (i.e., odd values of $k$).  Similarly, one can see that 1 $\leq$ $s{\scriptstyle_{i, 0}}$ $\leq$ 4 and 1 $\leq$ $s{\scriptstyle_{i, T}}$ $\leq$ 4 for all $i$.  Further, we can see that 2 $\leq$ $s{\scriptstyle_{i, k}}$ $\leq$ 8 for all $i$ and $k$ satisfying $\left( k \mod 2 \right) = 0$ (i.e., even values of $k$).  By construction, $s{\scriptstyle_{i, k}}$ is a symmetric tensor, thus $s{\scriptstyle_{i, k}}$ $=$ $s{\scriptstyle_{k, i}}$.

\noindent  \textbf{3D Integration:}  Adding a third dimension can make things slightly more complicated but it's mostly a variation on a theme at this point.  Now we have three sets of $N$ discrete, independent data points given by $\mathbf{x}$ $=$ $\{ x{\scriptstyle_{0}}, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, \dotsc , x{\scriptstyle_{N - 1}} \}$, $\mathbf{y}$ $=$ $\{ y{\scriptstyle_{0}}, y{\scriptstyle_{1}}, y{\scriptstyle_{2}}, \dotsc , y{\scriptstyle_{N - 1}} \}$, and $\mathbf{z}$ $=$ $\{ z{\scriptstyle_{0}}, z{\scriptstyle_{1}}, z{\scriptstyle_{2}}, \dotsc , z{\scriptstyle_{N - 1}} \}$ that are the abscissa of the function $f\left( \mathbf{x}, \mathbf{y}, \mathbf{z} \right)$.  We define $f{\scriptstyle_{i,j,k}}$ $\equiv$ $f\left( x{\scriptstyle_{i}}, y{\scriptstyle_{j}}, z{\scriptstyle_{k}} \right)$ and we want to integrate on the intervals $\left[ a, b \right]$ for $\mathbf{x}$, $\left[ c, d \right]$ for $\mathbf{y}$, and $\left[ e, f \right]$ for $\mathbf{z}$.  Similar to the 2D case, we define $\Delta x$ $\equiv$ $\tfrac{ b - a }{ T }$ and $\Delta y$ $\equiv$ $\tfrac{ d - c }{ T }$ but further define $\Delta z$ $\equiv$ $\tfrac{ f - e }{ T }$.  Construction of the coefficients, $s{\scriptstyle_{i,j,k}}$, is similar to 2D, of course, where we have:

\begin{subequations}
  \begin{align}
    s{\scriptstyle_{0,0,0}} & = 1 \label{eq:Simpsons_7a} \\
    s{\scriptstyle_{T,T,T}} & = 1 \label{eq:Simpsons_7b} \\
    s{\scriptstyle_{T,0,0}} & = 1 \label{eq:Simpsons_7c} \\
    s{\scriptstyle_{0,T,0}} & = 1 \label{eq:Simpsons_7d} \\
    s{\scriptstyle_{0,0,T}} & = 1 \label{eq:Simpsons_7e} \\
    s{\scriptstyle_{T,T,0}} & = 1 \label{eq:Simpsons_7f} \\
    s{\scriptstyle_{T,0,T}} & = 1 \label{eq:Simpsons_7g} \\
    s{\scriptstyle_{0,T,T}} & = 1 \label{eq:Simpsons_7h} \\
    s{\scriptstyle_{2 i + 1, j, k}} & = 4 * s{\scriptstyle_{j}} * s{\scriptstyle_{k}} \label{eq:Simpsons_7i} \\
    s{\scriptstyle_{2 i + 2, j, k}} & = 2 * s{\scriptstyle_{j}} * s{\scriptstyle_{k}} \label{eq:Simpsons_7j} \\
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  where we have again used the $s{\scriptstyle_{l}}$ notation to refer to the 1D version of the coefficients defined in Equations \ref{eq:Simpsons_2a}--\ref{eq:Simpsons_2d}.  Again, by construction, $s{\scriptstyle_{i, j, k}}$ is a symmetric tensor, thus $s{\scriptstyle_{i, j, k}}$ $=$ $s{\scriptstyle_{i, k, j}}$ $=$ $s{\scriptstyle_{j, k, i}}$ $=$ $s{\scriptstyle_{j, i, k}}$ $=$ $s{\scriptstyle_{k, i, j}}$ $=$ $s{\scriptstyle_{k, j, i}}$.  Note that the following is satisfied for these coefficients 1 $\leq$ $s{\scriptstyle_{i, j, k}}$ $\leq$ 64.  We can now define $S{\scriptstyle_{N}}$ in 3D as:

\begin{equation}
  \label{eq:Simpsons_8}
  S{\scriptstyle_{N}} = \frac{ \Delta x }{ 3 } \frac{ \Delta y }{ 3 } \frac{ \Delta z }{ 3 } \sum_{i=0}^{T} \ \sum_{j=0}^{T} \ \sum_{k=0}^{T} \ s{\scriptstyle_{i,j,k}} \ f{\scriptstyle_{i,j,k}}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Simpson's 1/3 Rule:  IDL 1D
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsubsection{Simpson's 1/3 Rule:  IDL 1D} \label{subsubsec:SimpsonsIDL1D}

\indent  To implement Simpson's 1/3 Rule in IDL in 1D and vectorize it is very straightforward when $\mathbf{x}$ are regularly spaced.  Defining $\Delta x$ is trivial, so we will only show how to define $s{\scriptstyle_{j}}$.  The approach is as follows: \\

\noindent  \verb+IDL> sc                    = REPLICATE(1d0,nx[0])+ \\
\noindent  \verb+IDL> sc[1:(nx[0] - 2L):2] *= 4d0+ \\
\noindent  \verb+IDL> sc[2:(nx[0] - 3L):2] *= 2d0+ \\
\noindent  \verb+IDL> sc[(nx[0] - 1L)]      = 1d0+ \\

\noindent  where $nx$ here is $N$ and $sc$ is $s{\scriptstyle_{j}}$.  Then the integration is really just a simple sum given by: \\

\noindent  \verb+IDL> output = TOTAL(h[0]*sc*f,/NAN)+ \\

\noindent  where $h$ is $\Delta x/3$ and $f$ is our $f\left( \mathbf{x} \right)$ array.  The $[0]$ used here is to ensure a scalar quantity as sometimes variables can unintentionally be defined as single-element arrays, which can cause concatination errors, among other things.

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Simpson's 1/3 Rule:  IDL 2D
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsubsection{Simpson's 1/3 Rule:  IDL 2D} \label{subsubsec:SimpsonsIDL2D}

\indent  Vectorizing Simpson's 1/3 Rule in IDL in 2D is a little trickier but not much.  Again, when $\mathbf{x}$ and $\mathbf{y}$ are regularly spaced, so it's trivial to define $\Delta x$ and $\Delta y$.  The approach to defining $s{\scriptstyle_{i,j}}$ starts the same as the 1D case described in Section \ref{subsubsec:SimpsonsIDL1D} but follows with: \\

\noindent  \verb+IDL> scx            = sc # REPLICATE(1d0,nx[0])+ \\
\noindent  \verb+IDL> scy            = REPLICATE(1d0,nx[0]) # sc+ \\
\noindent  \verb+IDL> scxy           = scx*scy+ \\

\noindent  Then the integration is really just a simple sum given by: \\

\noindent  \verb+IDL> output = TOTAL(h[0]*TOTAL(scxy*f,2,/NAN),/NAN)+ \\

\noindent  where $h$ is $\tfrac{ \Delta x \ \Delta y }{ 9 }$ and the summation is done over the second index then the first.

\indent  The call \verb+TOTAL(x,m[0],/NAN)+, the $n$-dimensional array $x$ is being summed over the $m^{th}$-dimension (for $m \leq n$).  In IDL, keywords specifying the dimension over which an operator is to be applied generally start indexing from one instead of zero.

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Simpson's 1/3 Rule:  IDL 3D
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsubsection{Simpson's 1/3 Rule:  IDL 3D} \label{subsubsec:SimpsonsIDL3D}

\indent  Vectorizing Simpson's 1/3 Rule in IDL in 3D is a little trickier still, but again not much more so than 2D.  Again, when $\mathbf{x}$, $\mathbf{y}$, and $\mathbf{z}$ are regularly spaced, so it's trivial to define $\Delta x$, $\Delta y$, and $\Delta z$.  The approach to defining $s{\scriptstyle_{i,j,k}}$ starts the same as the 1D case described in Section \ref{subsubsec:SimpsonsIDL1D} but follows with: \\

\noindent  \verb+IDL> scx            = sc # REPLICATE(1d0,nx[0])+ \\
\noindent  \verb+IDL> scx3d          = REBIN(scx,nx[0],nx[0],nx[0])+ \\
\noindent  \verb+IDL> scy3d          = TRANSPOSE(scx3d,[2,0,1])+ \\
\noindent  \verb+IDL> scz3d          = TRANSPOSE(scy3d,[0,2,1])+ \\
\noindent  \verb+IDL> scxyz          = scx3d*scy3d*scz3d+ \\

\noindent  Then the integration is really just a simple sum given by: \\

\noindent  \verb+IDL> output = h[0]*TOTAL(TOTAL(TOTAL(scxyz*f,3,/NAN),2,/NAN),/NAN)+ \\

\noindent  where $h$ is $\tfrac{ \Delta x \ \Delta y \ \Delta z }{ 27 }$ and the summation is done over the third index, then the second, then the first.

\indent  The call to \verb+TRANSPOSE(x,[l,m,n])+ takes the array $x$ and reorders the dimensions such that the $l^{th}$-dimension is now first, the $m^{th}$-dimension is now second, and the $n^{th}$-dimension is now third.  Suppose, for example, that the $l^{th}$-dimension has 10-elements, the $m^{th}$-dimension has 15-elements, and the $n^{th}$-dimension has 3-elements, then the output of \verb+TRANSPOSE(x,[l,m,n])+ would be a [10,15,3]-element array.  So if $x$ were originally a [3,15,10]-element array, then the call would be given by \verb+TRANSPOSE(x,[2,1,0])+ to convert it to a [10,15,3]-element array.

\indent  The call to \verb+REBIN(x,n,n,n)+ takes the originally 2D $x$, which had [n,n]-elements, and creates a new 3D array with [n,n,n]-elements.  The original expansion of \verb+scx+ to a 2D array using the \verb+#+ operator with an array of ones takes the \verb+sc+ array and copies the values into the second dimension.  That is, the values of \verb+scx[i,*]+ are all the same such that \verb+scx[i,*] = scx[j,*]+ for 0 $\leq$ $i$ $\leq$ $n - 1$ and 0 $\leq$ $j$ $\leq$ $n - 1$.  Similarly, the values of \verb+scx3d[i,j,*]+ are all the same as well.

%%----------------------------------------------------------------------------------------
%%  Subsection:  Simpson's 1/3 Rule:  Irregularly Gridded Data
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Simpson's 1/3 Rule:  Irregularly Gridded Data} \label{subsec:SimpsonsIrr}

\noindent  \textbf{1D Integration:}  Suppose we have a set of $N$ discrete, independent data points given by $\mathbf{x}$ $=$ $\{ x{\scriptstyle_{0}}, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, \dotsc , x{\scriptstyle_{N - 1}} \}$ that are the abscissa of the function $f\left( \mathbf{x} \right)$, where we define $f{\scriptstyle_{j}}$ $\equiv$ $f\left( x{\scriptstyle_{j}} \right)$.  If the abscissa are not evenly spaced, then we define the $i^{th}$ spacing as $h{\scriptstyle_{i}}$ $\equiv$ $x{\scriptstyle_{i+1}}$ $-$ $x{\scriptstyle_{i}}$ and the number of intervals as $T$ $\equiv$ $\left( N - 1 \right)$.  We can then define the numerical integral approximation as:

\begin{equation}
  \label{eq:Simpsons_9}
  S{\scriptstyle_{N}} = \sum_{j=0}^{T/2 - 1} \left\{ \frac{ h{\scriptstyle_{2 i}} + h{\scriptstyle_{2 i + 1}} }{ 6 } \left[ \left( 2 - \frac{ h{\scriptstyle_{2 i + 1}} }{ h{\scriptstyle_{2 i}} } \right) f{\scriptstyle_{2 i}} + \frac{ \left( h{\scriptstyle_{2 i}} + h{\scriptstyle_{2 i + 1}} \right)^{2} }{ h{\scriptstyle_{2 i}} \ h{\scriptstyle_{2 i + 1}} } f{\scriptstyle_{2 i + 1}} + \left( 2 - \frac{ h{\scriptstyle_{2 i}} }{ h{\scriptstyle_{2 i + 1}} } \right) f{\scriptstyle_{2 i + 2}} \right] \right\}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  \textbf{2D Integration:}  Extrapolation to 2D\footnote{I think this is correct but still need to check it...} can be treated in the following manner where we define:

\begin{equation}
  \label{eq:Simpsons_10}
  \begin{split}
    \mathcal{H}{\scriptstyle_{i,j}} & \equiv \Biggl\{ \frac{ h{\scriptstyle_{l,m}} + h{\scriptstyle_{l + 1, m + 1}} }{ 6 } \biggl[ \left( 2 - \frac{ h{\scriptstyle_{l + 1, m + 1}} }{ h{\scriptstyle_{l,m}} } \right) f{\scriptstyle_{l,m}} + \frac{ \left( h{\scriptstyle_{l,m}} + h{\scriptstyle_{l + 1, m + 1}} \right)^{2} }{ h{\scriptstyle_{l,m}} \ h{\scriptstyle_{l + 1, m + 1}} } f{\scriptstyle_{l + 1, m + 1}} \\
    & + \left( 2 - \frac{ h{\scriptstyle_{l,m}} }{ h{\scriptstyle_{l + 1, m + 1}} } \right) f{\scriptstyle_{l + 2, m + 2}} \biggr] \Biggr\}
  \end{split}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where we have assumed $l$ $=$ $2 i$, $m$ $=$ $2 j$, $f{\scriptstyle_{i,j}}$ $\equiv$ $f\left( x{\scriptstyle_{i}}, y{\scriptstyle_{j}} \right)$, and we used:

\begin{equation}
  \label{eq:Simpsons_11}
  h{\scriptstyle_{k,n}} \equiv \left( x{\scriptstyle_{k+1}} - x{\scriptstyle_{k}} \right) \left( x{\scriptstyle_{n+1}} - x{\scriptstyle_{n}} \right)
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}



\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Velocity Distribution Functions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Velocity Distribution Functions} \label{sec:VelocityDistributionFunctions}

\indent  Note that these details are published in the supplemental information of for the publications by \citet{wilsoniii19a}, \citet{wilsoniii19b}, and \citet{wilsoniii20a}.  The reference to the supplemental information is \citet{wilsoniii19k}.

\indent  Let us define a generalized Gaussian probability density function as:

\begin{equation}
  \label{eq:gauss_0}
  f{\scriptstyle_{s}}\left( x \right) = \frac{ A{\scriptstyle_{o}} }{\sqrt{ 2 \pi \sigma^{2} }} \ e^{^{\displaystyle - \frac{ (x - x{\scriptstyle_{o}})^{2} }{ 2 \sigma^{2} } }}
\end{equation}

\noindent  where $x{\scriptstyle_{o}}$ is the displacement of the peak from x $=$ 0, $A{\scriptstyle_{o}}$ is a normalization amplitude, $s$ denotes the specific set (later used for particle species) of data the distribution describes, and $\sigma^{2}$ is the variance (e.g., see Section \ref{subsubsec:variance}).  For this distribution, one can find the Full Width at Half Maximum (FWHM) as:

\begin{equation}
  \label{eq:gauss_1}
  FWHM = 2 \sqrt{ 2 \ln 2 } \ \sigma
\end{equation}

\noindent  which is an expression for the width of the distribution at half its peak value.  If we change x $\rightarrow$ v, where v is a velocity, then the distribution in Equation \ref{eq:gauss_0} is now referred to as a Maxwell-Boltzmann distribution, or Maxwellian.  A one dimensional Maxwellian is given by:

\begin{equation}
  \label{eq:gauss_2}
  f{\scriptstyle_{s}}\left( v \right) = \frac{ n{\scriptstyle_{o}} }{ \sqrt{\pi} \ V{\scriptscriptstyle_{T_{s}}} } \ e^{^{\displaystyle - \left( \frac{ v - v{\scriptstyle_{o}} }{ V{\scriptscriptstyle_{T_{s}}} } \right)^{2} }}
\end{equation}

\noindent  where $v{\scriptstyle_{o}}$ is the drift speed of the peak relative to zero, $n{\scriptstyle_{o}}$ is the particle number density, and we have replaced 2$\sigma^{2}$ with $V{\scriptstyle_{T_{s}}}^{2}$, the thermal speed\footnote{Note that this version is referred to as the \emph{most probable speed} for a 1D Gaussian.}, which is given by:

\begin{equation}
  \label{eq:gauss_3}
  V{\scriptstyle_{T_{s}}} = \sqrt{ \frac{ 2 k{\scriptstyle_{B}} T{\scriptstyle_{s}} }{ m{\scriptstyle_{s}} } }
\end{equation}

\noindent  where $k{\scriptstyle_{B}}$ is Boltzmann's constant, $T{\scriptstyle_{s}}$ is the temperature, $m{\scriptstyle_{s}}$ is the mass, and $s$ is the particle species.  Therefore, one can show that FWHM $=$ 2$\sqrt{\ln 2}$ $V{\scriptstyle_{T_{s}}}$.

\indent  The general representation of a two dimensional multivariate distribution is given by the following:

\begin{subequations}
  \begin{align}
    f\left( x, y \right) & = \alpha \ e^{^{\displaystyle - \left( \frac{ \beta }{ \sqrt{ 2 \left( 1 - \rho^{2} \right) } } \right)^{2} }}  \label{eq:gauss_4a} \\
    \alpha & = \frac{ A{\scriptstyle_{o}} }{ 2 \pi \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} \sqrt{1 - \rho^{2}} }  \label{eq:gauss_4b} \\
    \beta^{2} & = \left[ \left(\frac{ x - x{\scriptstyle_{o}} }{ \sigma{\scriptstyle_{x}} }\right)^{2} + \left(\frac{ y - y{\scriptstyle_{o}} }{ \sigma{\scriptstyle_{y}} }\right)^{2} - \left( \frac{ 2 \rho (x - x{\scriptstyle_{o}}) (y - y{\scriptstyle_{o}}) }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} } \right) \right]  \label{eq:gauss_4c}
  \end{align}
\end{subequations}

\noindent  where we define $\rho$ and $\sigma{\scriptstyle_{j}}$ in the following manner:

\begin{subequations}
  \begin{align}
    \rho & = \frac{ cov\left( x, y \right) }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} }  \label{eq:gauss_5a} \\
    cov\left( x, y \right) & = E\left[ \left( x - \mu{\scriptstyle_{x}} \right) \left( y - \mu{\scriptstyle_{y}} \right) \right]  \label{eq:gauss_5b} 
  \end{align}
\end{subequations}
\noindent  where $\rho$ is the correlation between x and y, $\mu{\scriptstyle_{x}}$ $=$ E[x] is the expected value of the aggregate data set X $=$ $\cup{\scriptstyle_{i}}$ $x{\scriptstyle_{i}}$, and $\mu{\scriptstyle_{x_{i}}}$ (e.g., see Section \ref{subsec:meanvarcovar}).  In the limit $\rho$ $\rightarrow$ 0 (i.e., x and y are uncorrelated), Equation \ref{eq:gauss_4b} reduces to:

\begin{equation}
  \label{eq:gauss_6}
  f\left( x, y \right) = \frac{ A{\scriptstyle_{x}} A{\scriptstyle_{y}} }{ 2 \pi \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} } \ e^{^{\displaystyle - \frac{1}{2} \left[ \left(\frac{ x - \mu{\scriptstyle_{x}} }{ \sigma{\scriptstyle_{x}} }\right)^{2} + \left(\frac{ y - \mu{\scriptstyle_{y}} }{ \sigma{\scriptstyle_{y}} }\right)^{2} \right] }} \text{  .}
\end{equation}

\noindent  In general, for uncorrelated variables, we can write:

\begin{equation}
  \label{eq:gauss_7}
  f\left( x, y, z \right) = f\left( x \right) f\left( y \right) f\left( z \right)
\end{equation}

\noindent  but if we let $V{\scriptstyle_{x}}$ $\rightarrow$ $V{\scriptstyle_{\perp}}$ $\cos{\phi}$, $V{\scriptstyle_{y}}$ $\rightarrow$ $V{\scriptstyle_{\perp}}$ $\sin{\phi}$, and $V{\scriptstyle_{z}}$ $\rightarrow$ $V{\scriptstyle_{\parallel}}$, where $\phi$ is the phase angle of the velocity and $\partial f/\partial \phi$ $=$ 0, the distribtuion is gyrotropic.  

%%----------------------------------------------------------------------------------------
%%  Subsection:  Bi-Maxwellian Distributions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Bi-Maxwellian Distributions} \label{subsec:bimaxwellian}

\indent  One can show that a gyrotropic distribution satisfies $V{\scriptstyle_{T \perp, x}}$ $=$ $V{\scriptstyle_{T \perp, y}}$ $\equiv$ $V{\scriptstyle_{T \perp}}$.  If we substitute into Equation \ref{eq:gauss_6} x $\rightarrow$ $V{\scriptstyle_{\parallel}}$, y $\rightarrow$ $V{\scriptstyle_{\perp}}$, $\mu{\scriptstyle_{j}}$ $\rightarrow$ $V{\scriptstyle_{o, j}}$, and $\sigma{\scriptstyle_{j}}$ $\rightarrow$ $V{\scriptstyle_{T, j}} / \sqrt{2}$ we arrive at a bi-Maxwellian distribution given by:

\begin{equation}
  \label{eq:gauss_8}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \frac{ n{\scriptstyle_{o}} }{ \pi^{3/2} V{\scriptstyle_{T \perp}}^{2} V{\scriptstyle_{T \parallel}} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{2} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{2} \right]}}
\end{equation}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Derivatives of Parameters: Bi-Maxwellian Distributions
%%----------------------------------------------------------------------------------------
\subsubsection{Derivatives of Parameters: Bi-Maxwellian Distributions} \label{subsubsec:bimaxwellian}

\indent  In the use of numerical methods like the Levenberg-Marquardt algorithm \citep[e.g.,][]{markwardt09a}, it is useful to define the derivatives of a function with respect to the free parameters.  In the case of velocity distributions, these are the density, thermal speeds, drift speeds, and exponent (for self-similar and kappa distributions).  First, we define some simplifying terms for brevity.  Let us define the following:

\begin{subequations}
  \begin{align}
    u{\scriptstyle_{j}} & = V{\scriptstyle_{j}} - v{\scriptstyle_{o j}} \label{eq:deriv_mdf0} \\
    w{\scriptstyle_{j}} & = \frac{ u{\scriptstyle_{j}} }{ V{\scriptstyle_{T j}} } \label{eq:deriv_mdf1} \\
    \psi\left( z \right) & = \frac{ \Gamma'\left( z \right) }{ \Gamma\left( z \right) } \equiv \text{digamma function} \label{eq:deriv_mdf2}
  \end{align}
\end{subequations}

\noindent  Now we can proceed and define the partial derivatives of Equation \ref{eq:gauss_8} which we will denote as $f^{(m)}$ for brevity:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ f^{(m)} } \frac{ \partial f^{(m)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:deriv_mdf3} \\
    \frac{ 1 }{ f^{(m)} } \frac{ \partial f^{(m)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left[ \frac{ 2 \ w{\scriptstyle_{\parallel}}^{2} - 1 }{ V{\scriptstyle_{T \parallel}} } \right] \label{eq:deriv_mdf4} \\
    \frac{ 1 }{ f^{(m)} } \frac{ \partial f^{(m)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left[ \frac{ 2 \ \left( w{\scriptstyle_{\perp}}^{2} - 1 \right) }{ V{\scriptstyle_{T \perp}} } \right] \label{eq:deriv_mdf5} \\
    \frac{ 1 }{ f^{(m)} } \frac{ \partial f^{(m)} }{ \partial v{\scriptstyle_{o \parallel}} } & = \left( \frac{ 2 \ w{\scriptstyle_{\parallel}} }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:deriv_mdf6} \\
    \frac{ 1 }{ f^{(m)} } \frac{ \partial f^{(m)} }{ \partial v{\scriptstyle_{o \perp}} } & = \left( \frac{ 2 \ w{\scriptstyle_{\perp}} }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:deriv_mdf7}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsection:  Bi-Kappa Distributions
%%----------------------------------------------------------------------------------------
\subsection{Bi-Kappa Distributions} \label{subsec:bikappa}

\indent  A generalized power-law particle distribution is given by a bi-kappa distribution \citep[e.g.,][]{livadiotis15a, mace10a}, for electrons here as:

\begin{equation}
  \label{eq:bikappa_0}
  f\left( V{\scriptstyle_{\perp}}, V{\scriptstyle_{\parallel}} \right) = \left[ \frac{ 1 }{ \pi \left( \kappa - \tfrac{3}{2} \right) } \right]^{3/2} \frac{ n{\scriptstyle_{o}} \ \Gamma\left( \kappa + 1 \right) }{ V{\scriptstyle_{T \perp}}^{2} \ V{\scriptstyle_{T \parallel}} \ \Gamma\left( \kappa - \tfrac{1}{2} \right) } \left\{ 1 + \frac{ 1 }{ \left( \kappa - \tfrac{3}{2} \right) } \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{2} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{2} \right]  \right\}^{-( \kappa + 1 )}
\end{equation}

\noindent  Note that we have again defined $V{\scriptstyle_{T j}}$ as the most probable speed of a 1D Gaussian for consistency, i.e., it does not depend upon $\kappa$.

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Derivatives of Parameters: Bi-Kappa Distributions
%%----------------------------------------------------------------------------------------
\subsubsection{Derivatives of Parameters: Bi-Kappa Distributions} \label{subsubsec:bikappa}

\indent  In the use of numerical methods like the Levenberg-Marquardt algorithm \citep[e.g.,][]{markwardt09a}, it is useful to define the derivatives of a function with respect to the free parameters.  In the case of velocity distributions, these are the density, thermal speeds, drift speeds, and exponent (for self-similar and kappa distributions).  First, we define some simplifying terms for brevity.  Let us define the following:

\begin{subequations}
  \begin{align}
    u{\scriptstyle_{j}} & = V{\scriptstyle_{j}} - v{\scriptstyle_{o j}} \label{eq:deriv_kdf0} \\
    w{\scriptstyle_{j}} & = \frac{ u{\scriptstyle_{j}} }{ V{\scriptstyle_{T j}} } \label{eq:deriv_kdf1} \\
    \psi\left( z \right) & = \frac{ \Gamma'\left( z \right) }{ \Gamma\left( z \right) } \equiv \text{digamma function} \label{eq:deriv_kdf2} \\
    D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) & = w{\scriptstyle_{\parallel}}^{2} + w{\scriptstyle_{\perp}}^{2} + \left( \kappa - \tfrac{3}{2} \right) \label{eq:deriv_kdf30}
  \end{align}
\end{subequations}

\noindent  Now we can proceed and define the partial derivatives of Equation \ref{eq:bikappa_0} which we will denote as $f^{(\kappa)}$ for brevity:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:deriv_kdf3} \\
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left[ \frac{ 2 \ w{\scriptstyle_{\parallel}}^{2} \ \left( \kappa + \tfrac{1}{2} \right) - w{\scriptstyle_{\perp}}^{2} - \left( \kappa - \tfrac{3}{2} \right) }{ V{\scriptstyle_{T \parallel}} \ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) } \right] \label{eq:deriv_kdf4} \\
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left\{ \frac{ 2 \ \left[ \kappa \ w{\scriptstyle_{\perp}}^{2} - w{\scriptstyle_{\parallel}}^{2} - \left( \kappa - \tfrac{3}{2} \right) \right] }{ V{\scriptstyle_{T \perp}} \ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) } \right\} \label{eq:deriv_kdf5} \\
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial V{\scriptstyle_{o \parallel}} } & = \left[ \frac{ 2 \ w{\scriptstyle_{\parallel}} \ \left( \kappa + 1 \right) }{ V{\scriptstyle_{T \parallel}} \ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) } \right] \label{eq:deriv_kdf6} \\
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial V{\scriptstyle_{o \perp}} } & = \left[ \frac{ 2 \ w{\scriptstyle_{\perp}} \ \left( \kappa + 1 \right) }{ V{\scriptstyle_{T \perp}} \ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) } \right] \label{eq:deriv_kdf7} \\
    \frac{ 1 }{ f^{(\kappa)} } \frac{ \partial f^{(\kappa)} }{ \partial \kappa } & = \left\{ \frac{ \left( w{\scriptstyle_{\parallel}}^{2} + w{\scriptstyle_{\perp}}^{2} \right) \left( \kappa - \tfrac{1}{2} \right) - \tfrac{3}{2} \ \left( \kappa - \tfrac{3}{2} \right) }{ \left( \kappa - \tfrac{3}{2} \right) \ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) } - \ln{ \lvert \frac{ D\left( w{\scriptstyle_{\parallel}}, w{\scriptstyle_{\perp}}, \kappa \right) }{ \left( \kappa - \tfrac{3}{2} \right) } \rvert } + \psi\left( \kappa + 1 \right) - \psi\left( \kappa - \tfrac{1}{2} \right) \right\} \label{eq:deriv_kdf8}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsection:  Self-Similar Distributions
%%----------------------------------------------------------------------------------------
\subsection{Self-Similar Distributions} \label{subsec:selfsimilar}

\indent  When a distribution evolves under the action of inelastic scattering, the result is a \emph{self-similar distribution} \citep{dum74a, dum75a, goldman84a, horton76a, horton79a, jain79a}, which in one dimension is given by:

\begin{equation}
  \label{eq:app4_0}
  f{\scriptstyle_{s}}\left( x, t \right) = C{\scriptstyle_{o}} \ e^{^{\displaystyle - \left(\frac{ x }{ x{\scriptstyle_{o}} }\right)^{p} }}
\end{equation}

\noindent  where we define $C{\scriptstyle_{o}}$ by defining:

\begin{subequations}
  \begin{align}
    n{\scriptstyle_{o}} & = \int_{-\infty}^{\infty} dv \ f{\scriptstyle_{s}}\left( v, t \right)  \label{eq:app4_1a}  \\
    & = 2 \int_{0}^{\infty} dv \ f{\scriptstyle_{s}}\left( v, t \right) \text{  (if symmetric)}  \label{eq:app4_1b}
  \end{align}
\end{subequations}

\noindent  where the general solution to Equation \ref{eq:app4_1b} is given by:

\begin{equation}
  \label{eq:app4_2}
  \int_{0}^{\infty} dx \ x^{n} e{^{\displaystyle - \alpha x^{p}}} = \frac{ \Gamma\left( k \right) }{ p \alpha^{k} }
\end{equation}

\noindent  for $n$ $>$ -1, $p$ $>$ 0, $\alpha$ $>$ 0, and $k$ $=$ ($n$ $+$ 1)/$p$\footnote{Note that $\Gamma \left(1/p\right)/p$ $=$ $\Gamma \left( 1 + 1/p \right)$}.  For $n$ $=$ 0, we can show:

\begin{equation}
  \label{eq:app4_3}
  C{\scriptstyle_{o}} = \frac{{\displaystyle n{\scriptstyle_{o}} \ p \ \alpha^{1/p} }}{{\displaystyle 2 \ \Gamma\left( 1/p \right) }}
\end{equation}

\noindent  Physically we can see that $\alpha$ $\rightarrow$ $V{\scriptstyle_{T_{s}}}^{-p}$, therefore the one dimensional form of the self-similar distribution can be given by:

\begin{equation}
  \label{eq:app4_4}
  f{\scriptstyle_{s}}\left( v, t \right) = \frac{{\displaystyle n{\scriptstyle_{o}} \ p }}{{\displaystyle 2 \ V{\scriptstyle_{T_{s}}} \ \Gamma\left( 1/p \right) }} \ e^{^{\displaystyle - \left(\frac{ v }{ V{\scriptstyle_{T_{s}}} }\right)^{p} }}
\end{equation}

\noindent  which in the limit as $p$ $\rightarrow$ 2 reduces to:

\begin{equation}
  \label{eq:app4_5}
  f{\scriptstyle_{s}}\left( v, t \right) = \frac{{\displaystyle n{\scriptstyle_{o}} }}{{\displaystyle \sqrt{\pi} \ V{\scriptstyle_{T_{s}}} }} \ e^{^{\displaystyle - \left(\frac{ v }{ V{\scriptstyle_{T_{s}}} }\right)^{2} }}
\end{equation}

\noindent  which matches Equation \ref{eq:gauss_2} for $v{\scriptstyle_{o}}$ $\rightarrow$ 0.

\indent  For the 3D case, the self-similar solution reduces to (for even $p$):

\begin{equation}
  \label{eq:app4_6}
  f\left( V{\scriptstyle_{x}}, V{\scriptstyle_{y}}, V{\scriptstyle_{z}} \right) = \left[ \frac{ p }{{\displaystyle 2 \Gamma\left( \frac{n+1}{p} \right) }} \right]^{3} \frac{ n{\scriptstyle_{o}} }{ \left( V{\scriptstyle_{T_{x}}} \ V{\scriptstyle_{T_{y}}} \ V{\scriptstyle_{T_{z}}} \right)^{n+1} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{x}} }{ V{\scriptstyle_{T_{x}}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{y}} }{ V{\scriptstyle_{T_{y}}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{z}} }{ V{\scriptstyle_{T_{z}}} } \right)^{p} \right]}}
\end{equation}

\noindent  where we can follow the same lines of reasoning that lead to Equation \ref{eq:gauss_8} to find (for $n$ $\rightarrow$ 0):

\begin{equation}
  \label{eq:app4_7}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \left[ \frac{ p }{{\displaystyle 2 \Gamma\left( \frac{1}{p} \right) }} \right]^{3} \frac{ n{\scriptstyle_{o}} }{ V{\scriptstyle_{T \perp}}^{2} V{\scriptstyle_{T \parallel}} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{\perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{p} \right]}}
\end{equation}

\noindent  After some manipulation and letting $V{\scriptstyle_{j}}$ $\rightarrow$ $V{\scriptstyle_{j}}$ - $v{\scriptstyle_{oj}}$ we find:

\begin{equation}
  \label{eq:app4_8}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \left[ 2 \Gamma\left( \frac{1 + p}{p} \right) \right]^{-3} \frac{ n{\scriptstyle_{o}} }{ V{\scriptstyle_{T \perp}}^{2} V{\scriptstyle_{T \parallel}} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{p} \right]}}
\end{equation}

\noindent  Note that we have again defined $V{\scriptstyle_{T j}}$ as the most probable speed of a 1D Gaussian for consistency, i.e., it does not depend upon $p$.  Further, one can see that Equation \ref{eq:app4_8} reduces to Equation \ref{eq:gauss_8} in the limit where $p \rightarrow 2$.

\indent  A slightly more general approach can be taken where the exponents are not uniform, e.g., one assumes:

\begin{equation}
  \label{eq:genbissvdf_0}
  f\left( V{\scriptstyle_{x}}, V{\scriptstyle_{y}}, V{\scriptstyle_{z}} \right) = C{\scriptstyle_{o}} \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{x}} }{ V{\scriptstyle_{T_{x}}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{y}} }{ V{\scriptstyle_{T_{y}}} } \right)^{q} + \left( \frac{ V{\scriptstyle_{z}} }{ V{\scriptstyle_{T_{z}}} } \right)^{r} \right]}}
\end{equation}

\noindent  then we find that the triple integral results in the following (still assuming we set the result equal to $n{\scriptstyle_{o}}$ and assuming the integrals are symmetric about zero):

\begin{equation}
  \label{eq:genbissvdf_1}
  n{\scriptstyle_{o}} = 2^{3} \ C{\scriptstyle_{o}} \ V{\scriptstyle_{T_{x}}} \ V{\scriptstyle_{T_{y}}} \ V{\scriptstyle_{T_{z}}} \ \Gamma\left( 1 + p^{-1} \right) \ \Gamma\left( 1 + q^{-1} \right) \ \Gamma\left( 1 + r^{-1} \right)
\end{equation}

\noindent  We can further reduce this by assuming gyrotropy about the mean magnetic field direction such that we let $r \rightarrow q$, $V{\scriptstyle_{T_{z}}} \rightarrow V{\scriptstyle_{T_{y}}}$, $V{\scriptstyle_{z}} \rightarrow V{\scriptstyle_{y}}$, and $x \rightarrow \parallel$ and $y \rightarrow \perp$, then we find the expression for the normalization constant to be:

\begin{equation}
  \label{eq:genbissvdf_2}
  C{\scriptstyle_{o}} = \frac{ p \ q^{2} \ n{\scriptstyle_{o}} }{ 2^{3} \ V{\scriptstyle_{T_{\parallel}}} \ V{\scriptstyle_{T_{\perp}}}^{2} \ \Gamma\left( p^{-1} \right) \ \Gamma^{2}\left( q^{-1} \right) }
\end{equation}

\noindent  Thus, the full bi-self-similar velocity distribution for non-homogenous exponents is given by:

\begin{equation}
  \label{eq:genbissvdf_3}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \frac{ p \ q^{2} \ n{\scriptstyle_{o}} }{ 2^{3} \ V{\scriptstyle_{T_{\parallel}}} \ V{\scriptstyle_{T_{\perp}}}^{2} \ \Gamma\left( p^{-1} \right) \ \Gamma^{2}\left( q^{-1} \right) } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{q} \right]}}
\end{equation}

\noindent  or in another form as:

\begin{equation}
  \label{eq:genbissvdf_4}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \frac{ n{\scriptstyle_{o}}  \ \Gamma^{-1}\left( \frac{1+p}{p} \right) \ \Gamma^{-2}\left( \frac{1+q}{q} \right) }{ 2^{3} \ V{\scriptstyle_{T_{\parallel}}} \ V{\scriptstyle_{T_{\perp}}}^{2} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{q} \right]}}
\end{equation}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Derivatives of Parameters: Self-Similar Distributions
%%----------------------------------------------------------------------------------------
\subsubsection{Derivatives of Parameters: Self-Similar Distributions} \label{subsubsec:selfsimilar}

\indent  In the use of numerical methods like the Levenberg-Marquardt algorithm \citep[e.g.,][]{markwardt09a}, it is useful to define the derivatives of a function with respect to the free parameters.  In the case of velocity distributions, these are the density, thermal speeds, drift speeds, and exponent (for self-similar and kappa distributions).  First, we define some simplifying terms for brevity.  Let us define the following:

\begin{subequations}
  \begin{align}
    u{\scriptstyle_{j}} & = V{\scriptstyle_{j}} - v{\scriptstyle_{o j}} \label{eq:deriv_sdf0} \\
    w{\scriptstyle_{j}} & = \frac{ u{\scriptstyle_{j}} }{ V{\scriptstyle_{T j}} } \label{eq:deriv_sdf1} \\
    \psi\left( z \right) & = \frac{ \Gamma'\left( z \right) }{ \Gamma\left( z \right) } \equiv \text{digamma function} \label{eq:deriv_sdf2}
  \end{align}
\end{subequations}

\noindent  Now we can proceed and define the partial derivatives of Equation \ref{eq:app4_8} which we will denote as $f^{(s)}$ for brevity:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:deriv_sdf3} \\
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p} - 1 }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:deriv_sdf4} \\
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left( \frac{ p \ w{\scriptstyle_{\perp}}^{p} - 2 }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:deriv_sdf5} \\
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial v{\scriptstyle_{o \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p - 1} }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:deriv_sdf6} \\
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial v{\scriptstyle_{o \perp}} } & = \left( \frac{ p \ w{\scriptstyle_{\perp}}^{p - 1} }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:deriv_sdf7} \\
    \frac{ 1 }{ f^{(s)} } \frac{ \partial f^{(s)} }{ \partial p } & = \left[ \frac{ 3 \ \psi\left( \frac{ p + 1 }{ p } \right) }{ p^{2} } - w{\scriptstyle_{\parallel}}^{p} \ \ln{ w{\scriptstyle_{\parallel}} } - w{\scriptstyle_{\perp}}^{p} \ \ln{ w{\scriptstyle_{\perp}} } \right] \label{eq:deriv_sdf8}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  We can also define the partial derivatives of Equation \ref{eq:genbissvdf_4}, denoted as $f^{(s2)}$ for brevity, as:

\begin{subequations}
  \begin{align}
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:deriv_s2df3} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p} - 1 }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:deriv_s2df4} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left( \frac{ q \ w{\scriptstyle_{\perp}}^{q} - 2 }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:deriv_s2df5} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial v{\scriptstyle_{o \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p - 1} }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:deriv_s2df6} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial v{\scriptstyle_{o \perp}} } & = \left( \frac{ q \ w{\scriptstyle_{\perp}}^{q - 1} }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:deriv_s2df7} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial p } & = \frac{ \psi\left( \frac{ p + 1 }{ p } \right) }{ p^{2} } - w{\scriptstyle_{\parallel}}^{p} \ \ln{ w{\scriptstyle_{\parallel}} } \label{eq:deriv_s2df8} \\
   \frac{ 1 }{ f^{(s2)} } \frac{ \partial f^{(s2)} }{ \partial q } & = \frac{ 2 \ \psi\left( \frac{ q + 1 }{ q } \right) }{ q^{2} } - w{\scriptstyle_{\perp}}^{q} \ \ln{ w{\scriptstyle_{\perp}} } \label{eq:deriv_s2df9}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsection:  Gyrotropic Self-Similar Distributions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Gyrotropic Self-Similar Distributions} \label{subsec:gyroselfsimilar}

\indent  In Section \ref{subsec:selfsimilar} the stage at which the normalization constant was determined may be too early\footnote{Ilya Kuzichev and Ivan Vasko brought this discrepancy to my attention on January 11, 2021.  The difference arises because one assumes a function of the form $f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right)$ $=$ $A{\scriptstyle_{as}} EXP\left[ - w{\scriptstyle_{\parallel}}^{p} - w{\scriptstyle_{\perp}}^{q} \right]$ then one solves for $A{\scriptstyle_{as}}$ in the usual fashion.  For all values of $s$ $=$ $p$ $=$ $q$ $<$ 2.4, the difference in the magnitude of the coefficients is $\lesssim$6\%.  As the exponents go to large values, the magnitude difference asymptotes to $\sim$20\% (e.g., at an exponent of $\sim$6 the magnitude difference is $\sim$18\%).  The parameter most likely impacted in the fits from \citet{wilsoniii19a}, \citet{wilsoniii19b}, and \citet{wilsoniii20a} is the number density.}, i.e., one should assume gyrotropy prior to calculating the normalization constant.  In doing so, Equation \ref{eq:app4_8} for the symmetric self-similar distribution will go to:

\begin{equation}
  \label{eq:gyrossvdf_0}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \frac{ n{\scriptstyle_{o}} \ \Gamma^{-1}\left( \frac{ s + 1 }{ s } \right) \ \Gamma^{-1}\left( \frac{ s + 2 }{ s } \right) }{ 2 \ \pi \ V{\scriptstyle_{T \perp}}^{2} V{\scriptstyle_{T \parallel}} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{s} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{s} \right]}}
\end{equation}

\noindent  where we replaced $p$ with $s$ to imply symmetric.  We can also rewrite Equation \ref{eq:genbissvdf_4} as:

\begin{equation}
  \label{eq:gyrossvdf_1}
  f\left( V{\scriptstyle_{\parallel}}, V{\scriptstyle_{\perp}} \right) = \frac{ n{\scriptstyle_{o}} \ \Gamma^{-1}\left( \frac{ p + 1 }{ p } \right) \ \Gamma^{-1}\left( \frac{ q + 2 }{ q } \right) }{ 2 \ \pi \ V{\scriptstyle_{T \perp}}^{2} V{\scriptstyle_{T \parallel}} } \ e^{^{\displaystyle - \left[ \left( \frac{ V{\scriptstyle_{\parallel}} - v{\scriptstyle_{o \parallel}} }{ V{\scriptstyle_{T \parallel}} } \right)^{p} + \left( \frac{ V{\scriptstyle_{\perp}} - v{\scriptstyle_{o \perp}} }{ V{\scriptstyle_{T \perp}} } \right)^{q} \right]}}
\end{equation}

\noindent  Now we can proceed and define the partial derivatives of Equation \ref{eq:gyrossvdf_0} which we will denote as $f^{(ss)}$ for brevity:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:gyrossvdf_2a} \\
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left( \frac{ s \ w{\scriptstyle_{\parallel}}^{s} - 1 }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:gyrossvdf_2b} \\
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left( \frac{ s \ w{\scriptstyle_{\perp}}^{s} - 2 }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:gyrossvdf_2c} \\
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial v{\scriptstyle_{o \parallel}} } & = \left( \frac{ s \ w{\scriptstyle_{\parallel}}^{s - 1} }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:gyrossvdf_2d} \\
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial v{\scriptstyle_{o \perp}} } & = \left( \frac{ s \ w{\scriptstyle_{\perp}}^{s - 1} }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:gyrossvdf_2e} \\
    \frac{ 1 }{ f^{(ss)} } \frac{ \partial f^{(ss)} }{ \partial s } & = \frac{ \psi\left( \frac{ s + 1 }{ s } \right) }{ s^{2} } + \frac{ 2 \ \psi\left( \frac{ s + 2 }{ s } \right) }{ s^{2} } - w{\scriptstyle_{\parallel}}^{s} \ \ln{ w{\scriptstyle_{\parallel}} } - w{\scriptstyle_{\perp}}^{s} \ \ln{ w{\scriptstyle_{\perp}} } \label{eq:gyrossvdf_2f}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  where $w{\scriptstyle_{j}}$ and $\psi\left( z \right)$ are defined by Equations \ref{eq:deriv_sdf1} and \ref{eq:deriv_sdf2}, respectively.  Similarly the partial derivatives of Equation \ref{eq:gyrossvdf_1} which we will denote as $f^{(as)}$ for brevity:

\begin{subequations}
  \begin{align}
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial n{\scriptstyle_{o}} } & = \frac{ 1 }{ n{\scriptstyle_{o}} } \label{eq:gyrossvdf_3a} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial V{\scriptstyle_{T \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p} - 1 }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:gyrossvdf_3b} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial V{\scriptstyle_{T \perp}} } & = \left( \frac{ q \ w{\scriptstyle_{\perp}}^{q} - 2 }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:gyrossvdf_3c} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial v{\scriptstyle_{o \parallel}} } & = \left( \frac{ p \ w{\scriptstyle_{\parallel}}^{p - 1} }{ V{\scriptstyle_{T \parallel}} } \right) \label{eq:gyrossvdf_3d} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial v{\scriptstyle_{o \perp}} } & = \left( \frac{ q \ w{\scriptstyle_{\perp}}^{q - 1} }{ V{\scriptstyle_{T \perp}} } \right) \label{eq:gyrossvdf_3e} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial p } & = \frac{ \psi\left( \frac{ p + 1 }{ p } \right) }{ p^{2} } - w{\scriptstyle_{\parallel}}^{p} \ \ln{ w{\scriptstyle_{\parallel}} } \label{eq:gyrossvdf_3f} \\
   \frac{ 1 }{ f^{(as)} } \frac{ \partial f^{(as)} }{ \partial q } & = \frac{ 2 \ \psi\left( \frac{ q + 2 }{ q } \right) }{ q^{2} } - w{\scriptstyle_{\perp}}^{q} \ \ln{ w{\scriptstyle_{\perp}} } \label{eq:gyrossvdf_3g}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section:  Energy Distribution Functions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Energy Distribution Functions} \label{sec:EnergyDistributionFunctions}

%%----------------------------------------------------------------------------------------
%%  Subsection:  Maxwell-Boltzmann Energy Distributions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Maxwell-Boltzmann Energy Distributions} \label{subsec:maxwellianboltzmann}

\indent  The Maxwell-Boltzmann energy distribution function (EDF) for a system with three degrees of freedom in the presence of a potential, $\phi{\scriptstyle_{s}}$, is given by:

\begin{equation}
  \label{eq:mbedf_0}
  f{\scriptstyle_{s}}\left( E \right) = 2 \ n{\scriptstyle_{s}} \sqrt{ \frac{ E - \phi{\scriptstyle_{s}} }{ \pi } } \ \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right)^{-3/2} \ e^{ {\displaystyle - \left( \frac{ E - \phi{\scriptstyle_{s}} }{ k{\scriptstyle_{B}} T{\scriptstyle_{s}} } \right)} }
\end{equation}

\noindent  where $k{\scriptstyle_{B}}$ is Boltzmann's constant, $T{\scriptstyle_{s}}$ is the temperature, and $n{\scriptstyle_{s}}$ is the number density of particle species $s$.  Note that unlike a velocity distribution function (VDF), the units of the EDF are given as number per unit volume per unit energy, e.g., \# cm$^{-3}$ eV$^{-1}$.  Normally Equation \ref{eq:mbedf_0} is shown in the limit where $\phi{\scriptstyle_{s}} \rightarrow 0$ and then moments of $f{\scriptstyle_{s}}\left( E \right)$ are integrated over all energies from zero to infinity.  However, in this case in the presence of a potential, the moments are given as:

\begin{equation}
  \label{eq:mbedf_1}
  \epsilon^{N} \equiv \int_{ \phi{\scriptstyle_{s}} }^{ \infty } \ dE \ \left( E - \phi{\scriptstyle_{s}} \right)^{N} \ f{\scriptstyle_{s}}\left( E \right)
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where $\epsilon^{N}$ is the N$^{th}$ energy moment of $f{\scriptstyle_{s}}\left( E \right)$.  Then the first four moments are given by:

\begin{subequations}
  \begin{align}
    \epsilon^{0} & = n{\scriptstyle_{s}} \label{eq:mbedf_2a} \\
    \epsilon^{1} & = n{\scriptstyle_{s}} \left[ \frac{ 3 }{ 2 } \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right) + \phi{\scriptstyle_{s}} \right] \label{eq:mbedf_2b} \\
    \epsilon^{2} & = n{\scriptstyle_{s}} \left[ \phi{\scriptstyle_{s}}^{2} + 3 \phi{\scriptstyle_{s}} \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right) + \frac{ 15 }{ 4 } \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right)^{2} \right] \label{eq:mbedf_2c} \\
    \epsilon^{3} & = n{\scriptstyle_{s}} \left[ \phi{\scriptstyle_{s}}^{3} + \frac{ 9 }{ 2 } \phi{\scriptstyle_{s}}^{2} \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right) + \frac{ 45 }{ 4 } \phi{\scriptstyle_{s}} \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right)^{2} + \frac{ 105 }{ 8 } \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right)^{3} \right] \label{eq:mbedf_2d}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
  \end{align}
\end{subequations}

\noindent  where the units of $\epsilon^{N}$ are given as, e.g., \# cm$^{-3}$ eV$^{N}$.

%%----------------------------------------------------------------------------------------
%%  Subsection:  Kappa Energy Distributions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Kappa Energy Distributions} \label{subsec:KappaEDFs}

\indent  The kappa energy distribution function (EDF) for a system with three kinetic degrees of freedom in the absence of a potential energy \citep[e.g.,][]{livadiotis15a, livadiotis15b} is given by:

\begin{equation}
  \label{eq:kpedf_0}
  f{\scriptstyle_{s}}\left( E \right) = \frac{ 2 \ n{\scriptstyle_{s}} E^{-1/2} \ \Gamma\left( \kappa{\scriptstyle_{s}} + 1 \right) }{ \sqrt{ \pi } \ \Gamma\left( \kappa{\scriptstyle_{s}} - \frac{ 1 }{ 2 } \right) } \left[ \left( \kappa{\scriptstyle_{s}} - \frac{ 3 }{ 2 } \right) \left( k{\scriptstyle_{B}} T{\scriptstyle_{s}} \right) \right]^{-3/2} \left[ 1 + \left( \kappa{\scriptstyle_{s}} - \frac{ 3 }{ 2 } \right)^{-1} \left( \frac{ E }{ k{\scriptstyle_{B}} T{\scriptstyle_{s}} } \right) \right]^{-\left( \kappa{\scriptstyle_{s}} + 1 \right)}
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  where $k{\scriptstyle_{B}}$ is Boltzmann's constant, $\kappa{\scriptstyle_{s}}$ is the kappa exponent, $T{\scriptstyle_{s}}$ is the temperature, and $n{\scriptstyle_{s}}$ is the number density of particle species $s$.  Equation \ref{eq:kpedf_0} assumes that any kinetic energies are given by the Galilean representation, i.e., $E$ $=$ $\tfrac{ m{\scriptstyle_{s}} }{ 2 } \left( \mathbf{v} - \mathbf{v}{\scriptstyle_{os}} \right)^{2}$ where $m{\scriptstyle_{s}}$ is the mass and $\mathbf{v}{\scriptstyle_{os}}$ is the drift velocity relative to relevant reference frame of $\mathbf{v}$.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Constructing Energy Distributions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Constructing Energy Distributions} \label{subsec:ConstructingEDFs}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Constructing EDFs:  Regular Grid
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsubsection{Constructing EDFs:  Regular Grid} \label{subsubsec:ConstructingEDFsRegular}

\indent  Suppose someone wanted to construct model energy distributions for electrons and they were given $n{\scriptstyle_{s}}$, $T{\scriptstyle_{s}}$, $\kappa{\scriptstyle_{s}}$, $\phi{\scriptstyle_{SC}}$ (spacecraft potential), and $\mathbf{v}{\scriptstyle_{os}}$.  For the sake of simplicity, let's assume that $\mathbf{v}{\scriptstyle_{os}}$ are all along the quasi-static magnetic field vector, $\mathbf{B}{\scriptstyle_{o}}$, and $T{\scriptstyle_{\parallel}}$ $=$ $T{\scriptstyle_{\perp}}$.  Then to construct a model $f{\scriptstyle_{s}}\left( E \right)$, one follows these steps: \\

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  Construct an array of energy bin values, $E{\scriptstyle_{o}}$
  \item  Shift $E{\scriptstyle_{o}}$ by $\phi{\scriptstyle_{SC}}$ to generate array of shifted energies, $E{\scriptstyle_{sh}}$ $=$ $E{\scriptstyle_{o}}$ - $\phi{\scriptstyle_{SC}}$
  \item  Compute the equivalent energies, $E{\scriptstyle_{os}}$, for each $\mathbf{v}{\scriptstyle_{os}}$ offset to create second shifted energy array, $E{\scriptstyle_{ss}}$ $=$ $E{\scriptstyle_{sh}}$ - $E{\scriptstyle_{os}}$
  \item  For Equation \ref{eq:mbedf_0}, $\left( E - \phi{\scriptstyle_{s}} \right)$ $\rightarrow$ $E{\scriptstyle_{ss}}$ and for Equation \ref{eq:kpedf_0}, the first energy (i.e., $E^{-1/2}$ factor outside of brackets) goes to $E{\scriptstyle_{sh}}$ and the second energy (i.e., the $E$ inside the brackets) goes to $E{\scriptstyle_{ss}}$, per \citet[][]{livadiotis15b}.  That is, only one of the energy terms is affected by the energy shift (which is acting as our potential here) due to the finite $\mathbf{v}{\scriptstyle_{os}}$.
  \item  Compute $f{\scriptstyle_{s}}\left( E \right)$ for each electron component (e.g., core, halo, and/or strahl) and sum them together, to generate the total electron EDF, $f{\scriptstyle_{e}}\left( E \right)$ $=$ $f{\scriptstyle_{ce}}\left( E \right)$ $+$ $f{\scriptstyle_{he}}\left( E \right)$ $+$ $f{\scriptstyle_{be}}\left( E \right)$
  \item  Compute a model EDF for the photoelectrons, $f{\scriptstyle_{ph}}\left( E \right)$, using Equation \ref{eq:mbedf_0}.  Example parameters\footnote{From an example EESA Low VDF I fit to on 2015-12-05 at $\sim$07:31:47.489 UTC.  There is a range of values due to the anisotropy of $\phi{\scriptstyle_{SC}}$ due to a dipole moment, so the given values are from the fit to the median values of the VDF.  The ranges are $n{\scriptstyle_{ph}}$ $\sim$ 43.4--104.9 $cm^{-3}$, $T{\scriptstyle_{ph}}$ $\sim$ 0.5651--0.9870 eV, and $\phi{\scriptstyle_{ph}}$ $\sim$ 1.04--2.90 eV.  These results are consistent with previous efforts to model photoelectrons near Earth \citep[e.g.,][]{pedersen95a}.} are $n{\scriptstyle_{ph}}$ $\sim$ 104.9 $cm^{-3}$, $T{\scriptstyle_{ph}}$ $\sim$ 0.6695 eV, and $\phi{\scriptstyle_{ph}}$ $\sim$ 2.24 eV, where $\left( E - \phi{\scriptstyle_{s}} \right)$ $\rightarrow$ $\left( E{\scriptstyle_{o}} - \phi{\scriptstyle_{ph}} \right)$
  \item  Sum the total electron and photoelectron EDFs together to get the total model EDF, $f{\scriptstyle_{mod}}\left( E \right)$ $=$ $f{\scriptstyle_{e}}\left( E \right)$ $+$ $f{\scriptstyle_{ph}}\left( E \right)$
\end{itemize}

\indent  Now that we have a model EDF\footnote{Note that when interpolating to the $E{\scriptstyle_{m}}$ values, the abscissa of $f{\scriptstyle_{mod}}\left( E \right)$ need to be the original energy grid values, $E{\scriptstyle_{o}}$.  This is because the instrument $E{\scriptstyle_{m}}$ values have not been corrected for $\phi{\scriptstyle_{SC}}$ or any distribution offsets, i.e., they are the unadjusted spacecraft frame energies.} $f{\scriptstyle_{mod}}\left( E{\scriptstyle_{o}} \right)$, we can interpolate to the energy bin values of an actual detector/instrument, $E{\scriptstyle_{m}}$, and then integrate the result to get moments of the EDF.  Note that in doing so, you will want to follow these steps: \\

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  Compute the base-10 logarithm of $f{\scriptstyle_{mod}}\left( E \right)$ then linearly interpolate to $E{\scriptstyle_{m}}$, giving $L{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$
  \item  Convert back to linear space to get the measured EDF, $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ $=$ $10^{ L{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right) }$
\end{itemize}

\indent  Note that $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ is at the unadjusted, spacecraft frame energies, $E{\scriptstyle_{m}}$.  That is, these energies have not yet accounted for $\phi{\scriptstyle_{SC}}$ so keep that in mind.  These are the energy bin values of a detector.  Thus, when finding the moments of $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$, only use the elements of $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ and $E{\scriptstyle_{m}}$ that satisfy $E{\scriptstyle_{m}}$ $>$ $\phi{\scriptstyle_{SC}}$ (assuming $\phi{\scriptstyle_{SC}}$ $>$ 0). \\

\indent  To compute the moments, follow these steps:

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  Define the good elements that satisfy $E{\scriptstyle_{m}}$ $>$ $\phi{\scriptstyle_{SC}}$
  \item  Define the shifted energies, $E{\scriptstyle_{msh}}$ $=$ $E{\scriptstyle_{m}}$ $-$ $\phi{\scriptstyle_{SC}}$
  \item  Calcuate the number density, $n{\scriptstyle_{em}}$, by numerically integrating over $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ with $E{\scriptstyle_{msh}}$ as the abscissa
  \item  Calcuate the temperature, $T{\scriptstyle_{em}}$, by numerically integrating over $E{\scriptstyle_{msh}} \ f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ with $E{\scriptstyle_{msh}}$ as the abscissa, then multiplying by $\tfrac{ 2 }{ 3 n{\scriptstyle_{em}} }$ (i.e., mean kinetic energy is defined as $\tfrac{ 3 }{ 2 } n{\scriptstyle_{em}} T{\scriptstyle_{em}}$, where $T{\scriptstyle_{em}}$ is in units of energy)
\end{itemize}

\indent  You can compare the results by numerically integrating $f{\scriptstyle_{mod}}\left( E \right)$ in a similar manner with $E{\scriptstyle_{sh}}$ as the abscissa and multiplying factor.  You can use $E{\scriptstyle_{sh}}$ instead of $E{\scriptstyle_{ss}}$ because the electron fit parameters should be defined in the solar wind bulk flow rest frame where $\mathbf{j}$ $=$ $- e \ \sum_{s} n{\scriptstyle_{s}} \ \mathbf{v}{\scriptstyle_{os}}$ $\simeq$ 0, i.e., the effective offset should be zero after accounting for the spacecraft potential because the sub-offsets used to construct the three electron components should balance.  The accuracy of the results depends upon the actual location of the discrete energy values and the range of energy values\footnote{It's actually more complicated than this, see \href{https://math.stackexchange.com/a/1599401/177342}{https://math.stackexchange.com/a/1599401/177342} for example}.  For instance, one can get a more accurate numerical integration result with a lower energy grid resolution if the lowest energy is closer to $E{\scriptstyle_{sh}} \sim 0$ than another grid with higher resolution but also a higher minimum energy bin value.  Thus, one can and should tailor the gridded energies accordingly\footnote{I artificially increased the grid resolution of $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ to improve the accuracy of the numerical integration to test the limits and constraints on the output.  I compared the EESA Low energy range ($\sim$5.18--1113.0 eV) with 15 energy bins to an artificial grid constructed to match an instrument on a potential future mission called MAKOS.  MAKOS will have higher angular and energy resolution than \emph{Wind}'s EESA Low, but for the example with $\phi{\scriptstyle_{SC}}$ $\sim$ 7.42 eV, the lowest $E{\scriptstyle_{sh}}$ for EESA Low happens to be closer to zero than that from the MAKOS grid of energy bins (even though the MAKOS energy range is $\sim$1.00--2000 eV, i.e., completely envelops the EESA Low range)}.

\indent  As an example, we used a model distribution with the following parameters \citep[taken from][]{wilsoniii19a, wilsoniii19b, wilsoniii20a} for the perpendicular direction (i.e., for simplicity to avoid any issues with $\mathbf{v}{\scriptstyle_{os}}$):

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  $n{\scriptstyle_{ec(h)[b]}}$ $\sim$ 8.29(0.27)[0.16] $cm^{-3}$
  \item  $T{\scriptstyle_{ec(h)[b]}}$ $\sim$ 12.91(47.66)[37.41] eV
  \item  $\kappa{\scriptstyle_{ec(h)[b]}}$ $\sim$ $\infty$(4.10)[3.84] N/A
  \item  $\phi{\scriptstyle_{SC}}$ $\sim$ 7.42 eV
\end{itemize}

%\clearpage
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Comparison btwn MMS DES and Wind EESA Low
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%\begin{wrapfigure}[35]{r}{0.55\textwidth}
\begin{figure}[!htb]
    \vspace{-5pt}
  \centering
    {\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.98\textwidth]{ExampleCompareMMSDESwithWind3dp}}
    \caption{Illustrative example electron VDFs from \emph{Wind} 3DP \citep[][]{wilsoniii21b} and MMS DES \citep[][]{pollock16a}.  The top row shows contours of constant phase space density [$cm^{-3} \ km^{-3} \ s^{+3}$] of a two-dimensional cut through a three-dimensional VDF.  The plane and coordinate basis are defined by the quasi-static magnetic field, $\mathbf{B}{\scriptstyle_{o}}$, and the ion bulk flow velocity, $\mathbf{V}{\scriptstyle_{i}}$.  The vertical axis is defined by the unit vector $\left( \mathbf{B}{\scriptstyle_{o}} \times \mathbf{V}{\scriptstyle_{i}} \right) \times \mathbf{B}{\scriptstyle_{o}}$ and the horizontal by $\mathbf{B}{\scriptstyle_{o}}$.  The bottom row shows one-dimensional cuts of the VDF along the horizontal (solid red line) and along the vertical (solid blue line).  The location of these cuts are defined by the color-coded cross hairs in the top row panels.  Also shown are the one-count (solid green line) and Poisson statistics (solid magenta line) levels for reference.  The VDF is shown in the ion bulk flow rest frame.}
    \label{fig:ExampleComparisonDESvsEL}
\end{figure}
%\end{wrapfigure}
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Comparison btwn MMS DES and Wind EESA Low
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\noindent  We construct a regular energy grid in logarithmic space to increase the density of points at lower energies where things change much more quickly to generate the initial $f{\scriptstyle_{mod}}\left( E \right)$.  We then follow the above steps to calculate $f{\scriptstyle_{m}}\left( E{\scriptstyle_{m}} \right)$ at energy bin values of \emph{Wind}'s EESA Low ($f{\scriptstyle_{EL}}\left( E{\scriptstyle_{EL}} \right)$), MMS's DES ($f{\scriptstyle_{DES}}\left( E{\scriptstyle_{DES}} \right)$), and a concept instrument for the potential future MAKOS mission ($f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$).  We numerically compute the energy moments using Simpson's 1/3 rule \citep[e.g., see discussion in][]{wilsoniii19b} (e.g., see Section \ref{sec:NumericalIntegration}) of $f{\scriptstyle_{mod}}\left( E \right)$ and compare those results to the numerical moments of $f{\scriptstyle_{EL}}\left( E{\scriptstyle_{EL}} \right)$, $f{\scriptstyle_{DES}}\left( E{\scriptstyle_{DES}} \right)$, and $f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$.  If we define the percent difference as $Q{\scriptstyle_{\%}}$ $=$ $\left( 1 - \tfrac{ Q{\scriptstyle_{m}} }{ Q{\scriptstyle_{mod}} } \right) \times 100\%$, where $Q{\scriptstyle_{m}}$ is the moment of the EDFs interpolated to the instrument energies and $Q{\scriptstyle_{mod}}$ is the moment of model EDF, then we find:

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.72 $cm^{-3}$
  \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 14.44 eV
  \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 8.78(7.16)[8.69] $cm^{-3}$
  \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 14.36(9.86)[14.42] eV
  \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ -0.70($+$17.9)[$+$0.30] \%
  \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$0.54($+$31.7)[$+$0.14] \%
\end{itemize}

\noindent  Also tried two other cases where everything stays the same except we alter the $T{\scriptstyle_{ec}}$ value to $\sim$5.025 eV and $\sim$27.11 eV.  The comparisons are as follows:

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  \textbf{Case where $T{\scriptstyle_{ec}}$ $\sim$ 5.025 eV}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.71 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 6.94 eV
    \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 8.53(7.78)[8.43] $cm^{-3}$
    \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 7.12(5.51)[7.10] eV
    \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$2.12($+$10.8)[$+$3.27] \%
    \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $-$2.56($+$20.6)[$-$2.24] \%
  \end{itemize}
  \item  \textbf{Case where $T{\scriptstyle_{ec}}$ $\sim$ 27.11 eV}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.72 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 27.93 eV
    \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 8.82(4.69)[8.75] $cm^{-3}$
    \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 27.48(12.02)[27.75] eV
    \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $-$1.14($+$46.2)[$-$0.38] \%
    \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$1.61($+$57.0)[$+$0.65] \%
  \end{itemize}
\end{itemize}

\noindent  Again note that the minimum regridded energy bin for $f{\scriptstyle_{EL}}\left( E{\scriptstyle_{EL}} \right)$ was closer to zero (after accounting for $\phi{\scriptstyle_{SC}}$) than that for $f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$, which affects the accuracy of some moments more than others, depending on temperature and density.

\indent  A final note about which energy bin values were used.  As can be seen in Figure \ref{fig:ExampleComparisonDESvsEL}, there are substantial regions of velocity space where the MMS DES detector has extremely low signal-to-noise (SNR) ratios, i.e., the data are not statistically significant.  So to get a proxy for the statistically significant energy bins, I examined the MMS DES VDFs in a burst interval on 2018-01-24 where the MMS1 spacecraft was in the solar wind\footnote{The interval used spans 2018-01-24/04:00:40.0000 UTC to 2018-01-24/04:01:50.0000 UTC which corresponds to 2333 DES VDFs.} (i.e., I exclude the first part of the interval ``contaminated'' by shock-reflected particles).  I use the SPEDAS \citep[][]{angelopoulos19a} software to retrieve the MMS DES data and I use their built-in routines to remove the secondary electron errors.  I then adjust for the spacecraft potential and an example of the end result is seen in Figure \ref{fig:ExampleComparisonDESvsEL}.  I then calculate the ratio between this corrected data and the Poisson statistics, $\tfrac{ \delta f }{ f }$, of the corrected data for all VDFs in the interval and all energy-angle bins.  Note that in burst mode, the DES instrument measures 32 energies, 16 poloidal angles, and 32 azimuthal angles, i.e., 16384 energy-angle bins per VDF.  In contrast, \emph{Wind} EL has 15 energies and 88 angle bins for a total of 1320 energy-angle bins.

%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Comparison btwn MMS DIS and Wind PESA Low
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%\begin{wrapfigure}[35]{r}{0.55\textwidth}
\begin{figure}[!htb]
    \vspace{-5pt}
  \centering
    {\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.98\textwidth]{ExampleCompareMMSDISwithWind3dp}}
    \caption{Illustrative example ion VDFs from \emph{Wind} 3DP \citep[][]{wilsoniii21b} and MMS DIS \citep[][]{pollock16a}.  The format is similar to that of Figure \ref{fig:ExampleComparisonDESvsEL} except there is an extra panel for PESA Low showing the zoomed-in view of the VDF (i.e., first column).  All VDFs have been transformed to the incident core, bulk flow rest frame (i.e., defined by centering the peak phase space density on the origin of this field-aligned coordinate basis).}
    \label{fig:ExampleComparisonDISvsPL}
\end{figure}
%\end{wrapfigure}
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Comparison btwn MMS DIS and Wind PESA Low
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\indent  So we now have a 4D array of data, i.e., [V,E,T,P]-elements where V is the number of VDFs, E is the number of energy bins, T is the number of poloidal bins, and P is the number of azimuthal bins.  I average $\tfrac{ \delta f }{ f }$ over the V VDFs and then determine how many of these energy-angle bins satisfy $\geq$0.5.  If the number is at least 200, then I consider it a good energy bin, otherwise not.  The ``good'' DES energy bins are: \\
\noindent  E [eV]:  8.54, 11.2, 14.6, 19.2, 25.1, 32.8, and 43.0. \\
\noindent  That is, there are only 7 energy bins that consistently have enough SNR to be statistically significant for DES when it is in burst mode in the solar wind (for this interval).  Performing the same calculations for \emph{Wind} EL shows that all 15 energy bins are consistently valid for 3 days worth of data\footnote{It is worth noting, again, that the MMS instruments were not designed for the cold, fast solar wind.  The \emph{Wind} 3DP instrument was and it integrates over $\sim$3 seconds, $\sim$100 times longer than MMS DES.  Thus, it's not surprising that MMS has low SNR in the solar wind.  However, it is incredibly important to consider this and know about the implications.}.  The \emph{Wind} EL energies are: \\
\noindent  E [eV]:  5.18, 5.92, 7.26, 9.41, 12.8, 18.3, 27.3, 41.8, 65.2, 103, 165, 265, 427, 689, and 1113. \\

\indent  The instrument concept for MAKOS (as of Jan. 21, 2022) includes a dedicated, low energy ($\sim$1--2000 eV) electron and ion ($\sim$400--6000 eV) instrument designed to measure the solar wind.  The electron(ion) instrument is planned to have $\Delta E/E$ $\sim$ 15\%(7\%) and $\Delta \alpha$ $<$ 15$^{\circ}$(6$^{\circ}$).  To determine the number of energy bins, $N$, based upon the energy range, $\left[ E{\scriptstyle_{min}}, E{\scriptstyle_{max}} \right]$, and $\Delta E/E$, one can show that:

\begin{equation}
  \label{eq:ConstEDF_0}
  \left( N - 1 \right) = \frac{ \log_{10} E{\scriptstyle_{max}} - \log_{10} E{\scriptstyle_{min}} }{ \log_{10} \lvert 2 + \tfrac{ \Delta E }{ E } \rvert - \log_{10} \lvert 2 - \tfrac{ \Delta E }{ E } \rvert }
%%    \left(  \right)
%%    \left[  \right]
%%    \frac{  }{  }
%%    \left( \frac{  }{  } \right)
%%    \left[ \frac{  }{  } \right]
\end{equation}

\noindent  Therefore, the electron(ion) instrument will need at least 52(40) energy bins.  These will most likely be on a uniform grid in logarithmic space.  Because we cannot know ahead of time which energy bins will have sufficient counting statistics for this analysis, we use the full array.

\indent  The above results were all derived from using a regularly gridded array of energy bins, i.e., we re-gridded the data onto uniform (linear space) energy grids before numerical integration.  The following subsection does not increase the number of energy bins and leaves the data on a non-uniform grid, for comparison.

%\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Constructing EDFs:  Irregular Grid
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsubsection{Constructing EDFs:  Irregular Grid} \label{subsubsec:ConstructingEDFsIrregular}

%%----------------------------------------------------------------------------------------
%%  Paragraph:  Irregular Grid:  Electrons
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\paragraph{Irregular Grid:  Electrons} \label{para:IrregularElectrons}

%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Irregular Grid Electrons
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\begin{wrapfigure}[35]{r}{0.35\textwidth}
%\begin{figure}[!htb]
    \vspace{-5pt}
  \centering
    {\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.98\textwidth]{ExampleInterp2IrregEnersElectronsWindMMSMAKOS}}
    \caption{Example of the model EDF for the case where $T{\scriptstyle_{ec}}$ $\sim$ 5.025 eV.  The color-coded legend is in the top panel and the model parameters are in the bottom panel.}
    \label{fig:ExampleIrregularGridElec}
%\end{figure}
\end{wrapfigure}
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Irregular Grid Electrons
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\indent  Again, let's use the example model distribution with the following parameters \citep[taken from][]{wilsoniii19a, wilsoniii19b, wilsoniii20a} for the perpendicular direction (i.e., for simplicity to avoid any issues with $\mathbf{v}{\scriptstyle_{os}}$).  However, instead of constructing an artificial, regular grid of energy bins (as was done in Section \ref{subsubsec:ConstructingEDFsRegular}) we use the actual midpoint energy bin values of \emph{Wind}'s EESA Low ($f{\scriptstyle_{EL}}\left( E{\scriptstyle_{EL}} \right)$), MMS's DES ($f{\scriptstyle_{DES}}\left( E{\scriptstyle_{DES}} \right)$), and a concept instrument for the potential future MAKOS mission ($f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$).  We use the energy bins shown at the end of Section \ref{subsubsec:ConstructingEDFsRegular} and use $\phi{\scriptstyle_{SC}}$ $\sim$ 7.42 eV to stay consistent with the results from Section \ref{subsubsec:ConstructingEDFsRegular}.  We use the same three example cases as in Section \ref{subsubsec:ConstructingEDFsRegular} but even the model is integrated on an irregular energy grid.  We report the results in terms of percent difference as before too.  So for the three cases we find:

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  \textbf{Initial model parameters}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{ec(h)[b]}}$ $\sim$ 8.29(0.27)[0.16] $cm^{-3}$
    \item  $T{\scriptstyle_{ec(h)[b]}}$ $\sim$ 12.91(47.66)[37.41] eV
    \item  $\kappa{\scriptstyle_{ec(h)[b]}}$ $\sim$ $\infty$(4.10)[3.84] N/A
    \item  $\phi{\scriptstyle_{SC}}$ $\sim$ 7.42 eV
  \end{itemize}
  \item  \textbf{Photoelectron model parameters, shown as Min-Max[used]}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{ph}}$ $\sim$ 43.4--104.9[104.9] $cm^{-3}$
    \item  $T{\scriptstyle_{ph}}$ $\sim$ 0.5651--0.9870[0.6695] eV
    \item  $\phi{\scriptstyle_{ph}}$ $\sim$ 1.04--2.90[2.24] eV
  \end{itemize}
  \item  \textbf{Case where $T{\scriptstyle_{ec}}$ $\sim$ 12.91 eV}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.72 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 14.43 eV
    \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 8.27(7.22)[8.71] $cm^{-3}$
    \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 14.95(9.85)[14.44] eV
    \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$5.20($+$17.2)[$+$0.07] \%
    \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $-$3.59($+$31.8)[$-$0.04] \%
  \end{itemize}
  \item  \textbf{Case where $T{\scriptstyle_{ec}}$ $\sim$ 5.025 eV}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.72 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 6.94 eV
    \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 7.35(7.91)[8.52] $cm^{-3}$
    \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 7.94(5.46)[7.09] eV
    \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$15.7($+$9.33)[$+$2.32] \%
    \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $-$14.5($+$21.3)[$-$2.25] \%
  \end{itemize}
  \item  \textbf{Case where $T{\scriptstyle_{ec}}$ $\sim$ 27.11 eV}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 8.72 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 27.93 eV
    \item  $n{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 8.50(4.71)[8.76] $cm^{-3}$
    \item  $T{\scriptstyle_{e, EL(DES)[MAK]}}$ $\sim$ 28.16(12.04)[27.81] eV
    \item  $n{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $+$2.53($+$46.0)[$-$0.44] \%
    \item  $T{\scriptstyle_{\%, EL(DES)[MAK]}}$ $\sim$ $-$0.83($+$56.9)[$+$0.46] \%
  \end{itemize}
\end{itemize}

\indent  Thus, it becomes quite clear that both \emph{Wind} and MAKOS do significantly better than MMS.  This should be expected, as both \emph{Wind} and MAKOS are/will be designed for the solar wind.  There are also rather significant improvements in the accuracy of the results using an irregularly spaced energy grid than the uniform in linear space results shown in Section \ref{subsubsec:ConstructingEDFsRegular}.  The reason being that even when the solar wind is considered ``hot'' as in our third example here, the distribution is still much cooler than typical magnetosheath and/or magnetosphere plasmas.  Further, the instrument often has rather low SNR in the solar wind, which means only a small subset of all possible energy bins are actually valid in the solar wind.

%%----------------------------------------------------------------------------------------
%%  Paragraph:  Irregular Grid:  Ions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\paragraph{Irregular Grid:  Ions} \label{para:IrregularIons}

%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Irregular Grid Ions
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\begin{wrapfigure}[31]{r}{0.35\textwidth}
%\begin{figure}[!htb]
    \vspace{-5pt}
  \centering
    {\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.98\textwidth]{ExampleInterp2IrregEnersIonsWindMMSMAKOS}}
    \caption{Example of the model EDF for the ions.  The color-coded legend is in the top panel and the model parameters are in the bottom panel.}
    \label{fig:ExampleIrregularGridIons}
%\end{figure}
\end{wrapfigure}
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%% Image:  Example Irregular Grid Ions
%%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\indent  Here we use the model distribution described by Equation \ref{eq:mbedf_0} with typical parameters \citep[taken from][]{wilsoniii18b} for the protons and alpha-particles.  Because most instruments measure energy per charge, we construct the EDFs in terms of speed of protons (i.e., convert from energy to speed assuming proton mass) and then convert back into energy space after drift speeds (relative to spacecraft frame), $\mathbf{v}{\scriptstyle_{os}}$, are taken into account.  Again, we use the actual midpoint energy bin values of \emph{Wind}'s PESA Low Burst ($f{\scriptstyle_{PL}}\left( E{\scriptstyle_{PL}} \right)$), MMS's DIS ($f{\scriptstyle_{DIS}}\left( E{\scriptstyle_{DIS}} \right)$), and a concept instrument for the potential future MAKOS mission ($f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$).  The energy bins

perpendicular direction (i.e., for simplicity to avoid any issues with $\mathbf{v}{\scriptstyle_{os}}$).  However, instead of constructing an artificial, regular grid of energy bins (as was done in Section \ref{subsubsec:ConstructingEDFsRegular}) we use the actual midpoint energy bin values of \emph{Wind}'s EESA Low ($f{\scriptstyle_{EL}}\left( E{\scriptstyle_{EL}} \right)$), MMS's DES ($f{\scriptstyle_{DES}}\left( E{\scriptstyle_{DES}} \right)$), and a concept instrument for the potential future MAKOS mission ($f{\scriptstyle_{MAK}}\left( E{\scriptstyle_{MAK}} \right)$).  We use the energy bins shown at the end of Section \ref{subsubsec:ConstructingEDFsRegular} and use $\phi{\scriptstyle_{SC}}$ $\sim$ 7.42 eV to stay consistent with the results from Section \ref{subsubsec:ConstructingEDFsRegular}.  We use the same three example cases as in Section \ref{subsubsec:ConstructingEDFsRegular} but even the model is integrated on an irregular energy grid.  We report the results in terms of percent difference as before too.  The \emph{Wind} PL energies are: \\
\noindent  E [eV]:  $\sim$670, 786, 922, 1079, 1265, 1485, 1740, 2037, 2384, 2797, 3278, 3837, 4495, 5264. \\
\noindent  The ``good'' DIS energy bins are: \\
\noindent  E [eV]:  $\sim$556, 736, 975, 1291, 1709, 2263, 2997, 3968, 5253, 6955. \\
\noindent  Similar to how we limited the DES ``good'' energy bins, we considered the statistical averages over an array of DIS VDFs in the solar wind.  However, the threshold for $\tfrac{ \delta f }{ f }$ here was 0.1 and only 10 angle bins needed to satisfy this requirement (i.e., because the ion VDF is much more narrow in energy-angle space than the electrons).

\indent  The instrument concept for MAKOS (as of Jan. 21, 2022) includes a dedicated, low energy ion ($\sim$400--6000 eV) instrument designed to measure the solar wind.  The ion instrument is planned to have $\Delta E/E$ $\sim$ 7\% and $\Delta \alpha$ $<$ 6$^{\circ}$.  Using Equation \ref{eq:ConstEDF_0} we can show we need at least 40 logarithmically spaced energy bins to achieve these parameters.  Similar to the electron MAKOS instrument, because we cannot know ahead of time which energy bins will have sufficient counting statistics for this analysis, we use the full array.  The parameters and resulting numerical integration values are given below:

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  \textbf{Initial model parameters}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{p[\alpha]}}$ $\sim$ 11.7[0.53] $cm^{-3}$
    \item  $T{\scriptstyle_{p[\alpha]}}$ $\sim$ 5.43[34.7] eV
    \item  $\mathbf{v}{\scriptstyle_{op}}$ $\sim$ $\left( -450, -7.22, -21.1 \right)$ km/s, GSE
    \item  $\mathbf{v}{\scriptstyle_{o \alpha}}$ $\sim$ $\left( -400, -0.83, -5.28 \right)$ km/s, GSE
  \end{itemize}
  \item  \textbf{Integrated results}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $n{\scriptstyle_{e, mod}}$ $\sim$ 12.23 $cm^{-3}$
    \item  $T{\scriptstyle_{e, mod}}$ $\sim$ 7.36 eV
    \item  $n{\scriptstyle_{e, PL(DIS)[MAK]}}$ $\sim$ 20.3(-2.19)[15.7] $cm^{-3}$
    \item  $T{\scriptstyle_{e, PL(DIS)[MAK]}}$ $\sim$ 10.2(-2.19)[8.79] eV
    \item  $n{\scriptstyle_{\%, PL(DIS)[MAK]}}$ $\sim$ $-$66.1($+$118)[$-$28.4] \%
    \item  $T{\scriptstyle_{\%, PL(DIS)[MAK]}}$ $\sim$ $-$38.2($+$130)[$-$19.4] \%
  \end{itemize}
\end{itemize}

\indent  As we can see, even with the very high resolution of the novel MAKOS instrumentation, a direct numerical integration of a measured EDF of a plasma with these parameters will still have relatively large uncertainties.  However, it will exceed \emph{Wind}'s capabilities by nearly a factor of 3 and this illustrates just how inaccurate the MMS DIS instrument can be under cold solar wind conditions.  Again, this is due to the FPI instrument being designed for the hot, slow magnetosphere and magnetosheath, not a flaw in the instrument itself.

\indent  It should also be noted that the above results for both the ions and electrons are kind of a worst case scenario to illustrate the difference that detector design can have on measured distributions.  In practical implementation of any electrostatic analyzer (ESA), the distribution is not integrated over just energy as has been done here.  There are geometric factor corrections, deadtime corrections, efficiency corrections, angular corrections, etc.  All of these corrections actually improve the results from those presented here, which is why, for example, \emph{Wind}'s velocity moments aren't off by upwards of 60\% just because the solar wind is cold.






\clearpage
%%----------------------------------------------------------------------------------------
%%  Section:  Wave Properties
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Wave Properties} \label{sec:waveprops}

\noindent  Before we begin, we should define some terms and parameters/functions that will be used later:

\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Wave Number:}  $\equiv$ effectively the number of wave crests (i.e., anti-node of local maximum) per unit length $\leftrightharpoons$ ``density'' of waves $\rightarrow$ $\mathbf{k}$ $=$ $\mathbf{k} \left(\omega,\mathbf{x},t\right)$ in general (sometimes denoted by $\boldsymbol{\kappa}$ as in \citet[][]{whitham99a})
  \item  \textbf{Wave Frequency:}  $\equiv$ effectively the number of wave crests crossing position $\mathbf{x}$ per unit time $\leftrightharpoons$ ``flux'' of waves $\rightarrow$ $\omega$ $=$ $\omega\left(\mathbf{k},\mathbf{x},t\right)$ in general
  \item  \textbf{Wave Phase:}  $\equiv$ position on a wave cycle between a crest and a trough (i.e., anti-node of local minimum) $\rightarrow$ $\phi$ $=$ $\phi\left(\mathbf{x},t\right)$ in general
  \item  \textbf{Dispersive Wave:}  $\equiv$ a propagating fluctuation where the wave frequency, $\omega\left(\mathbf{k},\mathbf{x},t\right)$, is nonlinearly dependent upon the wave number, $\mathbf{k} \left(\omega,\mathbf{x},t\right)$ $\Rightarrow$ modes with different $\mathbf{k}$ will propagate at different speeds $\Rightarrow$ modes will spread out spatially $=$ \emph{disperse}.
  \item  \textbf{Dispersion Relation:}  $\equiv$ the mathematical dependence of $\omega$ on $\mathbf{k}$ (or vice versa) $\leftrightharpoons$ mathematical relationship between $\omega$ and $\mathbf{k}$
  \item  \textbf{[Wave] Mode:}  $\equiv$ a general solution to a dispersion relation\footnote{There can be multiple \emph{modes} for one dispersion relation}
\end{enumerate}

\noindent  We can define an elementary solution to periodic wave equations as:

\begin{equation}
  \label{eq:waveprops_0a}
  \psi\left( \mathbf{x}, t \right) = \mathcal{A} \thickspace e^{ {\displaystyle i\left( \boldsymbol{\kappa} \cdot \mathbf{x} - \omega t \right) } }
\end{equation}

\noindent  where $\mathcal{A}$ is the wave amplitude and, in general, can be a function of $\boldsymbol{\kappa}$ and/or $\omega$, but we will assume constant for now.  Let us assume that a \emph{dispersion relation}, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right)$, exists and may be solved for positive real roots.  In general, there will be multiple solutions to the dispersion relation, where each solution is referred to as different \emph{modes}.  The term in the exponent is known as the \emph{wave phase}, given by:

\begin{equation}
  \label{eq:waveprops_1a}
  \phi\left( \mathbf{x}, t \right) = \boldsymbol{\kappa}\left( \omega, \mathbf{x}, t \right) \cdot \mathbf{x}  -  \omega\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) \thickspace t + \phi{\scriptstyle_{o}}
\end{equation}

\noindent  where we have used a general form for the frequency, $\omega$, and wave number, $\boldsymbol{\kappa}$.  Recall that the Fourier transform of an arbitrary function, $f(\mathbf{x},t)$, in four-dimensions is given by:

\begin{equation}
  \label{eq:waveprops_2a}
  f\left( \mathbf{x}, t \right) = \frac{1}{\left( 2 \pi \right)^{2}} \int_{-\infty}^{\infty} \thickspace d^{3}\kappa \thickspace d\omega \thickspace \tilde{f}\left( \boldsymbol{\kappa}, \omega \right) e^{ {\displaystyle i\left( \boldsymbol{\kappa} \cdot \mathbf{x} - \omega t \right) } }
\end{equation}

\noindent  where $\mathbf{x}(t)$ is an arbitrary position on a plane of constant phase at time $t$.  In other words, ``$\mathbf{x}$ represents the point of maximum constructive interference at time $t$ for a wave packet centered, in Fourier space, on $\boldsymbol{\kappa}$ and $\omega$'' \citep[page 71 of][]{stix92a}.  Because $\phi\left(\mathbf{x},t\right)$ results from solutions of the wave equation, its derivatives must satisfy the dispersion relation through the following:

\begin{equation}
  \label{eq:waveprops_3a}
  - \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial t } = \mathcal{W}\left( \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial \mathbf{x} }, \mathbf{x}, t \right)
\end{equation}

\noindent  and we can see from Equation \ref{eq:waveprops_1a} that the following is true:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial \mathbf{x} }  \label{eq:waveprops_4a}  \\
    \omega & = - \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial t }  \text{   .}  \label{eq:waveprops_4b}
  \end{align}
\end{subequations}

\noindent  We also know that $\partial^{2} \phi / \partial \mathbf{x} \partial t$ $=$ $\partial^{2} \phi / \partial t \partial \mathbf{x}$ \citep[page 119 of][]{kulsrud05b}, therefore:

\begin{subequations}
  \begin{align}
    \frac{ \partial^{2} \phi }{ \partial t \partial \mathbf{x} } - \frac{ \partial^{2} \phi }{ \partial \mathbf{x} \partial t } & = 0  \label{eq:waveprops_5a}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } - \frac{ - \partial \omega }{ \partial \mathbf{x} } = 0  \label{eq:waveprops_5b}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \mathbf{x} } = 0  \label{eq:waveprops_5c}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \nabla \omega = 0  \text{   .}  \label{eq:waveprops_5d}
  \end{align}
\end{subequations}

\noindent  One can see that Equation \ref{eq:waveprops_5d} looks similar to the \emph{continuity equation}, so long as $\boldsymbol{\kappa}$ $\leftrightharpoons$ ``density'' of the waves, and $\omega$ $\leftrightharpoons$ ``flux'' of the waves.  \\
\indent  From the above relations, we can see that on \emph{contours} of constant $\phi\left(\mathbf{x},t\right)$, we are sitting on local wave crests (i.e., \emph{phase fronts}) where $\boldsymbol{\kappa}$ is orthogonal to these \emph{contours}.  These phase fronts move parallel to $\boldsymbol{\kappa}$ at a speed, $\mathbf{V}{\scriptstyle_{\phi}}$, known as the \emph{phase velocity}.  The general form for this speed is given by:

\begin{equation}
  \label{eq:waveprops_6a}
  \mathbf{V}{\scriptstyle_{\phi}} \equiv \frac{ \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \kappa } \boldsymbol{\hat{\kappa}}  \text{   .}
\end{equation}

\noindent  If we multiply the 2nd term in Equation \ref{eq:waveprops_5c} by unity, we get:

\begin{subequations}
  \begin{align}
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \mathbf{x} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \boldsymbol{\kappa} } & = 0  \label{eq:waveprops_7a}  \\
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } & = 0  \label{eq:waveprops_7b}  \\
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \left( \mathbf{V}{\scriptstyle_{g}} \cdot \nabla \right) \boldsymbol{\kappa} & = 0  \label{eq:waveprops_7c}
  \end{align}
\end{subequations}

\noindent  where $\mathbf{V}{\scriptstyle_{g}}$ is called the \emph{group velocity}, where we note that:

\begin{equation}
  \label{eq:waveprops_8a}
  \frac{ \partial \omega }{ \partial \mathbf{x} } = \frac{ \partial \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \mathbf{x} }
\end{equation}

\noindent  which shows that $\partial \mathcal{W}$/$\partial \boldsymbol{\kappa}$ $=$ $\left( \partial \omega / \partial \boldsymbol{\kappa} \right){\scriptstyle_{\mathbf{x}}}$ $\Rightarrow$ \emph{different $\boldsymbol{\kappa}$'s propagate with velocity $\mathbf{V}{\scriptstyle_{g}}$} \citep[page 376 of][]{whitham99a}.  In other words, \emph{$\mathbf{V}{\scriptstyle_{g}}$ is the propagation velocity for $\kappa$} \citep[page 380 of][]{whitham99a} and \emph{$\mid$$\mathcal{A}$$\mid^{2}$ propagates with velocity $\mathbf{V}{\scriptstyle_{g}}$} \citep[page 379 of][]{whitham99a}.

\indent  We wish to define the term \emph{dispersive} more appropriately, so we require the following constraints:

\begin{subequations}
  \begin{align}
    \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) & = \text{ real and } \neq 0  \label{eq:waveprops_9a}  \\
    \frac{ \partial^{2} \thickspace \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \kappa^{2} } & \neq 0  \label{eq:waveprops_9b}
  \end{align}
\end{subequations}

\noindent  and finally:

\begin{equation}
  \label{eq:waveprops_10a}
  \det \left\lvert \frac{ \partial^{2} \thickspace \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \kappa{\scriptstyle_{i}} \thickspace \partial \kappa{\scriptstyle_{j}} } \right\rvert \neq 0  \text{   .}
\end{equation}

\noindent  Thus, an observer moving with the phase fronts (crests) moves at $\mathbf{V}{\scriptstyle_{\phi}}$, but they observe the local wave number and frequency to change in time $\Rightarrow$ neighboring phase fronts (crests) move away from the observer in this frame.  In contrast, for an observer moving with $\mathbf{V}{\scriptstyle_{g}}$, they observe constant local wave number and frequency (with respect to time), but phase fronts (crests) continuously move past the observer in this frame \citep[page 377 of][]{whitham99a}.

%%----------------------------------------------------------------------------------------
%%  Subsection:  Standing Waves
%%----------------------------------------------------------------------------------------
\subsection{Standing Waves} \label{subsec:StandingWaves}

\indent  Let $\tau$ be the tension and $\mu$ be a linear mass density (i.e., mass per unit length), then the wave equation for a string is given by:

\begin{equation}
  \label{eq:standwave_00a}
  \partial{\scriptstyle_{tt}} \psi \left(x,t\right) - \frac{ \tau }{ \mu } \partial{\scriptstyle_{xx}} \psi \left(x,t\right) = 0
\end{equation}

\noindent  where $\partial{\scriptstyle_{jj}} \equiv \partial^{2}/\partial j^{2}$ and $\psi \left(x,t\right)$ is a general solution to this equation, called the \textit{wave equation} \citep[e.g.,][]{french71a, whitham99a}.  This has a simple solution of the form:

\begin{equation}
  \label{eq:standwave_01a}
  \psi \left(x,t\right) = A \ e^{i \left( \pm \mathbf{k} \cdot \mathbf{x} \pm \omega t \right)}
\end{equation}

\noindent  where $A$ is some amplitude and the phase speed of the wave is given by:

\begin{equation}
  \label{eq:standwave_02a}
  \frac{\omega}{k} = \sqrt{\frac{ \tau }{ \mu }} \equiv C
\end{equation}

\indent  We want to find solutions of the form $f\left( x - C \ t \right)$, but this only works for \textit{non-dispersive waves} and does not work for nonlinear waves.  In other words, the solution applies when the wave's phase speed is $C$ = constant.

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Reflection and Transmission
%%----------------------------------------------------------------------------------------
\subsubsection{Reflection and Transmission} \label{subsubsec:ReflectionandTransmission}

\indent  First, assume $\tau$ is uniform throughout the string to avoid any unwanted acceleration.  Next, let us define a general form:

\begin{equation}
  \label{eq:standwave_03a}
  \psi{\scriptstyle_{j}} \left(x,t\right) = f{\scriptstyle_{j}} \left(x - v{\scriptstyle_{j}} t\right) = f{\scriptstyle_{j}} \left(t - \frac{x}{v{\scriptstyle_{j}}} \right)
\end{equation}

\noindent  where the subcript $j = i$ for incident, $r$ for reflected, and $t$ for transmitted waves.  Now let us assume there is some boundary at $x$ $=$ 0 and that our string has different mass densities on either side.  Let's define $\mu{\scriptstyle_{1}}$ for Region 1 (-$\infty < x < 0$) and $\mu{\scriptstyle_{2}}$ for Region 2 ($0 < x < \infty$).  Then we have:

\begin{subequations}
  \begin{align}
    v{\scriptstyle_{1}} & = \sqrt{\frac{ \tau }{ \mu{\scriptstyle_{1}} }}  \label{eq:standwave_04a} \\
    v{\scriptstyle_{2}} & = \sqrt{\frac{ \tau }{ \mu{\scriptstyle_{2}} }}  \label{eq:standwave_04b}
  \end{align}
\end{subequations}

\indent  Note that the reflected wave, $\psi{\scriptstyle_{r}} \left(x,t\right)$, will have a negative $v{\scriptstyle_{r}}$ and thus a positive sign in the expression for $f$.  Since the waves are linear, we can just write them a linear superposition of two waves for Region 1.  Then we have:

\begin{subequations}
  \begin{align}
    \psi{\scriptstyle_{1}} \left(x,t\right) & = \psi_{i} \left(x,t\right) + \psi_{r} \left(x,t\right) = f{\scriptstyle_{i}} \left(t - \frac{x}{v{\scriptstyle_{1}}} \right) + f{\scriptstyle_{r}} \left(t + \frac{x}{v{\scriptstyle_{1}}} \right)  \label{eq:standwave_05a} \\
    \psi{\scriptstyle_{2}} \left(x,t\right) & = \psi{\scriptstyle_{t}} \left(x,t\right) = f{\scriptstyle_{t}} \left(t - \frac{x}{v{\scriptstyle_{2}}} \right)  \label{eq:standwave_05b}
  \end{align}
\end{subequations}

\noindent  \textbf{Boundary Conditions (BCs)} \\

\indent  There are two boundary conditions (BCs) that must be met: (1) the string is continuous and (2) the slope of the string is continuous.  These can be written mathematically as:

\begin{subequations}
  \begin{align}
    \psi{\scriptstyle_{1}} \left(0,t\right) & = \psi{\scriptstyle_{2}} \left(0,t\right)  \label{eq:standwave_06a} \\
    \partial{\scriptstyle_{x}} \psi{\scriptstyle_{1}} \left(x,t\right) \vert_{x = 0} & = \partial{\scriptstyle_{x}} \psi{\scriptstyle_{2}} \left(x,t\right) \vert_{x = 0}  \label{eq:standwave_06b}
  \end{align}
\end{subequations}

\noindent  where these equations can be rewritten in terms of $f{\scriptstyle_{j}}$ (and integrating the second) to find:

\begin{subequations}
  \begin{align}
    f{\scriptstyle_{i}} \left(t - \frac{x}{v{\scriptstyle_{1}}} \right) + f{\scriptstyle_{r}} \left(t + \frac{x}{v{\scriptstyle_{1}}} \right) & = f{\scriptstyle_{t}} \left(t - \frac{x}{v{\scriptstyle_{2}}} \right)  \label{eq:standwave_07a} \\
    v{\scriptstyle_{2}} \left[ \ f{\scriptstyle_{i}} \left(t\right) - f{\scriptstyle_{r}} \left(t\right) \right] & = v{\scriptstyle_{1}} f{\scriptstyle_{t}} \left(t\right)  \label{eq:standwave_07b}
  \end{align}
\end{subequations}

\noindent  We can solve these two equations for $f{\scriptstyle_{r}}$ and $f{\scriptstyle_{t}}$ in terms of $f{\scriptstyle_{i}}$ to find:

\begin{subequations}
  \begin{align}
    f{\scriptstyle_{r}} & = \left( \frac{ v{\scriptstyle_{2}} - v{\scriptstyle_{1}} }{ v{\scriptstyle_{1}} + v{\scriptstyle_{2}} } \right) \ f{\scriptstyle_{i}}  \label{eq:standwave_08a} \\
    f{\scriptstyle_{t}} & = \left( \frac{ 2 \ v{\scriptstyle_{2}} }{ v{\scriptstyle_{1}} + v{\scriptstyle_{2}} } \right) \ f{\scriptstyle_{i}}  \label{eq:standwave_08b}
  \end{align}
\end{subequations}

\indent  We can see from the last two equations that the amplitudes of the reflected ($R$) and transmitted ($T$) wave are given by:

\begin{subequations}
  \begin{align}
    R & = \left( \frac{ v{\scriptstyle_{2}} - v{\scriptstyle_{1}} }{ v{\scriptstyle_{1}} + v{\scriptstyle_{2}} } \right) = \left( \frac{ \sqrt{ \mu{\scriptstyle_{1}} } - \sqrt{ \mu{\scriptstyle_{2}} } }{ \sqrt{ \mu{\scriptstyle_{1}} } + \sqrt{ \mu{\scriptstyle_{2}} } } \right)  \label{eq:standwave_09a} \\
    T & = \left( \frac{ 2 \ v{\scriptstyle_{2}} }{ v{\scriptstyle_{1}} + v{\scriptstyle_{2}} } \right) = \left( \frac{ 2 \ \sqrt{ \mu{\scriptstyle_{1}} } }{ \sqrt{ \mu{\scriptstyle_{1}} } + \sqrt{ \mu{\scriptstyle_{2}} } } \right)  \label{eq:standwave_09b}
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Massless Ring
%%----------------------------------------------------------------------------------------
\subsubsection{Massless Ring} \label{subsubsec:MasslessRing}

\indent  A massless ring\footnote{The ring must be massless to maintain the boundary conditions without requiring an infinite force to do so.  This results because we require continuity in slope and tension at the junction.  A finite mass would also result in $Z_{2} \neq 0$, as it would act like an second tension.  This scenario requires different boundary conditions} at one end of a string\footnote{Assume the ring is on a frictionless vertical rod.} is treated as a form of \textit{impedence}.  Because the ring is massless, we require that the net transverse (i.e., orthogonal to the direction of wave propagation, say, along the x/horizontal direction) force be zero.  A finite transverse force would result in an infinite acceleration.  The only difference in this case is that we need to use a non-uniform tension.  So we just follow the same steps as above but use $\tau{\scriptstyle_{j}}$ for Region $j$ and so we have:

\begin{subequations}
  \begin{align}
    \tau{\scriptstyle_{1}} \ \sin{\theta{\scriptstyle_{1}}} & = \tau{\scriptstyle_{2}} \ \sin{\theta{\scriptstyle_{2}}}  \label{eq:standwave_10a} \\
    \tau{\scriptstyle_{1}} \ \partial_{x} \psi{\scriptstyle_{1}} \left(x,t\right) \vert_{x = 0} & = \tau{\scriptstyle_{2}} \ \partial_{x} \psi{\scriptstyle_{2}} \left(x,t\right) \vert_{x = 0}  \label{eq:standwave_10b}
  \end{align}
\end{subequations}

\noindent  where the angles, $\theta{\scriptstyle_{j}}$, are relative to the x/horizontal direction.  We can define the impedence as $Z{\scriptstyle_{j}} = \tau_{j}/v_{j} = \sqrt{ \tfrac{ \tau{\scriptstyle_{j}} }{ \mu{\scriptstyle_{j}} } }$, which allows us to redefine the reflection ($R$) and transmission ($T$) coefficients as:

\begin{subequations}
  \begin{align}
    R & = \left( \frac{ Z{\scriptstyle_{1}} - Z{\scriptstyle_{2}} }{ Z{\scriptstyle_{1}} + Z{\scriptstyle_{2}} } \right)  \label{eq:standwave_11a} \\
    T & = \left( \frac{ 2 \ Z{\scriptstyle_{1}} }{ Z{\scriptstyle_{1}} + Z{\scriptstyle_{2}} } \right)  \label{eq:standwave_11b}
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Massive Ring
%%----------------------------------------------------------------------------------------
\subsubsection{Massive Ring} \label{subsubsec:MassiveRing}

\indent  In contrast to a massless ring, a massive ring requires an alteration of the BCs since we now need to include Newton's laws.  We can assume the string applies a force and the massive ring undergoes an acceleration, allowing us to write:

\begin{subequations}
  \begin{align}
    F & = \tau \ \partial{\scriptstyle_{x}} \psi \left(x,t\right)  \label{eq:standwave_12a} \\
    m \ a & = m \ \partial{\scriptstyle_{tt}} \psi \left(x,t\right)  \label{eq:standwave_12b}
  \end{align}
\end{subequations}

\noindent  Note that $F$ in Equation \ref{eq:standwave_12a} is the vertical force on the ring due to the tension in the string, $m$ in Equation \ref{eq:standwave_12b} is the mass of the ring, and $a$ in Equation \ref{eq:standwave_12b} is the acceleration of the ring\footnote{As an aside, one should note that BCs and differential equations are the primary constituents of a problem.  This is relevant as the reader can see that the superposition rule was not used in Equations \ref{eq:standwave_12a} and \ref{eq:standwave_12b} in contrast to the approach used in earlier sections.  The use of a superposition is just one of many possible methods one can use to solve the differential equations but is not required and may not apply in some circumstances.  That is, the BCs and differential equations exist independent of whether one can apply the superposition rule.}.

\indent  We can see that in the limit as $m \rightarrow 0$ we have $\partial{\scriptstyle_{x}} \psi \rightarrow 0$, thus the force is null as is necessary for a massless system.  We also see that as $m \rightarrow \infty$ we have $\partial{\scriptstyle_{tt}} \psi \rightarrow 0$, which implies a constant velocity for the massive ring (i.e., it would be zero here as the initial condition is that it starts from rest).

%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Boundary Examples
%%----------------------------------------------------------------------------------------
\subsubsection{Boundary Examples} \label{subsubsec:BoundaryExamples}

\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
  \item  \textbf{Uniform String:}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $\lim_{\mu{\scriptstyle_{2}} \rightarrow \mu{\scriptstyle_{1}}} \ v{\scriptstyle_{2}} = v{\scriptstyle_{1}}$ $\Rightarrow$ $R = 0$ and $T = +1$
  \end{itemize}
  \item  \textbf{Solid(inifinite?) Wall at $x = 0$:}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $\lim_{\mu{\scriptstyle_{2}} \rightarrow \infty} \ v{\scriptstyle_{2}} = 0$ $\Rightarrow$ $R = -1$ and $T = 0$
  \end{itemize}
  \item  \textbf{Zero mass string for $x > 0$:}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $\lim_{\mu{\scriptstyle_{2}} \rightarrow 0} \ v{\scriptstyle_{2}} = \infty$ $\Rightarrow$ $R = +1$ and $T = +2$
  \end{itemize}
  \item  \textbf{Zero mass ring at $x = 0$:}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $\lim_{\tau{\scriptstyle_{2}} \rightarrow 0} \ Z{\scriptstyle_{2}} = 0$ $\Rightarrow$ $R = +1$ and $T = +2$
  \end{itemize}
  \item  \textbf{Massive ring at $x = 0$:}
  \begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item  $\lim_{m \rightarrow 0} \ \partial{\scriptstyle_{x}} \psi = 0$ $\Rightarrow$ $R = +1$ and $T = +2$
    \item  $\lim_{m \rightarrow \infty} \ \partial{\scriptstyle_{tt}} \psi = 0$ $\Rightarrow$ $R = -1$ and $T = 0$
  \end{itemize}
\end{itemize}


%%----------------------------------------------------------------------------------------
%%  Subsection:  Inhomogeneous Media
%%----------------------------------------------------------------------------------------
\subsection{Inhomogeneous Media} \label{subsec:inhomogeneous}

\indent  Recall that for an arbitrary function, $\mathcal{F}$ $=$ $\mathcal{F} \left( t, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, ..., x{\scriptstyle_{n - 1}}, x{\scriptstyle_{n}} \right)$, the \emph{exact derivative} or \emph{total derivative} is given by:

\begin{equation}
  \label{eq:inhomogeneous_0a}
  \frac{ d \mathcal{F} }{ dt } = \frac{ \partial \mathcal{F} }{ \partial t } + \sum_{i = 1}^{n} \frac{ \partial \mathcal{F} }{ \partial x{\scriptstyle_{i}} } \thickspace \frac{ d x{\scriptstyle_{i}} }{ dt }  \text{   .}
\end{equation}

\indent  Let us start with the assumption of a lossless dispersion relation, $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right)$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right)$ $=$ 0, where $\mathcal{W}$ $=$ 0 for all points along a trajectory following $\mathbf{V}{\scriptstyle_{g}}$.  The variation of the dispersion relation, $\delta \mathcal{W}$, is given by:

\begin{equation}
  \label{eq:inhomogeneous_0b}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \cdot \delta \mathbf{x} + \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \omega + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa} = 0
\end{equation}

\noindent  which we require to $=$ 0 as well\footnote{this is actually the constraint that makes $\mathcal{W}$ $\rightarrow$ 0 for all points along the $\mathbf{V}{\scriptstyle_{g}}$-trajectory}.  Now if we let $\tau$ be a measure of distance along this trajectory and we vary the parameters with respect to $\tau$ (e.g., $\delta \boldsymbol{\kappa}$ $\rightarrow$ $d\boldsymbol{\kappa} / d\tau$ $\delta \tau$), then we find the following relations that cause the terms in Equation \ref{eq:inhomogeneous_0b} to cancel accordingly:

\begin{subequations}
  \begin{align}
    \frac{ d \mathbf{x} }{ d \tau } & = \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} }  \label{eq:inhomogeneous_1a}  \\
    \frac{ d \boldsymbol{\kappa} }{ d \tau } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_1b}  \\
    \frac{ d t }{ d \tau } & = - \frac{ \partial \mathcal{W} }{ \partial \omega }  \label{eq:inhomogeneous_1c}  \\
    \frac{ d \omega }{ d \tau } & = \frac{ \partial \mathcal{W} }{ \partial t }  \label{eq:inhomogeneous_1d}  \\
    \intertext{therefore,}
    \delta \mathbf{x} & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \delta \tau  \label{eq:inhomogeneous_1e}  \\
    \delta \boldsymbol{\kappa} & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \delta \tau  \label{eq:inhomogeneous_1f}  \\
    \delta t & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \tau  \label{eq:inhomogeneous_1g}  \\
    \delta \omega & \rightarrow \frac{ \partial \mathcal{W} }{ \partial t } \delta \tau  \label{eq:inhomogeneous_1h}
  \end{align}
\end{subequations}

\noindent  which shows us that the 2nd and 4th terms and 1st and 3rd terms in Equation \ref{eq:inhomogeneous_0b} cancel, respectively \citep[Chapter 4.7 of][]{stix92a}.  Combining Equations \ref{eq:inhomogeneous_1a} and \ref{eq:inhomogeneous_1c}, we find:

\begin{equation}
  \label{eq:inhomogeneous_1i}
  \frac{ d \mathbf{x} }{ d t } = - \frac{ \partial \mathcal{W}/\partial \boldsymbol{\kappa} }{ \partial \mathcal{W}/\partial \omega } = \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} } \equiv \mathbf{V}{\scriptstyle_{g}}  \text{   .}
\end{equation}

\indent  Now let us consider the dispersion relation, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right)$, then variation can be shown to be:

\begin{equation}
  \label{eq:inhomogeneous_2a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \cdot \delta \mathbf{x} + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa} = 0
\end{equation}

\noindent  which gives us the following relationships:

\begin{subequations}
  \begin{align}
    \frac{ d \mathbf{x} }{ d t } & = \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } = \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} }  \label{eq:inhomogeneous_3a}  \\
    \frac{ d \boldsymbol{\kappa} }{ d t } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } = - \frac{ \partial \omega }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_3b}  \\
    \frac{ d \omega }{ d t } & = \frac{ \partial \mathcal{W} }{ \partial t } = \frac{ \partial \omega }{ \partial t }  \text{   .}  \label{eq:inhomogeneous_3c}
  \end{align}
\end{subequations}

\indent  We can show this by considering that the total derivative of $\boldsymbol{\kappa}$ is given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } dt + \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } \cdot d \mathbf{x}  \notag \\
    \frac{ d \boldsymbol{\kappa} }{ dt } & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99a}  \\
    & = - {\displaystyle \left( \frac{ \partial \omega }{ \partial \mathbf{x} } \right)_{{\scriptstyle_{t}}} } + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99b}  \\
    & = - \left\{ \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \right\} + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99c}  \\
    & = - \left\{ \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \right\} + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99d}  \\
    \intertext{therefore,}
    \frac{ d \boldsymbol{\kappa} }{ dt } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \equiv - \left( \frac{ \partial \omega }{ \partial \mathbf{x} } \right)_{{\scriptstyle_{\boldsymbol{\kappa}}}}  \label{eq:inhomogeneous_99e}
  \end{align}
\end{subequations}

\noindent  where the notation $\left(  \right){\scriptstyle_{\alpha}}$ considers the expression within the parentheses a constant with respect to $\alpha$.  Equation \ref{eq:inhomogeneous_99e} is known as the \emph{wave normal equation}, or sometimes as the \emph{eikonal equation} \citep[Chapter 5.6 of][]{kulsrud05b}.  This equation defines the total rate of change of $\boldsymbol{\kappa}$ for a wave packet while following the wave packet\footnote{this is also the wave packet's \emph{refraction}}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Anisotropic Media
%%----------------------------------------------------------------------------------------
\subsection{Anisotropic Media} \label{subsec:AnisotropicMedia}

In anisotropic media, the angle between $\mathbf{V}{\scriptstyle_{gr}}$ and $\mathbf{V}{\scriptstyle_{ph}}$, $\alpha$, is given by:

\begin{subequations}
  \begin{align}
    \tan{\alpha} & = \frac{{\displaystyle \frac{ 1 }{ k } \left(\frac{ \partial \omega }{ \partial \theta }\right)_{k} }}{{\displaystyle \left(\frac{ \partial \omega }{ \partial k }\right)_{\theta} }}  \label{eq:definition_9a}  \\
    & = \frac{{\displaystyle \frac{ 1 }{ n } \left(\frac{ \partial \omega }{ \partial \theta }\right)_{n} }}{{\displaystyle \left(\frac{ \partial \omega }{ \partial n }\right)_{\theta} }}  \label{eq:definition_9b}  \\
    & = - \frac{ 1 }{ n } \left( \frac{ \partial n }{ \partial \theta } \right)_{\omega}  \text{  .}  \label{eq:definition_9c}
  \end{align}
\end{subequations}

Often times it is useful to write $\mathbf{V}{\scriptstyle_{gr}}$ in terms of $\mathbf{V}{\scriptstyle_{ph}}$.  This can be done in the following way:

\begin{subequations}
  \begin{align}
    \frac{ \partial \omega }{ \partial k } & = \frac{ c }{ n } - \frac{ k c }{ n^{2} } \left(\frac{ \partial n }{ \partial k }\right)  \label{eq:definition_10a}  \\
    & = \frac{ c }{ n } \left[ 1 - \frac{ k }{ n } \left( \frac{ \partial \omega }{ \partial k } \cdot \frac{ \partial n }{ \partial \omega } \right) \right]  \label{eq:definition_10b}  \\
    \frac{ c }{ n } & = \left[ 1 + \frac{ \omega }{ n } \left( \frac{ \partial n }{ \partial \omega } \right) \right] \frac{ \partial \omega }{ \partial k }  \label{eq:definition_10c}  \\
    \frac{ \partial \omega }{ \partial k } & = \frac{{\displaystyle c }}{{\displaystyle n + \omega \frac{ \partial n }{ \partial \omega } }}  \label{eq:definition_10d}  \\
    \frac{ \partial \omega }{ \partial k } & = V{\scriptstyle_{gr}} = \frac{{\displaystyle V{\scriptstyle_{ph}} }}{{\displaystyle \frac{ c }{ V{\scriptstyle_{ph}} } + c \omega \frac{ \partial V{\scriptstyle_{ph}}^{-1} }{ \partial \omega } }}  \label{eq:definition_10e}  \\
    & = \frac{{\displaystyle c }}{{\displaystyle \frac{ c }{ V{\scriptstyle_{ph}} } \left( 1 - \frac{ \omega }{ V{\scriptstyle_{ph}} } \frac{ \partial V{\scriptstyle_{ph}} }{ \partial \omega } \right) }}  \label{eq:definition_10f}
  \end{align}
\end{subequations}

which gives us a simple relation between the group and phase speeds, given by:

\begin{equation}
  \label{eq:definition_11}
  \mathbf{V}{\scriptstyle_{gr}} = \frac{{\displaystyle V{\scriptstyle_{ph}} }}{{\displaystyle \left( 1 - \frac{ \omega }{ V{\scriptstyle_{ph}} } \frac{ \partial V{\scriptstyle_{ph}} }{ \partial \omega } \right) }}  \text{  .}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Nonlinear Optics
%%----------------------------------------------------------------------------------------
\subsection{Nonlinear Optics} \label{subsec:nonlinoptics}

\indent  To be more general, let us assume that a nonlinear phase, $\vartheta\left(\mathbf{x},t\right)$, contains separable terms where one part represents the phase of the wave if the medium was uniform, stationary, and linear ($\mathbf{k}{\scriptstyle_{o}}$, $\omega{\scriptstyle_{o}}$) and a second part that allows for nonlinear terms ($\boldsymbol{\varkappa}$, $\varpi$).  Under these conditions, we can see that $\omega{\scriptstyle_{o}}$ $=$ $\omega{\scriptstyle_{o}} \left( \mathbf{k}{\scriptstyle_{o}}, 0 \right)$.  Therefore, we can say:

\begin{equation}
  \label{eq:nonlinoptics_0a}
  \vartheta\left( \mathbf{x}, t \right) = \left[ \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} - \omega{\scriptstyle_{o}} \thickspace t \right] + \varphi\left( \mathbf{x}, t \right)
\end{equation}

\noindent  where we now use $\varphi\left(\mathbf{x},t\right)$ to describe the nonlinear part of the wave phase \citep[Chapter 15 of][]{sagdeev88a}.  From this, we can see that the total wave number, $\boldsymbol{\kappa}$, and frequency, $\omega$, are given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \nabla \vartheta\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1a}  \\
    & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) - \nabla \left( \omega{\scriptstyle_{o}} \thickspace t \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1b}  \\
    & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1c}  \\
    \intertext{and}
    \omega & = - \partial{\scriptstyle_{t}} \thickspace \vartheta\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1d}  \\
    & = - \partial{\scriptstyle_{t}} \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) - \partial{\scriptstyle_{t}} \left( - \omega{\scriptstyle_{o}} \thickspace t \right) - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right) \label{eq:nonlinoptics_1e}  \\
    & = \partial{\scriptstyle_{t}} \left( \omega{\scriptstyle_{o}} \thickspace t \right) - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_1f}
  \end{align}
\end{subequations}

\noindent  If we assume that $\partial{\scriptstyle_{t}}$ and $\nabla$ acting on either $\mathbf{k}{\scriptstyle_{o}}$ or $\omega{\scriptstyle_{o}}$ $\rightarrow$ 0, then we have:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_2a}  \\
    \intertext{and}
    \omega & = \omega{\scriptstyle_{o}} - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_2b}
  \end{align}
\end{subequations}

\noindent  To proceed further, we need to recall some rules for vector calculus.  These rules are:

\begin{subequations}
  \begin{align}
    \nabla \left( \mathbf{A} \cdot \mathbf{B} \right) & = \mathbf{A} \times \left( \nabla \times \mathbf{B} \right) + \mathbf{B} \times \left( \nabla \times \mathbf{A} \right) + \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B} + \left( \mathbf{B} \cdot \nabla \right) \thickspace \mathbf{A}  \label{eq:nonlinoptics_3a}  \\
    \nabla \cdot \left( \mathbf{A} \thickspace \mathbf{B} \right) & = \left( \nabla \cdot \mathbf{A} \right) \thickspace \mathbf{B} + \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3b}  \\
    \mathbf{A} \times \left( \nabla \times \mathbf{B} \right) & = \left( \nabla \thickspace \mathbf{B} \right) \cdot \mathbf{A} - \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3c}  \\
    \nabla \times \left( \mathbf{A} \times \mathbf{B} \right) & = \mathbf{A} \thickspace \left( \nabla \cdot \mathbf{B} \right) - \mathbf{B} \thickspace \left( \nabla \cdot \mathbf{A} \right) + \left( \mathbf{B} \cdot \nabla \right) \thickspace \mathbf{A} - \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3d}  \\
    \intertext{and}
    \nabla \times \mathbf{x} & = 0  \label{eq:nonlinoptics_3e}  \\
    \nabla \cdot \mathbf{x} & = 3  \label{eq:nonlinoptics_3f}  \\
    \nabla \thickspace \mathbf{x} & = \overleftrightarrow{\mathbb{I}}  \label{eq:nonlinoptics_3g}
  \end{align}
\end{subequations}

\noindent  where $\overleftrightarrow{\mathbb{I}}$ is the unit dyad\footnote{often this is called the unit tensor or unit matrix or identity matrix because it satisfies the multiplication identity for rank two tensors}.  Using these relationships, we can show:

\begin{subequations}
  \begin{align}
    \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) & = \mathbf{k}{\scriptstyle_{o}} \times \left( \nabla \times \mathbf{x} \right) + \mathbf{x} \times \left( \nabla \times \mathbf{k}{\scriptstyle_{o}} \right) + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} + \left( \mathbf{x} \cdot \nabla \right) \thickspace \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_4a}  \\
    & = 0 + \left\{ \left( \nabla \thickspace \mathbf{k}{\scriptstyle_{o}} \right) \cdot \mathbf{x} - \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} \right\} + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} + \left( \mathbf{x} \cdot \nabla \right) \thickspace \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_4b}  \\
    & = \left( \nabla \thickspace \mathbf{k}{\scriptstyle_{o}} \right) \cdot \mathbf{x} + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x}  \label{eq:nonlinoptics_4c}  \\
    & = 0 + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x}  \text{   .}  \label{eq:nonlinoptics_4d}
  \end{align}
\end{subequations}

\noindent  The final term can be reduced even further by noticing:

\begin{subequations}
  \begin{align}
    \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} & = \left( \mathbf{k}{\scriptstyle_{ox}} \thickspace \partial{\scriptstyle_{x}} + \mathbf{k}{\scriptstyle_{oy}} \thickspace \partial{\scriptstyle_{y}} + \mathbf{k}{\scriptstyle_{oz}} \thickspace \partial{\scriptstyle_{z}} \right) \thickspace \left\{ x \thickspace \textbf{i} + y \thickspace \textbf{j} + z \thickspace \mathbf{k} \right\}  \label{eq:nonlinoptics_5a}  \\
    & = \left( \mathbf{k}{\scriptstyle_{ox}} \thickspace \partial{\scriptstyle_{x}} \thickspace x \right) \textbf{i} + \left( \mathbf{k}{\scriptstyle_{oy}} \thickspace \partial{\scriptstyle_{y}} \thickspace y \right) \textbf{j} + \left( \mathbf{k}{\scriptstyle_{oz}} \thickspace \partial{\scriptstyle_{z}} \thickspace z \right) \mathbf{k}  \label{eq:nonlinoptics_5b}  \\
    & = \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_5c}
  \end{align}
\end{subequations}

\noindent  which means that the final forms for $\boldsymbol{\kappa}$ and $\omega$ are given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \mathbf{k}{\scriptstyle_{o}} + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_6a}  \\
    \omega & = \omega{\scriptstyle_{o}} - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_6b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
%%    \left\{  \right\}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Doppler Effect
%%----------------------------------------------------------------------------------------
\subsection{Doppler Effect}  \label{subsec:doppler}

\noindent  If we assume we are at rest with respect to a fluid moving at a velocity of $\mathbf{V}{\scriptstyle_{sw}}$, then the frequency of a signal convecting with the fluid would be given by:

\begin{equation}
  \label{eq:doppler_0}
  \omega{\scriptstyle_{obs}} = \gamma \left( \omega{\scriptstyle_{o}} + \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{V}{\scriptstyle_{sw}} \right)
\end{equation}

\noindent  where $\omega{\scriptstyle_{obs}}$ is the frequency we observe, $\omega{\scriptstyle_{o}}$ is the actual frequency of the source, $\gamma$ is the relativistic factor, and $\mathbf{k}{\scriptstyle_{o}}$ is the wave vector of the source.  The relationship in Equation \ref{eq:doppler_0} holds because the phase of any signal, $\phi$, is a Lorentz invariant \citep[see page 529 of][]{jackson98a}.  This means:

\begin{equation}
  \label{eq:doppler_1}
  \phi = \omega{\scriptstyle_{obs}} t{\scriptstyle_{obs}} - \mathbf{k}{\scriptstyle_{obs}} \cdot \mathbf{x}{\scriptstyle_{obs}} = \omega{\scriptstyle_{o}} t{\scriptstyle_{o}} - \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x}{\scriptstyle_{o}}
\end{equation}

\noindent  where t${\scriptstyle_{j}}$ is the time in the $j$-frame and $\mathbf{x}{\scriptstyle_{j}}$ the position in the $j$-frame.  The wave number is given by:

\begin{equation}
  \label{eq:doppler_2}
  k{\scriptstyle_{j}}\left(\omega{\scriptstyle_{j}}\right) = \frac{ \omega{\scriptstyle_{j}} }{ c } n\left(\omega{\scriptstyle_{j}}\right)
\end{equation}

\noindent  where n$\left(\omega{\scriptstyle_{j}}\right)$ is the wave index of refraction in the $j$-frame.  Thus, we can show for stationary phase ($\partial \thickspace \phi$/$\partial \thickspace \omega$ $=$ 0):

\begin{equation}
  \label{eq:doppler_3}
  c \frac{ d \thickspace k{\scriptstyle_{j}} }{ d \thickspace \omega{\scriptstyle_{j}} } = n\left(\omega{\scriptstyle_{j}}\right) + \omega{\scriptstyle_{j}} \frac{ d \thickspace n\left(\omega{\scriptstyle_{j}}\right) }{ d \thickspace \omega{\scriptstyle_{j}} }  \text{  .}
\end{equation}

\noindent  In general the frequency and wave number are a four vector with the following Lorentz transformations:

\begin{subequations}
  \begin{align}
    \frac{ \omega' }{ c } & = \gamma \left( \frac{ \omega }{ c } - \frac{ \mathbf{v} }{ c } \cdot \mathbf{k} \right)  \label{eq:doppler_4a}  \\
    k{\scriptstyle_{\parallel}}' & = \gamma \left( \frac{ \mathbf{v} \cdot \mathbf{k} }{ \mid \mathbf{v} \mid } - \frac{ v }{ c } \frac{ \omega }{ c } \right)  \label{eq:doppler_4b}  \\
    \mathbf{k}{\scriptstyle_{\perp}}' & = \mathbf{k}{\scriptstyle_{\perp}}  \text{  .}  \label{eq:doppler_4c}
  \end{align}
\end{subequations}

\noindent  For an electromagnetic wave with the angle between $\mathbf{k}$ and $\mathbf{v}$ defined as $\theta$, we have:

\begin{equation}
  \label{eq:doppler_4}
  \tan{\theta'} = \frac{{\displaystyle \sin{\theta} }}{{\displaystyle \gamma \left( \cos{\theta} - \frac{ v }{ c } \right) }}
\end{equation}

\noindent  which shows that there exists a Doppler shift even when $\mathbf{k}$ is orthogonal to $\mathbf{v}$ (i.e. $\theta$ $\rightarrow$ $\pi$/2) \citep[see page 530 of][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Heat flux from 10 Moment Diffusive Term
%%----------------------------------------------------------------------------------------
\section{Heat flux from 10 Moment Diffusive Term}
\indent  Let us define the following quantities:
\begin{enumerate}
  \item $\mathbb{Q}$   $\equiv$ heat flux $=$ $\mathbb{Q}{\scriptstyle_{0}}$ $+$ $\mathbb{Q}{\scriptstyle_{1}}$
  \item $\mathbb{T}$($\mathbb{P}$) $\equiv$ temperature(pressure) tensor
  \item $\mathfrak{T}$ $\equiv$ $\mathbb{T}^{-1}$ (inverse of the temperature tensor)
  \item $\mathbb{I}$   $\equiv$ identity tensor
  \item $\tilde{\mathbb{A}}$   $\equiv$ arbitrary 3-rank tensor
  \item $\mathbb{A}$\textbf{:}$\mathbb{B}$ $\equiv$ Frobenius inner product $=$ $\sum_{i,j}$ $\mathbb{A}{\scriptstyle_{ij}}$ $\mathbb{B}{\scriptstyle_{ij}}$
  \item Tr[]           $\equiv$ trace
  \item Sym[]          $\equiv$ tensor symmetrization operator
\end{enumerate}
where for example, we define:
\begin{subequations}
  \begin{align}
    Sym\left[ \nabla \textbf{u} \right] & = \frac{1}{2} \left[ \nabla \textbf{u} + \left( \nabla \textbf{u} \right)^{T} \right]  \label{eq:symmetric_0} \\
    Sym\left[ \mathbb{P} \cdot \nabla \textbf{u} \right] & = Sym\left[ \nabla \cdot \left( \mathbb{P} \textbf{u} \right) - \textbf{u} \nabla \cdot \mathbb{P} \right]  \label{eq:symmetric_1}
  \end{align}
\end{subequations}
which we have chosen because in general, diffusion terms have the form $\nabla$ $\cdot$ ($\tilde{\mathbb{A}}$\textbf{:}$\nabla$\textbf{h}).  Thus we can write:
\begin{subequations}
  \begin{align}
    \mathbb{Q} &= 3 A{\scriptstyle_{1}} Sym\left[ \nabla \mathfrak{T} \right] + 3 A{\scriptstyle_{0}} Sym\left[ \mathbb{I} \cdot Tr\left[ \nabla \mathfrak{T} \right] \right]  \label{eq:symmetric_2}  \\
    \left[ \mathbb{Q}{\scriptstyle_{1}} \right]{\scriptstyle_{ijk}} & = A{\scriptstyle_{1}} \left( \partial{\scriptstyle_{i}} \mathfrak{T}{\scriptstyle_{jk}} + \partial{\scriptstyle_{k}} \mathfrak{T}{\scriptstyle_{ij}} + \partial{\scriptstyle_{j}} \mathfrak{T}{\scriptstyle_{ik}}  \right)   \label{eq:symmetric_3}  \\
    \left[ \mathbb{Q}{\scriptstyle_{0}} \right]{\scriptstyle_{ijk}} & = A{\scriptstyle_{0}} \left[ \delta{\scriptstyle_{ij}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{nk}} + \partial{\scriptstyle_{k}} \mathfrak{T}{\scriptstyle_{nn}} \right) + \delta{\scriptstyle_{ik}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{nj}} + \partial{\scriptstyle_{j}} \mathfrak{T}{\scriptstyle_{nn}} \right) +   \delta{\scriptstyle_{jk}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{ni}} + \partial{\scriptstyle_{i}} \mathfrak{T}{\scriptstyle_{nn}} \right) \right] \label{eq:symmetric_4}
  \end{align}
\end{subequations}
where A${\scriptstyle_{0,1}}$ are positive parameters determined by a collisional integral. 

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Rotations and Transformations
%%----------------------------------------------------------------------------------------
\section{Rotations and Transformations} \label{sec:RotationsandTransformations}

\subsection{Constructing Rotation Matrices} \label{subsec:ConstructingRotationMatrices}

Let's assume we have two arbitrary vectors, \textbf{A} and \textbf{B}.  Let their unit vectors be denoted by: \textbf{$\hat{a}$} and \textbf{$\hat{b}$}.  If we want to find the parts of vector \textbf{A} which are parallel and perpendicular to \textbf{B}, we can do a couple of things:
\begin{enumerate}
  \item We can find \textbf{A}$_{\perp}$ and \textbf{A}$_{\parallel}$ with dot and cross products, but leave the the resultant vectors in the original coordinate basis
  \item We can find \textbf{A}$_{\perp}$ and \textbf{A}$_{\parallel}$ by rotating both vectors to a new coordinate basis where \textbf{B}' is now the Z'-Axis and \textbf{A}' is in the X'Z'-Plane.
\end{enumerate}
The method to deal with the first method is the following:  1) First find the unit vectors in the typical manner:
\begin{subequations}
  \begin{align}
    \textbf{a} & \equiv \frac{\textbf{A}}{\lvert \textbf{A} \rvert} \label{eq:rotate_1} \\
    \textbf{b} & \equiv \frac{\textbf{B}}{\lvert \textbf{B} \rvert} \label{eq:rotate_2} \text{ ,}
  \end{align}
\end{subequations}
2) then we find the parallel vector by the following method:
\begin{subequations}
  \begin{align}
    \textbf{a}_{\parallel} & = \Bigl(\textbf{a} \cdot \textbf{b}\Bigr)\textbf{b} = \lvert\textbf{a}\rvert \lvert\textbf{a}\rvert \cos{\theta_{ab}} \textbf{b} \label{eq:rotate_3} \\
    \textbf{a}_{\perp}     & \equiv \Bigl(\textbf{b} \times \textbf{a} \Bigr) \times \textbf{a} = \textbf{a} - \Bigl(\textbf{b} \cdot \textbf{a} \Bigr)\textbf{b} \label{eq:rotate_4} \text{ ,}
  \end{align}
\end{subequations}
which only need to be multiplied by the magnitude of the vector, \textbf{A}, to be turned back into vectors.  It should be noted that these two vectors, \textbf{A}$_{\parallel}$ and \textbf{A}$_{\perp}$, satisfy the following condition:
\begin{equation}
  \label{eq:rotate_5}
  \lvert \textbf{A} \rvert = \sqrt{\Bigl(\textbf{A}_{\parallel}\Bigr)^{2} + \Bigl(\textbf{A}_{\perp}\Bigr)^{2}} \equiv \sqrt{\Biggl(\sum_{i}^{3} A_{i}^{2} \Biggr)} \text{ .}
\end{equation}
The second method to find these vectors is by constructing a matrix which can rotate both vectors into a new coordinate system where \textbf{b}' is parallel to the new Z'-Axis and \textbf{a}' is in the X'Z'-Plane.  To do this, we start with the unit vectors again.  The first thing we do is define the following two vectors:
\begin{subequations}
  \begin{align}
    \textbf{c} & \equiv \textbf{b} \times \textbf{a} \label{eq:rotate_6} \\
    \textbf{d} & \equiv \textbf{c} \times \textbf{b} \label{eq:rotate_7}
  \end{align}
\end{subequations}
which we use to construct the following matrix:
\begin{equation}
  \label{eq:rotate_8}
 \textbf{R} = \left[
  \begin{array}{ c c c }
     d_{1} & d_{2} & d_{3}  \\
     c_{1} & c_{2} & c_{3}  \\
     a_{1} & a_{2} & a_{3}
  \end{array} \right]
\end{equation}
The original vectors can now be rotated into a new coordinate system.  Let's consider an example for illustrative purposes.  Let the following be true:
\begin{subequations}
  \begin{align}
    \textbf{A} & = \bigl\{0.2,0.3,0.4 \bigr\} \label{eq:rotate_9} \\
    \textbf{B} & = \bigl\{0.1,0.5,0.7 \bigr\} \label{eq:rotate_10} \\
    \lvert \textbf{A} \rvert & = 0.5385165215 \label{eq:rotate_11} \\
    \lvert \textbf{B} \rvert & = 0.8660253882 \label{eq:rotate_12} \\
    \textbf{a} & = \bigl\{0.37139,0.55709,0.74278 \bigr\} \label{eq:rotate_13} \\
    \textbf{b} & = \bigl\{0.11547,0.57735,0.80829 \bigr\} \label{eq:rotate_14} \\
    \textbf{c} & = \bigl\{-0.02144,0.21442,-0.15010 \bigr\} \label{eq:rotate_15} \\
    \textbf{d} & = \bigl\{0.25997,1.30385\times10^{-8},-0.03714 \bigr\} \label{eq:rotate_16}
  \end{align}
\end{subequations}
where the Y-component of \textbf{d} is a consequence of rounding errors, which I'll show turn out to actually matter.  Thus our matrix is:
\begin{equation}
  \label{eq:rotate_17}
 \textbf{R} = \left[
  \begin{array}{ c c c }
     0.25997 & 1.3038\times10^{-8} & -0.03714  \\
     -0.02144 & 0.21442 & -0.15010  \\
     0.11547 & 0.57735 & 0.80829
  \end{array} \right]
\end{equation}
which produces the following new vectors:
\begin{subequations}
  \begin{align}
    \textbf{a}' & = \bigl\{0.06897,-1.84871\times10^{-9},0.964901 \bigr\} \label{eq:rotate_18} \\
    \textbf{b}' & = \bigl\{-1.87482\times10^{-9},-7.16156\times10^{-9},1.00000 \bigr\} \label{eq:rotate_19} \text{ .}
  \end{align}
\end{subequations}
One can see that \textbf{a}' and \textbf{b}' are not normalized, nor are they what we \emph{expected} them to be.  Meaning, I claimed that \textbf{b}' should be PURELY in the Z'-direction, but this has small, finite values in the X'Y'-Plane.  Before we complain too much about this atrocity, let's normalize the unit vectors, which makes them now:
\begin{subequations}
  \begin{align}
    \textbf{a}' & = \bigl\{0.07129,-1.91108\times10^{-9},0.99746 \bigr\} \label{eq:rotate_20} \\
    \textbf{b}' & = \bigl\{-1.87482\times10^{-9},-7.16156\times10^{-9},1.00000 \bigr\} \label{eq:rotate_21} \text{ .}
  \end{align}
\end{subequations}
Recall that I claimed these \emph{small} rounding errors made a difference in your final answer, so let's go back to our first set of rotated unit vectors in Equations \ref{eq:rotate_18} and \ref{eq:rotate_19} and intentionally force those \emph{small} rounding errors to zero before we renormalize the unit vectors.  Let's define these new ones as \textbf{w}' and \textbf{u}' to avoid confusion with our vectors in Equations \ref{eq:rotate_20} and \ref{eq:rotate_21}, and they become (after renormalizing):
\begin{subequations}
  \begin{align}
    \textbf{w}' & = \bigl\{0.07129,0.00000,0.99746 \bigr\} \label{eq:rotate_22} \\
    \textbf{u}' & = \bigl\{0.00000,0.00000,1.00000 \bigr\} \label{eq:rotate_23} \text{ .}
  \end{align}
\end{subequations}
We now take the magnitudes of our original vectors and multiply that by these unit vectors to get the new vectors:
\begin{subequations}
  \begin{align}
    \lvert \textbf{A} \rvert * \textbf{a}' & \equiv \textbf{A}' = \bigl\{0.0383921,-1.02915\times10^{-9},0.537146 \bigr\} \label{eq:rotate_24} \\
    \lvert \textbf{B} \rvert * \textbf{b}' & \equiv \textbf{B}' = \bigl\{-1.62364\times10^{-9},-6.20209\times10^{-9},0.866025 \bigr\} \label{eq:rotate_25} \\
    \lvert \textbf{A} \rvert * \textbf{w}' & \equiv \textbf{W}' = \bigl\{0.0383921,0.00000,0.537146 \bigr\} \label{eq:rotate_26} \\
    \lvert \textbf{B} \rvert * \textbf{u}' & \equiv \textbf{U}' = \bigl\{0.00000,0.00000,0.866025 \bigr\} \label{eq:rotate_27} \text{ .}
  \end{align}
\end{subequations}
If we use double precision instead of single, our rotation matrix is now:
\begin{equation}
  \label{eq:rotate_28}
 \textbf{R}_{d} = \left[
  \begin{array}{ c c c }
     0.25997347 & -6.5919492\times10^{-17} & -0.037139068  \\
     -0.021442251 & 0.21442251 & -0.15009575  \\
     0.11547005 & 0.57735027 & 0.80829038
  \end{array} \right]
\end{equation}
which produces the following new vectors (after normalization):
\begin{subequations}
  \begin{align}
    \textbf{a}'_{d} & = \bigl\{0.071292300580,9.63019443558\times10^{-18},0.997455466614 \bigr\} \label{eq:rotate_29} \\
    \textbf{b}'_{d} & = \bigl\{-3.88116288919\times10^{-18},1.40180631841\times10^{-18},1.000000000000 \bigr\} \label{eq:rotate_30} \text{ .}
  \end{align}
\end{subequations}
Again we step back and intentionally remove the rounding errors before renormalizing to get (keep the same names this time):
\begin{subequations}
  \begin{align}
    \textbf{a}'_{d} & = \bigl\{0.071292300580,0.000000000000,0.997455466614 \bigr\} \label{eq:rotate_31} \\
    \textbf{b}'_{d} & = \bigl\{0.000000000000,0.000000000000,1.000000000000 \bigr\} \label{eq:rotate_32} \text{ .}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Normal Incidence Frame and Coordinate Basis
%%----------------------------------------------------------------------------------------
\subsection{Normal Incidence Frame and Coordinate Basis} \label{subsec:NormalIncidenceFrameandCoordinateBasis}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: The Normal Incidence Frame
%%----------------------------------------------------------------------------------------
\subsubsection{The Normal Incidence Frame} \label{subsubsec:TheNormalIncidenceFrame}

\indent  In this section, we will define our reference frame transformation into the Normal Incidence Frame (NIF) and coordinate basis rotations into the Normal incidence frame Coordinate Basis (NCB).  We will present the transformations/rotations in a generalized manner, but for the purposes of this manuscript the measurements are in the SpaceCraft Frame (SCF) and GSE coordinate basis.  We define the generalized basis as the Input Coordinate Basis (ICB).  In the following, we will use the notation $\mathbf{V}{\scriptstyle_{Coord}}^{Ref}$ to represent a 3-vector in the coordinate basis, \emph{Coord}, and reference frame, \emph{Ref}.

\indent  We can define the velocity transformation from any arbitrary frame of reference (e.g., SCF) to the shock frame of reference (SHF) as:

\begin{equation}
  \label{eq:fieldtrans_0}
  \mathbf{V}{\scriptstyle_{ICB}}^{SHF} = \mathbf{V}{\scriptstyle_{ICB}}^{arb.} - \left( \mathbf{V}{\scriptstyle_{sh, ICB}}^{arb.} \cdot \hat{\mathbf{n}} \right) \hat{\mathbf{n}}
\end{equation}

\noindent  where $\hat{\mathbf{n}}$ is the vector normal to the assumed planar shock front (see Appendix \ref{app:rhequations}).  For an experimentalist's purposes with spacecraft observations, $\mathbf{V}{\scriptstyle_{ICB}}^{arb.}$ $\rightarrow$ $\mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF}$ $\equiv$ the bulk flow solar wind velocity in the SCF and ICB.  Let us define $\left( \mathbf{V}{\scriptstyle_{sh, ICB}}^{arb.} \cdot \hat{\mathbf{n}} \right)$ $=$ $V{\scriptstyle_{sh,n}}^{SCF}$ as the shock speed along the unit normal vector, $\hat{\textbf{n}}$, in the SCF in the upstream region and $U{\scriptstyle_{j,n}}^{SHF}$ as the shock normal speed in the SHF, determined from the numerical Rankine-Hugoniot solution techniques \citep[e.g.,][]{vinas86a, koval08a}, in the $j^{th}$ region.  Let us also define $\langle Q \rangle{\scriptstyle_{region}}$ as the spatial ensemble average of any parameter, $Q$, over a given space (i.e., upstream or downstream)\footnote{Note that $\hat{\textbf{n}}$, $V{\scriptstyle_{sh,n}}^{SCF}$, and $U{\scriptstyle_{j,n}}^{SHF}$ are, by definition, assumed to be averages over the upstream or downstream regions.  I did not include $\langle  \rangle$'s out of laziness.  Note I have also omitted the fact that $\hat{\textbf{n}}$ is generally defined in the ICB in the SCF.}.

\indent  Therefore, we can define the average upstream incident bulk flow velocity in the SHF, which is given by:

\begin{equation}
  \label{eq:fieldtrans_2}
  \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right)  \text{  .}
\end{equation}

\noindent  From the relationship for $\langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{j}}$, we can show that:

\begin{equation}
  \label{eq:fieldtrans_1}
  U{\scriptstyle_{j,n}}^{SHF} = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{j}} \cdot \hat{\mathbf{n}}  \text{  .}
\end{equation}

\indent  There are two physically significant frames of reference:  the Normal Incidence Frame (NIF) and the de Hoffmann-Teller frame (dHT).  The NIF is useful because the upstream flow velocity is entirely along the shock normal vector\footnote{assuming a locally planar discontinuity, as in Appendix \ref{app:rhequations}}.  The dHT frame is useful because the upstream flow velocity is entirely along the upstream averaged quasi-static magnetic field ($\mathbf{B}{\scriptstyle_{u}}$)\footnote{which results in the convective electric field $\rightarrow$ 0}.  The transformation velocity from the SHF to the NIF or dHT are given by:

\begin{subequations}
  \begin{align}
    \mathbf{V}{\scriptstyle_{ICB}}^{NIF} & = \hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)  \label{eq:fieldtrans_3a}  \\
    \mathbf{V}{\scriptstyle_{ICB}}^{dHT} & = \frac{ \hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \right) }{ \hat{\mathbf{n}} \cdot \langle \mathbf{B}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} }  \label{eq:fieldtrans_3b}
  \end{align}
\end{subequations}

\noindent  so that the upstream flow velocity in each reference frame is given by:

\begin{subequations}
  \begin{align}
    \langle \mathbf{V}{\scriptstyle_{ICB}}^{NIF} \rangle{\scriptstyle_{up}} & = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} - \mathbf{V}{\scriptstyle_{ICB}}^{NIF}  \label{eq:fieldtrans_4a}  \\
    \langle \mathbf{V}{\scriptstyle_{ICB}}^{dHT} \rangle{\scriptstyle_{up}} & = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} - \mathbf{V}{\scriptstyle_{ICB}}^{dHT}  \text{  .}  \label{eq:fieldtrans_4b}
  \end{align}
\end{subequations}

\noindent  Note that $\mathbf{V}{\scriptstyle_{ICB}}^{NIF}$ $=$ $\hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)$ $=$ $\hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)$ because $\hat{\mathbf{n}} \times \hat{\mathbf{n}}$ $=$ 0.  Since the change in velocity between any shock rest frame the local SC frame satisfies $\mid$$\boldsymbol{\beta}$$\mid$ $\equiv$ $\mid$$\Delta$$\mathbf{V}\mid$/c $\ll$ 1 for any shock within the heliosphere, the Lorentz transformations of the electric and magnetic fields \citep[page 558 of][]{jackson98a} can be given by:

\begin{subequations}
  \begin{align}
    \mathbf{E}' & = \gamma \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{E} \right)  \label{eq:fieldtrans_5a}  \\
    \lim_{\gamma \rightarrow 1} \mathbf{E}' & \approx \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right)  \label{eq:fieldtrans_5b}  \\
    \mathbf{B}' & = \gamma \left( \mathbf{B} - \boldsymbol{\beta} \times \mathbf{E} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{B} \right)  \label{eq:fieldtrans_5c}  \\
    \lim_{\gamma \rightarrow 1} \mathbf{B}' & \approx \mathbf{B}  \text{  .}  \label{eq:fieldtrans_5d}
  \end{align}
\end{subequations}

\noindent  The difference in flow velocity, $\Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)}$, between the SCF and relevant shock rest frames, i.e., NIF and dHT, is given by:

\begin{subequations}
  \begin{align}
    \Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)} & = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \langle \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)} \rangle{\scriptstyle_{up}}  \label{eq:fieldtrans_6a}  \\
    & = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left[ \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right) \right] + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)}  \label{eq:fieldtrans_6b}  \\
    & = \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right) + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)}  \label{eq:fieldtrans_6c}
  \end{align}
\end{subequations}

\noindent  which allows us to show that the electric field in a relevant shock rest frame, $\mathbf{E}{\scriptstyle_{ICB}}^{NIF,dHT}$, can be determined from the electric field observed in the SCF, $\mathbf{E}{\scriptstyle_{ICB}}^{SCF}$, through the following:

\begin{subequations}
  \begin{align}
    \mathbf{E}{\scriptstyle_{ICB}}^{NIF(dHT)} & = \mathbf{E}{\scriptstyle_{ICB}}^{SCF} + \left( \Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)} \times \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \right)  \label{eq:fieldtrans_7a}  \\
     & = \mathbf{E}{\scriptstyle_{ICB}}^{SCF} + \left[ \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)} \right) \times \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \right]  \text{  .}  \label{eq:fieldtrans_7b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  NIF Coordinate Basis
%%----------------------------------------------------------------------------------------
\subsubsection{NIF Coordinate Basis}  \label{subsubsec:NIFCoordinateBasis}

\indent  We can rotate into the Normal incidence frame Coordinate Basis (NCB) from the Input Coordinate Basis (ICB) by defining a rotation matrix, $\mathbb{A}$ \citep{scudder86a}, given by:

\begin{equation}
  \label{eq:nifbasis_1}
 \mathbb{A} = \left[
  \begin{array}{ c c c }
    n{\scriptstyle_{x}}     & n{\scriptstyle_{y}}     & n{\scriptstyle_{z}}      \\
    \beta{\scriptstyle_{x}} & \beta{\scriptstyle_{y}} & \beta{\scriptstyle_{z}}  \\
    \zeta{\scriptstyle_{x}} & \zeta{\scriptstyle_{y}} & \zeta{\scriptstyle_{z}}
  \end{array} \right]
\end{equation}

\noindent  where $\hat{\mathbf{n}}$ is the shock normal vector and $\boldsymbol{\beta}$ and $\boldsymbol{\zeta}$ are given by:

\begin{subequations}
  \begin{align}
    \hat{\textbf{y}} = \boldsymbol{\beta} & = \frac{ \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{dn}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up}} }{ \mid \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{dn}} \mid }  \label{eq:nifbasis_2a}  \\
    \hat{\textbf{z}} = \boldsymbol{\zeta} & = \frac{ \hat{\mathbf{n}} \times \boldsymbol{\beta} }{ \mid \hat{\mathbf{n}} \times \boldsymbol{\beta} \mid }  \label{eq:nifbasis_2b}
  \end{align}
\end{subequations}

\noindent  where $\langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up(dn)}}$ is the average upstream(downstream) magnetic field vector.  If the vectors $\hat{\mathbf{n}}$, $\boldsymbol{\beta}$, and $\boldsymbol{\zeta}$ start in the ICB (e.g., GSE), then one would expect that $\mathbb{A}$ acting on $\hat{\mathbf{n}}$, $\boldsymbol{\beta}$, or $\boldsymbol{\zeta}$ should give the corresponding NCB axis unit vector.  Meaning, we expect the following to be true:

\begin{subequations}
  \begin{align}
    \mathbb{A} \cdot \hat{\mathbf{n}} & = \langle 1, 0, 0 \rangle  \label{eq:nifbasis_3a}  \\
    \mathbb{A} \cdot \boldsymbol{\beta} & = \langle 0, 1, 0 \rangle  \label{eq:nifbasis_3b}  \\
    \mathbb{A} \cdot \boldsymbol{\zeta} & = \langle 0, 0, 1 \rangle \text{  .}  \label{eq:nifbasis_3c}
  \end{align}
\end{subequations}

\noindent  Thus, $\mathbb{A}$ should rotate any ICB vector into the NCB.

%%  Caveat
\indent  If the coordinate vectors used to create $\mathbb{A}$ are not orthogonal, then the correct rotation tensor is given by $\mathbb{R}$ $=$ ($\mathbb{A}^{T}$)$^{-1}$, or the inverse transpose of $\mathbb{A}$.  The need to perform the inverse transpose of $\mathbb{A}$ arises from the non-orthogonal nature of the NIF basis.  If the NIF were created from an orthogonal basis, then $\mathbb{A}$ would be an orthogonal matrix, which means $\mathbb{A}^{T}$ $=$ $\mathbb{A}^{-1}$.  For any invertible matrix, the following is true:  ($\mathbb{A}^{T}$)$^{-1}$ $=$ ($\mathbb{A}^{-1}$)$^{T}$.  Thus, an orthogonal NIF basis would imply $\mathbb{R}$ $=$ ($\mathbb{A}^{T}$)$^{-1}$ $=$ ($\mathbb{A}^{T}$)$^{T}$ $=$ $\mathbb{A}$.  In general, however, the NIF basis vectors are not orthogonal and thus $\mathbb{R}$ $\neq$ $\mathbb{A}$.


\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: David Oliver's book
%%----------------------------------------------------------------------------------------
\section{Notes from the Shaggy Steed of Physics Book}
%%----------------------------------------------------------------------------------------
%%  Subsection: The Action Principle
%%----------------------------------------------------------------------------------------
\subsection{The Action Principle}

\indent  Action \citep{oliver04a} is mathematically defined by the integral:

\begin{equation}
\label{eq:oliver1}
S = \int_{t_{1}}^{t_{2}} L dt
\end{equation}

\noindent  where, \emph{L} is defined as the Lagrangian\footnote{Note: Oliver refers to the Lagrangian as the \textit{the gene of motion}.}.  The action principle can be stated as follows: \emph{of all the possible paths the particles may take between any two given points in space and time, they take those paths for which the action, S, has the least possible value}.  The Lagrangian is really nothing more than the difference between kinetic and potential energy, in Galilean space-time, but in its evolution, nature seeks to minimize any deviation between kinetic and potential energy, regardless of the continual interchange between the two.  In relativistic space-time, the action becomes the dominating factor in what path, for any given particle, the universe will conspire to create.  

\indent  If we define the total energy as:

\begin{equation}
\label{eq:oliver2}
  H = \sum_{\alpha} \textbf{p}_{\alpha} \cdot \dot{\textbf{x}}_{\alpha} - L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)
\end{equation}

\noindent  where \textbf{p}$_{\alpha}$ is the momentum of a particle defined by:

\begin{equation}
\label{eq:oliver3}
  \textbf{p}_{\alpha} \equiv \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \text{ .}
\end{equation}

\noindent  The kinetic energy can be defined as:

\begin{equation}
\label{eq:oliver4}
  T \equiv \frac{1}{2} \sum_{\alpha} \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \cdot \dot{\textbf{x}}_{\alpha} \text{ ,}
\end{equation}

\noindent  and the angular momentum is:

\begin{equation}
\label{eq:oliver5}
  \textbf{J} \equiv \textbf{x}_{\alpha} \times \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \text{ .}
\end{equation}

\indent  We are also met with another way of describing what is meant when one claims \emph{the action must be minimized:}  think of the bottom of a valley as minimum in potential energy.  It can also be said that if the valley is a smoothly varying ''bowl,'' if you will, then one might claim that the bottom of the valley has an approximately zero slope.  What does it mean to have no \emph{slope}?  For any given function, f(q$_{1}$,q$_{2}$,q$_{3}$,$\dotsc$,q$_{n}$), the following statement defines what it means to have \emph{no slope} at some point, q$_{o}$, in space-time:

\begin{equation}
\label{eq:oliver6}
  \frac{\partial f}{\partial q_{i}} \Bigr\rvert_{q_{i} = q_{o}} = 0 \text{ ,}
\end{equation}

\noindent  or one could also say that the \emph{variation} of a function must vanish at some point, q$_{o}$, in space-time:

\begin{equation}
\label{eq:oliver7}
  \delta f = \frac{\partial f}{\partial q_{i}} \delta q_{i} = 0 \text{ ,}
\end{equation}

\noindent  where $\delta$q$_{i}$ are the \emph{arguments} of the function, f.  The variation of the function, f, illustrate how small changes in its arguments, q$_{i}$, cause changes in the function itself.  That means, if the partial derivative of one of the arguments vanishes, the variation in \emph{f} suffers no change (e.g. $\partial$f/$\partial$q$_{i}$ = 0, thus $\delta$f = 0 regardless of $\delta$q$_{i}$).  Since the variation of the arguments, $\delta$q$_{i}$, are arbitrary, the vanishing variation of \emph{f} requires that every partial derivative, $\partial$f/$\partial$q$_{i}$, vanishes at the arbitrary point, q$_{o}$.  Thus, \emph{least action} is define as the conditions for which the functions, q(t), satisfy the requirements to force $\delta$S = 0.  The action is defined as a \emph{functional} $\equiv$ \emph{a quantity that has a single value corresponding to an entire function}.  Thus, the variation of the action is defined as:

\begin{equation}
\label{eq:oliver8}
  \delta S \equiv \int_{t_{1}}^{t_{2}} \delta L dt = \int_{t_{1}}^{t_{2}} \Biggl(\frac{\partial L}{\partial q_{i}}\delta q_{i} + \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} \Biggr) dt
\end{equation}

\noindent  where the second term in the equation is defined as:

\begin{subequations}
  \begin{align}
  \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} & = \frac{\partial L}{\partial \dot{q}_{i}} \frac{d}{dt} \Bigl(\delta q_{i}\Bigr) \label{eq:oliver9} \\
  & = \frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr) - \Biggl[\frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \Biggr)\Biggr] \delta q_{i} \label{eq:oliver10}\text{ .}
  \end{align}
\end{subequations}

\indent  An important thing to note is that the path variations, $\delta$q, vanish at the end points, ($\delta$q,t)$_{1}$ and ($\delta$q,t)$_{2}$.  So we look at the first term in Equation \ref{eq:oliver10} and notice the following:

\begin{equation}
  \label{eq:oliver11}
  \int_{t_{1}}^{t_{2}} \frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr) dt = \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr)\Biggr\rvert_{t_{1}}^{t_{2}} = 0\text{ .}
\end{equation}

\noindent  So we then have the following from Equation \ref{eq:oliver10}:

\begin{equation}
  \label{eq:oliver12}
  \int_{t_{1}}^{t_{2}} \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} dt = - \int_{t_{1}}^{t_{2}}\Biggl[\frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \Biggr)\Biggr] \delta q_{i} dt
\end{equation}

\noindent  which means we've now transformed the second term in Equation \ref{eq:oliver8} into a variation of $\delta$ q$_{i}$, instead of $\delta\dot{q}_{i}$.  This implies that we can do the following:

\begin{equation}
  \label{eq:oliver13}
  \delta S = - \int_{t_{1}}^{t_{2}} \Biggl(\frac{d}{dt} \Bigl(\frac{\partial L}{\partial \dot{q}_{i}} \Bigr) -  \frac{\partial L}{\partial q_{i}}\Biggr)\delta q_{i} dt\text{ .}
\end{equation}

%%----------------------------------------------------------------------------------------
%%  Subsection: Other important definitions
%%----------------------------------------------------------------------------------------
\subsection{Other important definitions}

\textbf{The Total Time Derivative}

\begin{subequations}
  \begin{align}
  \frac{df}{dt} & = \frac{\partial f}{\partial t} + \Biggl(\frac{\partial f}{\partial q_{i}} \dot{q}_{i} + \frac{\partial f}{\partial p_{i}} \dot{p}_{i} \Biggr) \label{eq:oliver14} \\
  & = \frac{\partial f}{\partial t} + \Bigl\{f,\mathcal{H} \Bigr\} \label{eq:oliver15}
  \end{align}
\end{subequations}

\noindent  where \{f,$\mathcal{H}$\} is called a \emph{Poisson bracket} and $\mathcal{H}$ is the Hamiltonian, defined by:

\begin{equation}
  \label{eq:oliver16}
  \Bigl\{f,g \Bigr\} \equiv \Biggl(\frac{\partial f}{\partial q_{i}} \frac{\partial g}{\partial p_{i}} - \frac{\partial g}{\partial q_{i}} \frac{\partial f}{\partial p_{i}}\Biggr) \text{ .}
\end{equation}

\indent  It is important to note that the position-momentum pair is an \emph{antisymmetric manifestation of a symplectic structure}.  Here are some general rules of Poisson brackets:

\begin{enumerate}
  \item If both \emph{f} and \emph{g} are scalars, \{f,g\} is a scalar
  \item If \emph{f} is a vector and \emph{g} is a scalar, \{f,g\} is a vector
  \item If both \emph{f} and \emph{g} are vectors, \{f,g\} is a second rank tensor.
\end{enumerate}

\indent  The Hamilton equations of motion allow/show an example of where this notation can be of constructive use with the following two examples/definitions:

\begin{subequations}
  \begin{align}
    \dot{q}_{i} & = \Bigl\{q_{i},\mathcal{H} \Bigr\} \label{eq:oliver17} \\
    \dot{p}_{i} & = \Bigl\{p_{i},\mathcal{H} \Bigr\} \label{eq:oliver18} \\
    \Bigl\{q_{i},q_{j} \Bigr\} & = 0 \label{eq:oliver19} \\
    \Bigl\{p_{i},p_{j} \Bigr\} & = 0 \label{eq:oliver20} \\
    \Bigl\{q_{i},p_{j} \Bigr\} & = \delta_{ij} \label{eq:oliver21}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \begin{split}
    \Bigl\{J_{i},J_{j} \Bigr\} & = \epsilon_{ijk}J_{k}  \\
    \textbf{J} \cdot \Bigl\{\textbf{J},J_{j} \Bigr\} & = J_{i} \Bigl\{J_{i},J_{j} \Bigr\} \\
    & = \epsilon_{ijk} J_{i} J_{k} \\
    & \equiv 0 \label{eq:oliver22}
   \end{split}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \begin{split}
      \Bigl\{J^{2},J_{j} \Bigr\} & = 0 \\
      & = \Bigl\{J_{i} J_{i},J_{j} \Bigr\} \\
      & = 2 J_{i} \Bigl\{J_{i},J_{j} \Bigr\} \label{eq:oliver23}
   \end{split}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Bigl\{x_{i},J_{j} \Bigr\} & = \epsilon_{ijk} x_{k} \label{eq:oliver24} \\
    \Bigl\{p_{i},J_{j} \Bigr\} & = \epsilon_{ijk} p_{k} \label{eq:oliver25} \\
    \Bigl\{F_{i},J_{j} \Bigr\} & = \epsilon_{ijk} F_{k} \label{eq:oliver26} \\
    \Bigl\{f,J_{j} \Bigr\}     & = 0                    \label{eq:oliver27}
  \end{align}
\end{subequations}

\noindent  (where \textbf{F} is any arbitrary vector function and \emph{f} is any arbitrary scalar).  Phase space is always an even-dimensional manifold that describes the space of motion.  This motion fills phase space with the phase trajectories described by q(t) and p(t).  The essence of configuration space is Euclidean while phase space is symplectic\footnote{\emph{Symplectic structure} is named after the Greek word, $\pi \lambda \epsilon \kappa \acute{o} \varsigma$, meaning \emph{twined} or \emph{braided}.  It is an antisymmetric pairing of coordinates induced by the action principle.}.  If we define the following as the single 2s state vector of phase space:

\begin{equation}
  \label{eq:oliver28}
  \xi = \bigl(q , p \bigr)
\end{equation}

\noindent  where the first s-components of $\xi$ are position coordinates of all the particles in the phase space you're trying to describe.  The \emph{symplectic} is a 2s $\times$ 2s antisymmetric matrix defined as:

\begin{equation}
  \label{eq:oliver29}
 \textit{J} = \left[
  \begin{array}{ c c }
     0 & \textit{I} \\
     -\textit{I} & 0
  \end{array} \right]
\end{equation}

\noindent  where \textit{I} is the identity or unit matrix (of dimension \emph{s}).  The \textit{J} is a perfect example of symplectic space (Oliver calls it the \emph{signature} of symplectic phase space).  It has the following properties:

\begin{equation}
  \label{eq:oliver30}
   \textit{J} = - \textit{J}^{\dagger} = - \textit{J}^{-1} 
\end{equation}

\noindent  where \textit{J}$^{\dagger}$ is defined by:

\begin{equation}
  \label{eq:oliver31}
   \textit{J}_{ij}^{\dagger} = -\textit{J}_{ji}^{*}
\end{equation}

\noindent  or, in other words, the \emph{transpose conjugate}.  The symplectic also has a square:

\begin{equation}
  \label{eq:oliver32}
 \textit{J}^{2} = - \left[
  \begin{array}{ c c }
     0 & \textit{I} \\
     -\textit{I} & 0
  \end{array} \right] = - \textit{J}
\end{equation}

\noindent  and it has the \emph{defining property} of inducing a vanishing scalar product on any phase space vector:

\begin{equation}
  \label{eq:oliver33}
   \xi_{i} \textit{J}_{ij}\xi_{j} = 0 \text{ (often seen as } \xi \textit{J} \xi = 0 \text{).}
\end{equation}

\noindent  The symplectic allows us to rewrite the Poisson bracket notation in a different manner:

\begin{equation}
  \label{eq:oliver34}
   \Bigl\{f,g \Bigr\} = \frac{\partial f}{\partial \xi} \textit{J} \frac{\partial g}{\partial \xi}
\end{equation}

\noindent  which allows one to look at the Poisson bracket of any function, f($\xi$), with the phase space vector, $\xi$:

\begin{equation}
  \label{eq:oliver35}
   \Bigl\{\xi,f \Bigr\} = \textit{J} \frac{\partial f}{\partial \xi} \text{ .}
\end{equation}

\noindent  We are finally allowed to look at the Hamiltonian equations using the symplectic and phase space vectors:

\begin{equation}
  \label{eq:oliver36}
   \dot{\xi} = \Bigl\{\xi,\mathcal{H} \Bigr\} = \textit{J} \frac{\partial \mathcal{H}}{\partial \xi} \text{ .}
\end{equation}

\noindent  A useful relationship between any two quantities, F$\bigl(\xi\bigr)$ and G$\bigl(\xi \bigr)$, can be shown to be:

\begin{equation}
  \label{eq:oliver37}
    \Bigl\{F,G \Bigr\} = \frac{\partial F}{\partial \xi} \textit{J} \frac{\partial G}{\partial \xi} = \frac{\partial F}{\partial \xi} \cdot \Bigl\{\xi,G \Bigr\} = -\frac{\partial G}{\partial \xi} \cdot \Bigl\{\xi,F \Bigr\} \text{ .}
\end{equation}

\noindent  So the Poisson bracket illustrates a number of points:

\begin{enumerate}
  \item The Poisson bracket is a \emph{projection of the normals of the level surfaces of one quantity upon the tangents of the level surfaces of the other}.
  \item If $\bigl\{$F,G$\bigr\}$ $\ne$ 0, the \emph{flow of F} does NOT stay on level surfaces of G, rather it cuts ACROSS them! $\Rightarrow$ G is NOT a constant on the flow of F.
  \item If $\bigl\{$F,G$\bigr\}$ $=$ 0, the \emph{flow of F} not only stays on its own level surface, it is on the level surfaces of G too. $\Rightarrow$ The two quantities become one common surface to which both flows are confined.
  \item Every mechanical quantity, F$\bigl(\xi\bigr)$, has an image in phase space as \emph{sheets} of level surfaces filled with streamlines generated by $\bigl\{ \xi$,F$\bigr\}$ $\equiv$ the flow. $\Rightarrow$ The Poisson bracket describes the intersection of the two flows produced by F$\bigl(\xi\bigr)$ and G$\bigl(\xi \bigr)$.
  \item The \emph{phase flow} is incompressible.
\end{enumerate}

%%----------------------------------------------------------------------------------------
%%  Subsection: Hamilton-Jacobi Theory
%%----------------------------------------------------------------------------------------
\subsection{Hamilton-Jacobi Theory}

\indent  \emph{The motion of the world is imaged as a flow in phase space}.  The manner in which to find these \emph{flows} involves the integrals to Hamilton's equations.  The mathematical forms of the action principle, Hamilton's equations, and Poisson brackets are independent of the coordinate system they are expressed in.  \emph{Motion with s-degrees of freedom has 2s canonical coordinates which form s-conjugate pairs}. $\Rightarrow$ Any set of canonical coordinates is related to another set by transformations which preserve the action principle.

\indent  Consider the set of canonical coordinates (Q,P), where Q = (Q$_{1}$,Q$_{2}$,$ \dotsc$,Q$_{s}$) and P = (P$_{1}$,P$_{2}$,$ \dotsc$,P$_{s}$).  Though it may appear that Q and P are actual position and momentum coordinates, they need not be.  Now Hamilton's equations are:

\begin{subequations}
  \begin{align}
    \dot{Q}_{i} & = \quad \frac{\partial \mathcal{H}}{\partial P_{i}} \label{eq:oliver38} \\
    \dot{P}_{i} & = -\frac{\partial \mathcal{H}}{\partial Q_{i}} \label{eq:oliver39}
  \end{align}
\end{subequations}

\noindent  so that now $\mathcal{H}$ $\rightarrow$ $\mathcal{H}$'$\bigl(Q,P\bigr)$ which still satisfies:

\begin{equation}
  \label{eq:oliver40}
  \delta S = \delta \int_{t_{1}}^{t_{2}} \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr) =  \delta \int_{t_{1}}^{t_{2}} \Bigl(P_{i}dQ_{i} - \mathcal{H}' dt \Bigr) \text{ .}
\end{equation}

\indent  These two integrals may differ by any function, F, which has a vanishing variation (i.e. $\delta$F = 0).  Thus the difference between the integrals in Equation \ref{eq:oliver40} must be the total differential of F:

\begin{equation}
  \label{eq:oliver41}
  dF = p_{i}dq_{i} - P_{i}dQ_{i} - \Bigl(\mathcal{H} - \mathcal{H}'\Bigr)dt
\end{equation}

\noindent  which defines what is referred to as \emph{the generating function}.  The generating function, from Equation \ref{eq:oliver41}, is F = F$\bigl($q,Q,t$\bigr)$.  The relationship of any coordinate can be described as follows:

\begin{subequations}
  \begin{align}
    p_{i} & = \quad \frac{\partial F}{\partial q_{i}} \label{eq:oliver42} \\
    P_{i} & = -\frac{\partial F}{\partial Q_{i}} \label{eq:oliver43} \\
    \Bigl(\mathcal{H} - \mathcal{H}'\Bigr) & = \quad \frac{\partial F}{\partial t} \label{eq:oliver44}
  \end{align}
\end{subequations}

\noindent  but the generating function can be written in a different form as:

\begin{equation}
  \label{eq:oliver45}
  dG = d\Bigl(F + P_{i}Q_{i}\Bigr) = p_{i}dq_{i} + P_{i}dQ_{i} - \Bigl(\mathcal{H} - \mathcal{H}'\Bigr)dt \text{ .}
\end{equation}

\noindent  Here, G = G$\bigl($q,P,t$\bigr)$, where the coordinates are:

\begin{subequations}
  \begin{align}
    p_{i} & = \quad \frac{\partial G}{\partial q_{i}} \label{eq:oliver46} \\
    Q_{i} & = \quad \frac{\partial G}{\partial P_{i}} \label{eq:oliver47} \\
    \Bigl(\mathcal{H} - \mathcal{H}'\Bigr) & = - \frac{\partial G}{\partial t} \label{eq:oliver48}
  \end{align}
\end{subequations}

\begin{enumerate}
  \item The generating function incorporates ONE coordinate from the old pair and ONE coordinate from the new pair.
  \item The canonical transformation presents one of the coordinates explicitly and one of them implicitly.  Meaning, in the transformation from the state (q,p) to the state (Q,P) by the generating function, G$\bigl($q,P,t$\bigr)$, the implicit-explicit nature can be seen in Equations \ref{eq:oliver49} and \ref{eq:oliver50}.
  \item \emph{Explicit Dependence} $\equiv$ A direct relationship between two quantities, e.g. f(t) explicitly depends on $t$ if the variable $t$ exists directly in the function f(t), NOT if a variable in f(t) has a dependence on time.  Meaning, f$\bigl($\textbf{x}(t)$\bigr)$ would not explicitly depend on $t$ UNLESS $t$ existed indpendent of \textbf{x}(t) in the function.
  \item \emph{Implicit Dependence} $\equiv$ An indirect relationship between two quantities, e.g. f$\bigl($\textbf{x}(t)$\bigr)$ implicitly depends on the variable $t$, but \textbf{x} explicitly depends on $t$
\end{enumerate}

\begin{subequations}
  \begin{align}
    \frac{\partial}{\partial q_{i}} G\Bigl(q, P, t\Bigr) & = p_{i} \label{eq:oliver49} \\
    Q_{i} & = \frac{\partial}{\partial P_{i}} G\Bigl(q, P, t\Bigr)  \label{eq:oliver50}
  \end{align}
\end{subequations}

\noindent  where the new coordinates, Q$\bigl($q,p$\bigr)$, are given \textbf{explicitly} by Equation \ref{eq:oliver50} and the coordinates, P$\bigl($q,p$\bigr)$, are given \textbf{implicitly} by Equation \ref{eq:oliver49}.  The two following examples illustrate some trivial transformations:

\begin{subequations}
  \begin{align}
    G\Bigl(q, P, t\Bigr) & = q_{i} P_{i} \label{eq:oliver51} \\
    F\Bigl(q, Q, t\Bigr) & = q_{i} Q_{i} \label{eq:oliver52}
  \end{align}
\end{subequations}

\noindent  yield the following identity and inverse-identity transformations: 1) for G$\bigl($q,P,t$\bigr)$ we have the identity transformation given by:

\begin{subequations}
  \begin{align}
    Q_{i} & = q_{i} \label{eq:oliver53} \\
    P_{i} & = p_{i} \label{eq:oliver54} \\
    \mathcal{H}' & = \mathcal{H} \label{eq:oliver55}
  \end{align}
\end{subequations}

\noindent  and 2) for F$\bigl($q,Q,t$\bigr)$ we have the inverse-identity transformation given by:

\begin{subequations}
  \begin{align}
    Q_{i} & = \quad p_{i} \label{eq:oliver56} \\
    P_{i} & = -q_{i} \label{eq:oliver57} \\
    \mathcal{H}' & = \quad \mathcal{H} \label{eq:oliver58} \text{ .}
  \end{align}
\end{subequations}

\indent  A nineteenth-century astronomer, C.E. Delaunay, used the fact that the transformations must ONLY be canonical in nature by attempting to simplify the problem of motion.  In his attempts, he found coordinates that are now referred to as \emph{elementary flow} coordinates, which occur when one of the coordinates, P$_{i}$ or Q$_{i}$, are selected as constants (i.e. assume we chose P$_{i}$ $\equiv$ I$_{i}$, then $\dot{I}_{i}$ = 0 and Q$_{i}$ $\equiv$ $\alpha_{i}$).  Now Hamilton's equations simplify dramatically to:

\begin{subequations}
  \begin{align}
    \dot{\alpha}_{i} & = \quad \frac{\partial \mathcal{H}'}{\partial I_{i}} \label{eq:oliver59} \\
    \dot{I}_{i} & = -\frac{\partial \mathcal{H}'}{\partial \alpha_{i}} = 0 \label{eq:oliver60} 
  \end{align}
\end{subequations}

\noindent  which simplifies our job of solving Hamilton's equations even more than just having Equation \ref{eq:oliver60} be null.  The reason for the underlying simplicity is due to the coordinate's symplectic union.  This equation also shows us that the Hamiltonian is now only a function of one canonical coordinate, namely $\mathcal{H}$' = $\mathcal{H}$'$\bigl(I\bigr)$.  We may now rewrite the R.H.S. of Equation \ref{eq:oliver61} as a function of only $I$ also:

\begin{equation}
  \label{eq:oliver61}
  \omega_{i}\bigl(I\bigr) \equiv \frac{\partial \mathcal{H}'}{\partial I_{i}} \text{ .}
\end{equation}

\noindent  This reformulation of the coordinates allows for a remarkably trivial integral form, which might otherwise be seemingly impossible:

\begin{equation}
  \label{eq:oliver62}
  \alpha_{i} = \int dt \Bigl(\frac{\partial \mathcal{H}'}{\partial I_{i}}\Bigr) = \omega_{i} t + \beta_{i}
\end{equation}

\noindent  where $\beta_{i}$ (i = 1, 2, $\dotsc$, s) are the integration constants.

\begin{enumerate}
  \item The elementary flow ONLY ''flows'' along the $\alpha$-coordinates $\Rightarrow$ its phase velocity has no components in the invariant coordinate, $I$, since $\dot{I}$ = 0.
  \item The integrals of elementary flow depend upon the two constants of integration, $I$ and $\beta$, which make up the two sets of \emph{s} quantities.
  \item We can define the \emph{Phase Vector} (Equation \ref{eq:oliver63}), the phase vector's \emph{Phase Velocity} (Equation \ref{eq:oliver64}), and the \emph{Integration Constants} (Equation \ref{eq:oliver65}).
\end{enumerate}

\begin{subequations}
  \begin{align}
    \Xi         & = \bigl(\alpha, I\bigr) \label{eq:oliver63} \\
    \Omega      & = \bigl(\omega, 0\bigr) \label{eq:oliver64} \\
    \mathcal{I} & = \bigl(\beta,  I\bigr) \label{eq:oliver65}
  \end{align}
\end{subequations}

\noindent  Thus the elementary flow can be described by:

\begin{equation}
  \label{eq:oliver66}
  \Xi\bigl(t\bigr) = \Omega t + \mathcal{I} \text{ .}
\end{equation}

\indent  The elementary flow can be seen to depend upon the constants of flow, $\mathcal{I}$, which are also invariant in coordinate phase space because: \emph{Quantities invariant on the flow in one set of coordinates are invariant on the IMAGE of this flow in all other canonical coordinates} $\Rightarrow$ they are the 2s invariants of motion!  This is important because the elementary phase space coordinates, $\Xi$ = $\bigl(\alpha, I\bigr)$, are NOT connected to their image phase space coordinates, $\xi$ = $\bigl(q, p\bigr)$, in any simple way (typically a VERY ''ugly'' transformation connects them).  However, the invariants are the ''same'' in both phase spaces, linking the two together, satisfying an important conservation law:

\begin{equation}
  \label{eq:oliver67}
  \Delta\bigl(\mathcal{I}\bigr) = 0 \text{ .}
\end{equation}

\noindent  These invariants also satisfy the condition that its rate of change along the ''flow'' (i.e. its total time derivative) vanishes by:

\begin{equation}
  \label{eq:oliver68}
  \frac{d\mathcal{I}}{dt} = \frac{\partial \mathcal{I}}{\partial t} + \Bigl\{\mathcal{I}, \mathcal{H}\Bigr\} = 0 \text{ .}
\end{equation}

\indent  It is often the case where $\mathcal{I}$ $\ne$ $\mathcal{I}\bigl(t\bigr)$, \textbf{explicitly} (i.e. $\partial \mathcal{I}/\partial t = 0$), but only a function of the canonical phase space coordinates, $\mathcal{I}$ $=$ $\mathcal{I}\bigl(q,p\bigr)$.  Thus any quantity which does \textbf{NOT EXPLICITLY} depend upon time satisfies the following:

\begin{equation}
  \label{eq:oliver69}
  \Bigl\{\mathcal{I}, \mathcal{H}\Bigr\} = 0 \text{ ,}
\end{equation}

\noindent  namely, it's Poisson bracket with the Hamiltonian vanishes.

%%----------------------------------------------------------------------------------------
%%  Subsection: Action Again
%%----------------------------------------------------------------------------------------
\subsection{Action Again}

\indent  Since we can say that the Lagrangian is really just the total time derivative of the action, we can also say:

\begin{equation}
  \label{eq:oliver70}
    dS = -\mathcal{H} dt + p_{i} dq_{i} = \frac{\partial S}{\partial t}dt + \frac{\partial S}{\partial q_{i}}dq_{i} \text{ .}
\end{equation}

\noindent  As one might expect from our previous treatments of such things, we can say:

\begin{subequations}
  \begin{align}
    \mathcal{H}  & = -\frac{\partial S}{\partial t} \label{eq:oliver71} \\
    p_{i}        & = \quad \frac{\partial S}{\partial q_{i}}dq_{i} \label{eq:oliver72} 
  \end{align}
\end{subequations}

\noindent  and since $\mathcal{H}$ $=$ $\mathcal{H}\bigl(q,p\bigr)$, we can rewrite Equation \ref{eq:oliver71} as:

\begin{equation}
  \label{eq:oliver73}
    \frac{\partial S}{\partial t} + \mathcal{H}\Bigl(q,\frac{\partial S}{\partial q}\Bigr) = 0 \text{ .}
\end{equation}

\indent  We now know that S = S$\bigl(q,t\bigr)$ is a solution to the 1$^{st}$-Order partial differential equation in the $s$-position coordinates, $q$, and the time, $t$.  The solution, in general, depends upon $s$ $+$ 1 constants of integration, where one of these constants is purely additive; meaning, if S$\bigl(q,t\bigr)$ is a solution of the Hamilton-Jacobi equation, then S$\bigl(q,t\bigr)$ $+$ $A$ is too (assuming $A$ is an additive constant)\footnote{So, it's not entirely clear why, but the remaining constants must be invariants of motion.}.  We also assume the total energy of our system is constant, meaning, $\mathcal{H}$ $=$ $\mathcal{E}$ $=$ $\partial S/\partial t$, which leads to an action of the form:

\begin{equation}
  \label{eq:oliver74}
    S\Bigl(q,I,t\Bigr) = -\mathcal{E} t + S_{o}\Bigl(q,I\Bigr)
\end{equation}

\noindent  where S$_{o}\Bigl(q,I\Bigr)$ is the time-independent part of the action.  One should also note that the constant, $\mathcal{E}$, is one of the invariants, $I$ $=$ $\bigl(I_{1},I_{2},\dotsc,I_{s}\bigr)$.  So now we have the new momentum-like coordinates, P $\equiv$ $I$ in G$\bigl(q, P, t\bigr)$ $\equiv$ S$_{o}\Bigl(q,I\Bigr)$.  This leaves the remaining coordinates as:

\begin{subequations}
  \begin{align}
    p_{i}      & = \frac{\partial}{\partial q_{i}} S_{o}\Bigl(q,I\Bigr)\label{eq:oliver75} \\
    \alpha_{i} & = \frac{\partial}{\partial I_{i}} S_{o}\Bigl(q,I\Bigr) \label{eq:oliver76} \\
  \end{align}
\end{subequations}

\noindent  or they may also be expressed as:

\begin{subequations}
  \begin{align}
    p_{i}      & = \frac{\partial}{\partial q_{i}} S\Bigl(q,I,t\Bigr) \label{eq:oliver77} \\
    \beta_{i} & = \frac{\partial}{\partial I_{i}} S\Bigl(q,I,t\Bigr) \label{eq:oliver78} \\
  \end{align}
\end{subequations}

\noindent  which leads us to the conclusion that the Hamiltonians in the two sets of coordinates are the same, just of different form, given by:

\begin{equation}
  \label{eq:oliver79}
    \mathcal{H}'\Bigl(I\Bigr) = \mathcal{H}\Bigl(q, p\Bigr) \text{ .}
\end{equation}

\noindent  We already know that we can write the action as an integral of the canonical coordinates:

\begin{equation}
  \label{eq:oliver80}
    S = \int \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr) 
\end{equation}

\noindent  but this form is \emph{clearly} not an invariant\footnote{Due the indefinite nature of the integral and the lack of rotational invaraince in the p$_{i}$dq$_{i}$ terms, the integral can't be said to be an invariant of motion.}, so we reconsider this case as a contour integral over a closed contour, $\gamma$, in phase space as:

\begin{equation}
  \label{eq:oliver81}
    S = \oint_{\gamma} \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr)  \text{ .}
\end{equation}

\indent  One should note that the contour, $\gamma$, itself is not invariant because it is deformed as it is swept through phase space by the flow, however, the integral over this moving contour can be invariant\footnote{The invariance arises from the integration within a closed boundary.  When one considers the integral at hand, one can see we are really integrating the Lagrangian within a closed boundary.  That means, for this integral to NOT be an invariant of motion, requires a violation of the conservation of energy}.  This closed integral is the Poincar$\acute{e}$ invariant.  Though this invariant exists for all motion, its form is completely opaque unless the canonical coordinates are known functions and one can actually solve the integrals. 

%%----------------------------------------------------------------------------------------
%%  Subsection: Hooke Motion
%%----------------------------------------------------------------------------------------
\subsubsection{Hooke Motion}

\indent  As a way to illustrate how these transformations work, we'll consider a few examples.  The first of which is an idea proposed by Robert Hooke, one of Newton's most prominent contemporaries\footnote{Hooke happened to be a rather short man, while Newton was very tall.  Both men did not get along very well, and as a way to mock Hooke, Newton said the famous quote: \emph{If I have seen further it is by standing on ye shoulders of Giants.}} involved a force corresponding to the potential:

\begin{equation}
  \label{eq:oliver82}
    V\bigl(q\bigr) = \frac{\kappa q^{2}}{2} \text{ ,}
\end{equation}

\noindent  where $\kappa$ is a constant.  It gives rise to a force, denoted by:

\begin{equation}
  \label{eq:oliver83}
    -\frac{\partial}{\partial q}V\bigl(q\bigr) = -\kappa q \equiv \mathcal{F} \text{ ,}
\end{equation}

\noindent  which really doesn't correspond to any fundamental force in nature, it's only an approximation to the force between two bodies bound by an elastic material (e.g. a spring)\footnote{Oliver goes into a paragraph-long explanation of how the inverse-square law Coulomb force actually averages out to a linear force when dealing with the macroscopic scale of a spring.  This results in enormous cancelations of forces.}.  The Hamiltonian can be written as:  

\begin{equation}
  \label{eq:oliver84}
    \mathcal{H} = \frac{p^{2}}{2 m} + \frac{\kappa q^{2}}{2} \text{ ,}
\end{equation}

\noindent  corresponding to the Hamiltonian for the simple harmonic oscillator with natural frequency, $\omega_{o}$ $=$ $\sqrt{\kappa/m}$.  The canonical invariant of motion and its elementary flow (like all elementary flows) turns out to be:

\begin{subequations}
  \begin{align}
    I & = \mathcal{H}/\omega_{o} \label{eq:oliver85} \\
    \alpha_{i} & = \omega_{i} + \beta_{i} \label{eq:oliver86} 
  \end{align}
\end{subequations}

\noindent  which gives us a new Hamiltonian, $\mathcal{H}'\bigl(I\bigr)$ $=$ $\omega_{o} I$, and the coordinates, $\bigl(\alpha, I\bigr)$.  The generating function for this case is the action found in Equation \ref{eq:oliver74}.  This can be found from solving Equation \ref{eq:oliver73} for one degree of freedom, which reduces to:

\begin{equation}
  \label{eq:oliver87}
  \frac{dS_{o}}{dq} = p\bigl(q\bigr)
\end{equation}

\indent  After some algebraic manipulation, the integral for S$_{o}$ can be found\footnote{Solve for p(q) in Equation \ref{eq:oliver84} by replacing $\kappa$ with $m\omega_{o}^{2}$, and $\mathcal{H}$ with $\mathcal{H}'$ $=$ I$\omega_{o}$.} to be:

\begin{equation}
  \label{eq:oliver88}
  S_{o}\bigl(q, I\bigr) = \int dq \sqrt{2 m \omega_{o} \Bigl(I - \frac{m \omega_{o} q^{2}}{2}  \Bigr)} \text{ .}
\end{equation}

\clearpage
\appendix
%%----------------------------------------------------------------------------------------
%%  Appendix:  Definitions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Definitions}  \label{app:Definitions}
%%----------------------------------------------------------------------------------------
%%  Appendix:  Symbols and Parameters
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Symbols and Parameters}  \label{subapp:SymbolsandParameters}
\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Fundamental Constants}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\varepsilon{\scriptstyle_{o}}$ $\equiv$ permittivity of free space [F m$^{-1}$ or s$^{2}$ C$^{2}$ kg$^{-1}$ m$^{-3}$]
    \item  $\mu{\scriptstyle_{o}}$ $\equiv$ permeability of free space [H  m$^{-1}$ or kg m C$^{-2}$]
    \item  $c$ $=$ 1/$\sqrt{ \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} }$ $\equiv$ speed of light in vacuum [m s$^{-1}$]
    \item  $e$ $\equiv$ fundamental charge [C]
    \item  k${\scriptstyle_{B}}$ $\equiv$ Boltzmann constant [J K$^{-1}$ or kg m$^{2}$ s$^{-2}$ K$^{-1}$]
  \end{enumerate}
  \item  \textbf{Particle/Plasma-Related Parameters}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  m${\scriptstyle_{s}}$ $\equiv$ mass of particle species $s$ [kg]
    \item  q${\scriptstyle_{s}}$ $\equiv$ charge of particle species $s$ [C]
    \item  n${\scriptstyle_{s}}$ $\equiv$ number density of particle species $s$ [m$^{-3}$]
    \item  $\rho{\scriptstyle_{s}}$ $=$ m${\scriptstyle_{s}}$ n${\scriptstyle_{s}}$ $\equiv$ mass density of particle species $s$ [kg m$^{-3}$]
    \item  B${\scriptstyle_{o}}$ $\equiv$ quasi-static magnetic field magnitude [T]
    \item  \textbf{j} $\equiv$ electrical current density [e.g., from Ampere's law]
    \item  $\omega{\scriptstyle_{ps}}$ $=$ $\sqrt{ n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}/(m{\scriptstyle_{s}} \varepsilon{\scriptstyle_{o}}) }$ $\equiv$ plasma frequency of particle species $s$ [rad s$^{-1}$]
    \item  $\Omega{\scriptstyle_{cs}}$ $=$ q${\scriptstyle_{s}}$B${\scriptstyle_{o}}$/($\gamma$ m${\scriptstyle_{s}}$) $\equiv$ cyclotron frequency of particle species $s$ [rad s$^{-1}$]\footnote{$\gamma$ $\equiv$ Lorentz frame transformation factor $=$ $\left[ 1 - \left( v/c \right)^{2} \right]^{-1/2}$}
    \item  $\mathbf{V}{\scriptstyle_{s}}$ $\equiv$ bulk flow velocity\footnote{refers to the 1st velocity moment} of species $s$ [m s$^{-1}$]
    \item  T${\scriptstyle_{s}}$ $\equiv$ average temperature\footnote{refers to the 2nd velocity moment in the bulk flow rest frame per unit mass} of particle species $s$ [eV or K]
    \item  V${\scriptstyle_{Ts}}$ $=$ $\sqrt{ (2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})/m{\scriptstyle_{s}} }$ $\equiv$ average thermal speed\footnote{here it is the \emph{most probable speed}, whereas V${\scriptstyle_{Ts}}$/$\sqrt{2}$ is the \emph{root mean square speed}} of particle species $s$ [m s$^{-1}$]
    \item  $\lambda{\scriptstyle_{s}}$ $=$ c/$\omega{\scriptstyle_{ps}}$ $\equiv$ inertial length (or skin depth) of particle species $s$ [m]
    \item  $\lambda{\scriptstyle_{Ds}}$ $=$ $\sqrt{ (\varepsilon{\scriptstyle_{o}} k{\scriptstyle_{B}} T{\scriptstyle_{s}})/(n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}) }$ $\equiv$ Debye length of particle species $s$ [m]
    \item  $\rho{\scriptstyle_{cs}}$ $=$ V${\scriptstyle_{Ts}}$/$\omega{\scriptstyle_{ps}}$ $\equiv$ thermal gyroradius of particle species $s$ [m]
    \item  V${\scriptstyle_{A}}$ $=$ $\sqrt{ B{\scriptstyle_{o}}^{2}/( \mu{\scriptstyle_{o}} m{\scriptstyle_{i}} n{\scriptstyle_{i}} ) }$ $\equiv$ Alfv\'{e}n speed [m s$^{-1}$]\footnote{we also refer to an electron Alfv\'{e}n speed, V${\scriptstyle_{Ae}}$, on occasion, but it does not have the same physical significance as V${\scriptstyle_{A}}$}
    \item  C${\scriptstyle_{s}}$ $=$ $\sqrt{ k{\scriptstyle_{B}} \left( Z{\scriptstyle_{i}} \gamma{\scriptstyle_{e}} T{\scriptstyle_{e}} + \gamma{\scriptstyle_{i}} T{\scriptstyle_{i}} \right)/m{\scriptstyle_{i}} }$ $\equiv$ ion sound speed in an electron-ion plasma with differing ratios of specific heats\footnote{typical values are $\gamma{\scriptstyle_{e}}$ $=$ 3 (or 1) and $\gamma{\scriptstyle_{i}}$ $=$ 1}, $\gamma{\scriptstyle_{e}}$ and $\gamma{\scriptstyle_{i}}$, for each species and ion charge state, $Z{\scriptstyle_{i}}$.
    \item  P${\scriptstyle_{s}}$ $=$ n${\scriptstyle_{s}}$ k${\scriptstyle_{B}}$ T${\scriptstyle_{s}}$ $\equiv$ thermal pressure of particle species $s$ [Pa or N m$^{-2}$ or J m$^{-3}$ or kg m$^{-1}$ s$^{-2}$]
  \end{enumerate}
  \item  \textbf{Wave/Fluctuation-Related Parameters}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\omega$ $\equiv$ angular frequency (typically used for waves)
    \item  k $\equiv$ wavenumber [$=$ 2$\pi$/$\lambda$]
    \item  $\lambda$ $\equiv$ wavelength (typically when no subscript is present)
  \end{enumerate}
  \item  \textbf{Useful Relationships}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\omega{\scriptstyle_{pi}}$ $=$ ($c$ $\Omega{\scriptstyle_{ci}}$)/V${\scriptstyle_{A}}$
    \item  $\omega{\scriptstyle_{pe}}$ $=$ ($c$ $\Omega{\scriptstyle_{ce}}$)/V${\scriptstyle_{Ae}}$
  \end{enumerate}
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Terminology and Jargon
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Terminology and Jargon}  \label{subapp:TerminologyandJargon}
\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Phase Front:}  plane of constant phase
  \item  \textbf{V}${\scriptstyle_{ph}}$ $\equiv$ phase velocity
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  the velocity associated with a fixed value of phase $\Rightarrow$ representing an advance of position, \textbf{r}, with t \citep[see page 233 of][]{french71a}
  \end{enumerate}
  \item  \textbf{Dispersive:}  a medium where the phase speed of a wave depends upon the frequency of the wave \citep[see page 398 of][]{griffiths99a}
  \item  \textbf{V}${\scriptstyle_{gr}}$ $\equiv$ group velocity
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  the velocity associated with a modulated envelope, which encloses a group of phase fronts (or short waves)
    \item  so long as the wave source is slowly varying, constructive interference will maximize where the phase is stationary (i.e., where d\textbf{r}/dt $=$ $\partial \omega$/$\partial$\textbf{k}) $\Rightarrow$ the locus of points satisfying this condition define the group velocity \citep[see page 76 of][]{stix92a}
    \item  is perpendicular to a contour of constant $\omega$ in \textbf{k}-space \citep[pages 82-83 of][]{gurnett05}
  \end{enumerate}
  \item  \textbf{Index of Refraction:}  a dimensionless vector that has the direction of \textbf{k} and the magnitude of c/\textbf{V}${\scriptstyle_{ph}}$ (defined as \textbf{n})
  \item  \textbf{Wave Normal Surface:}  the locus of \textbf{V}${\scriptstyle_{ph}}$ azimuthally revolved around \textbf{B}${\scriptstyle_{o}}$ with a 2D cross-section shown as a polar plot of $\omega$/k vs. $\theta$ ($=$ angle between \textbf{k} and \textbf{B}${\scriptstyle_{o}}$)
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  formed by plotting u ($=$ $\omega$/kc $=$ 1/n) vs. $\theta$
    \item  formed by the locus of the tip of the vector \textbf{n}$^{-1}$ $\equiv$ \textbf{n}/n$^{2}$
  \end{enumerate}
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Maxwell Equations
%%----------------------------------------------------------------------------------------
\section{Maxwell Equations}  \label{app:maxwell}

\indent  We start with the Maxwell equations:

\begin{subequations}
  \begin{align}
    \nabla \cdot \mathbf{E} & = \frac{ \rho{\scriptstyle_{c}} }{\varepsilon{\scriptstyle_{o}}}  \label{eq:maxwell_0}  \\
    \nabla \cdot \mathbf{B} & = 0  \label{eq:maxwell_1}  \\
    \nabla \times \mathbf{E} + \frac{ \partial \mathbf{B} }{ \partial t } & = 0  \label{eq:maxwell_2}  \\
    \nabla \times \mathbf{B} & = \mu{\scriptstyle_{o}} \mathbf{j} + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t }  \label{eq:maxwell_3}
  \end{align}
\end{subequations}

\noindent  Since we know from vector calculus that the divergence of the curl of a vector is zero, then from Equation \ref{eq:maxwell_1} we can define the vector potential as:

\begin{equation}
  \label{eq:maxwell_4}
  \mathbf{B} = \nabla \times \mathbf{A}
\end{equation}

\noindent  which we then substitute into Equation \ref{eq:maxwell_2} to get:

\begin{equation}
  \label{eq:maxwell_5}
  \nabla \times \left[ \mathbf{E} + \frac{ \partial \mathbf{A} }{ \partial t } \right] = 0 \text{  .}
\end{equation}

\noindent  From vector calculus, we know that the curl of the gradient of a scalar function is zero, thus we can say:

\begin{equation}
  \label{eq:maxwell_6}
  \mathbf{E} = - \nabla \Phi - \frac{ \partial \mathbf{A} }{ \partial t }
\end{equation}

\noindent  where the negative sign is chosen due to the physical relationship between potentials and forces.  The arbitrariness in the choice of $\mathbf{A}$ and $\Phi$ suggest that the following two operations would not affect the electric or magnetic fields:

\begin{subequations}
  \begin{align}
    \mathbf{A} \rightarrow \mathbf{A}' & = \mathbf{A} + \nabla \Lambda  \label{eq:maxwell_7}  \\
    \Phi \rightarrow \Phi' & = \Phi - \frac{ \partial \Lambda }{ \partial t }  \label{eq:maxwell_8}
  \end{align}
\end{subequations}

\noindent  which means we can choose any ($\mathbf{A}$, $\Phi$) such that they satisfy:

\begin{equation}
  \label{eq:maxwell_9}
  \nabla \cdot \mathbf{A} + \frac{1}{c^{2}} \frac{ \partial \Phi }{ \partial t } = 0 \text{  .}
\end{equation}

\noindent  This allows us to rewrite Equations \ref{eq:maxwell_0} and \ref{eq:maxwell_3} as:

\begin{subequations}
  \begin{align}
    \nabla^{2} \Phi - \frac{1}{c^{2}} \frac{ \partial^{2} \Phi }{ \partial t^{2} } & = - \frac{\rho}{\varepsilon{\scriptstyle_{o}}}  \label{eq:maxwell_10} \\
    \nabla^{2} \mathbf{A} - \frac{1}{c^{2}} \frac{ \partial^{2} \mathbf{A} }{ \partial t^{2} } & = - \mu{\scriptstyle_{o}} \mathbf{j}  \label{eq:maxwell_11}  \text{  .}
  \end{align}
\end{subequations}

\noindent  For more information, see Chapter 6 of \citet[][]{jackson98a}.

%%----------------------------------------------------------------------------------------
%%  Appendix:  Poynting's Theorem
%%----------------------------------------------------------------------------------------
\subsection{Poynting's Theorem} \label{subapp:poynting}

\indent  We can define the, if there exists a continuous distribution of charge and current, the total rate of work done by electromagnetic fields in a volume by:

\begin{equation}
  \label{eq:poynting_0}
  \frac{ d W{\scriptstyle_{EM}} }{ dt } = \int_{V} \thickspace d^{3}x \thickspace \mathbf{j} \cdot \mathbf{E}  \text{  .}
\end{equation}

\noindent  This power represents the rate at which electromagnetic field energy is converted into mechanical or thermal energy.  To conserve energy, the increase in mechanical or thermal energy must be balanced by a decrease in electromagnetic field energy.  We can change the form of this equation and represent it in terms of fields only by using Equation \ref{eq:maxwell_3} to replace $\mathbf{j}$, which results in:

\begin{subequations}
  \begin{align}
    \frac{ d W{\scriptstyle_{EM}} }{ dt } & = \int_{V} \thickspace d^{3}x \thickspace \left[ \frac{1}{\mu{\scriptstyle_{o}}} \nabla \times \mathbf{B} - \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t } \right] \cdot \mathbf{E}  \label{eq:poynting_1a}  \\
    & = \int_{V} \thickspace d^{3}x \thickspace \left[ \frac{1}{\mu{\scriptstyle_{o}}} \mathbf{E} \cdot \left( \nabla \times \mathbf{B} \right) - \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right]  \label{eq:poynting_1b} \\
    & = \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \frac{1}{\mu{\scriptstyle_{o}}} \left[ \mathbf{B} \cdot \left( \nabla \times \mathbf{E} \right) - \nabla \cdot \left( \mathbf{E} \times \mathbf{B} \right) \right] - \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right\rbrace  \label{eq:poynting_1c} \\
    & = - \frac{1}{\mu{\scriptstyle_{o}}} \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \left[ \mathbf{B} \cdot \frac{ \partial \mathbf{B} }{ \partial t } + \nabla \cdot \left( \mathbf{E} \times \mathbf{B} \right) \right] + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right\rbrace  \label{eq:poynting_1d} \\
    & = - \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \nabla \cdot \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) + \frac{ \partial }{ \partial t } \left[ \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } + \frac{ \varepsilon{\scriptstyle_{o}} \left( \mathbf{E} \cdot \mathbf{E} \right) }{ 2 } \right] \right\rbrace  \label{eq:poynting_1e} \\
    - \mathbf{j} \cdot \mathbf{E} & = \nabla \cdot \mathbf{S} + \frac{ \partial }{ \partial t } \left( \mathcal{W}{\scriptstyle_{B}} + \mathcal{W}{\scriptstyle_{E}} \right)  \label{eq:poynting_1f}
%%    & = \int_{V} \thickspace d^{3}x \thickspace \left[  \right]
  \end{align}
\end{subequations}

\noindent  where we have used $\nabla \cdot ( \mathbf{A} \times \mathbf{B})$ $=$ $\mathbf{B} \cdot ( \nabla \times \mathbf{A} )$ - $\mathbf{A} \cdot ( \nabla \times \mathbf{B} )$, Equation \ref{eq:maxwell_2}, and $\mathbf{S}$ is the Poynting flux.  $\mathbf{S}$ is the flow of electromagnetic field energy per unit area per unit time.  Since ($\mathbf{j} \cdot \mathbf{E}$) is a rate of work to convert electromagnetic field energy into a mechanical energy, $E{\scriptstyle_{mech}}$, we can relate this to Newton's 2nd law through the following:

\begin{equation}
  \label{eq:poynting_2}
  \frac{ d \textbf{P}{\scriptstyle_{mech}} }{ dt } = \int_{V} \thickspace d^{3}x \thickspace \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right]
\end{equation}

\noindent  where $\rho{\scriptstyle_{c}}$ is the charge density and \textbf{P}${\scriptstyle_{mech}}$ is the total momenta of all particles in the volume.  Using Equations \ref{eq:maxwell_0} and \ref{eq:maxwell_3}, we can change the integrand to the following:

\begin{subequations}
  \begin{align}
    \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right] & = \varepsilon{\scriptstyle_{o}} \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + \frac{1}{\mu{\scriptstyle_{o}}} \left( \nabla \times \mathbf{B} \right) \times \mathbf{B} - \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t } \times \mathbf{B}  \label{eq:poynting_3a}  \\
    & = \varepsilon{\scriptstyle_{o}} \left[ \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + \mathbf{B} \times \frac{ \partial \mathbf{E} }{ \partial t } - c^{2} \mathbf{B} \times \left( \nabla \times \mathbf{B} \right) \right]  \text{  .}  \label{eq:poynting_3b}
  \end{align}
\end{subequations}

\noindent  We can manipulate this further by using the following:

\begin{subequations}
  \begin{align}
    \mathbf{B} \times \frac{ \partial \mathbf{E} }{ \partial t } & = - \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right) + \mathbf{E} \times \frac{ \partial \mathbf{B} }{ \partial t }  \label{eq:poynting_4a}  \\
    & = - \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right) - \mathbf{E} \times \left( \nabla \times \mathbf{E} \right)  \text{  .}  \label{eq:poynting_4b}
  \end{align}
\end{subequations}

\noindent  Now we can take this result and use $c^{2} \mathbf{B} ( \nabla \cdot \mathbf{B} )$ $=$ 0 to change the terms in brackets to:

\begin{equation}
  \label{eq:poynting_5}
  \begin{split}
    \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right] & = \\
    & \varepsilon{\scriptstyle_{o}} \left\lbrace \left[ \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + c^{2} \mathbf{B} \left( \nabla \cdot \mathbf{B} \right) \right] - \left[ \mathbf{E} \times \left( \nabla \times \mathbf{E} \right) + c^{2} \mathbf{B} \times \left( \nabla \times \mathbf{B} \right) \right] \right\rbrace \\
    & - \varepsilon{\scriptstyle_{o}} \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right)  \text{  .}
  \end{split}
\end{equation}

\noindent  We can simplify this slightly by recognizing the following rule about the divergence of a 2nd rank tensor:

\begin{equation}
  \label{eq:poynting_6}
  \left[ \mathbf{A} \left( \nabla \cdot \mathbf{A} \right) - \mathbf{A} \times \left( \nabla \times \mathbf{A} \right) \right]{\scriptstyle_{\alpha}} = \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left[ A{\scriptstyle_{\alpha}} A{\scriptstyle_{\beta}} - \frac{ \mathbf{A} \cdot \mathbf{A} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right]
\end{equation}

\noindent  where we have used tensor notation\footnote{Meaning, $\partial / \partial x^{\beta}$ $\equiv$ $\partial{\scriptstyle_{\beta}}$ $=$ $\left\lbrace \partial/\partial x^{0}, \nabla \right\rbrace$, where $x^{\beta}$ $\equiv$ \emph{contravariant vector} (or rank one tensor) and $x{\scriptstyle_{\beta}}$ $\equiv$ \emph{covariant vector} \citep[e.g., see Chapter 11 of][]{jackson98a}.}, therefore, we can rewrite Equation \ref{eq:poynting_2} as:

\begin{equation}
  \label{eq:poynting_7}
  \begin{split}
    \frac{ d P{\scriptstyle_{mech, \alpha}} }{ dt } & + \frac{ d }{ dt } \int_{V} \thickspace d^{3}x \thickspace \varepsilon{\scriptstyle_{o}} \left( \mathbf{E} \times \mathbf{B} \right){\scriptstyle_{\alpha}} = \\
    & \varepsilon{\scriptstyle_{o}} \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \left[ \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left( E{\scriptstyle_{\alpha}} E{\scriptstyle_{\beta}} - \frac{ \mathbf{E} \cdot \mathbf{E} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right) \right] + \left[ c^{2} \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left( B{\scriptstyle_{\alpha}} B{\scriptstyle_{\beta}} - \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right) \right] \right\rbrace  \text{  .}
  \end{split}
\end{equation}

\noindent  At this point, we can define the \emph{Maxwell stress tensor}, $T{\scriptstyle_{\alpha \beta}}$, as:

\begin{equation}
  \label{eq:poynting_8}
  T{\scriptstyle_{\alpha \beta}} = \varepsilon{\scriptstyle_{o}} \left[ \left( E{\scriptstyle_{\alpha}} E{\scriptstyle_{\beta}} + B{\scriptstyle_{\alpha}} B{\scriptstyle_{\beta}} \right) - \frac{1}{2} \left( \mathbf{E} \cdot \mathbf{E} + c^{2} \mathbf{B} \cdot \mathbf{B} \right) \delta{\scriptstyle_{\alpha \beta}} \right]
\end{equation}

\noindent  which shows us that Equation \ref{eq:poynting_7} can be rewritten, in component form, as:

\begin{subequations}
  \begin{align}
    \frac{ d }{ dt } \left( \textbf{P}{\scriptstyle_{mech}} + \textbf{P}{\scriptstyle_{EM}} \right){\scriptstyle_{\alpha}} & = \sum_{\beta} \thickspace \int_{V} \thickspace d^{3}x \thickspace \frac{ \partial }{ \partial x^{\beta} } T{\scriptstyle_{\alpha \beta}}  \label{eq:poynting_9a}  \\
    & = \oint_{S} \thickspace dA \thickspace \sum_{\beta} T{\scriptstyle_{\alpha \beta}} \thickspace n{\scriptstyle_{\beta}}  \text{  .}  \label{eq:poynting_9b}
  \end{align}
\end{subequations}

\noindent  We should note that $T{\scriptstyle_{\alpha \beta}}$ is not the rank two antisymmetric field-strength tensor $F^{\alpha \beta}$ $=$ $\partial^{\alpha} A^{\beta}$ - $\partial^{\beta} A^{\alpha}$ (where $A^{\alpha}$ is the 4-vector electromagnetic potentials) and $\partial^{\beta} A^{\lambda}$ $=$ -$F^{\lambda \beta}$ + $\partial^{\lambda} A^{\beta}$, but the two are related through:

\begin{equation}
  \label{eq:poynting_10}
  T^{\alpha \beta} = \frac{1}{4 \pi} \left\lbrace \left[ g^{\alpha \mu} F{\scriptstyle_{\mu \lambda}} F^{\lambda \beta} + \frac{1}{4} g^{\alpha \beta} F{\scriptstyle_{\mu \nu}} F^{\mu \nu} \right] - g^{\alpha \mu} F{\scriptstyle_{\mu \lambda}} \partial^{\lambda} A^{\beta} \right\rbrace  \text{  .}
\end{equation}

\noindent  For more information, see Chapters 6, 11, and 12 of \citet[][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Lorentz Transformation
%%----------------------------------------------------------------------------------------
\subsection{Lorentz Transformation}  \label{subsec:lorentz}

\indent  The general Lorentz transformation for electromagnetic fields is derived from the properties of the second-rank antisymmetric field-strength tensor given by:

\begin{equation}
  \label{eq:lorentz_0}
  F^{\alpha \beta} = \partial^{\alpha} A^{\beta} - \partial^{\beta} A^{\alpha}
\end{equation}

\noindent  where $A^{\alpha}$ [$=$ ($\Phi$,$\mathbf{A}$)] is the scalar and vector potentials.  The covariant form of the inhomogeneous Maxwell's equations are given by:

\begin{equation}
  \label{eq:lorentz_1}
  \partial{\scriptstyle_{\alpha}} F^{\alpha \beta} = \frac{4 \pi}{c} J^{\beta}
\end{equation}

\noindent  where $J^{\alpha}$ [$=$ (c$\rho$,$\mathbf{J}$)] is the 4-vector current density.  The covariant form of the homogeneous Maxwell's equations are given by:

\begin{subequations}
  \begin{align}
    \partial{\scriptstyle_{\alpha}} \mathcal{F}^{\alpha \beta} & = 0  \label{eq:lorentz_2a}  \\
    \mathcal{F}^{\alpha \beta} & = \frac{1}{2} \epsilon^{\alpha \beta \gamma \delta} F{\scriptstyle_{\gamma \delta}}  \label{eq:lorentz_2b}
  \end{align}
\end{subequations}

\noindent  where $\epsilon^{\alpha \beta \gamma \delta}$ is defined on page 556 of \citet{jackson98a}.  Note that the Lorentz force can be written in its covariant form as:

\begin{equation}
  \label{eq:lorentz_3}
  \frac{ d p^{\alpha} }{ d \tau } = m \frac{ d U^{\alpha} }{ d \tau } = \frac{ q }{ c } F^{\alpha \beta} U{\scriptstyle_{\beta}}
\end{equation}

\noindent  which means that the equations of motion are given by:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ c } \frac{ d \mathcal{E} }{ d \tau } & = \frac{ q }{ c } \mathbf{U} \cdot \mathbf{E}  \label{eq:lorentz_4a}  \\
    \frac{ d \mathbf{p} }{ d \tau } & = \frac{ q }{ c } \left( \frac{ \mathcal{E} }{ m c } \mathbf{E} + \mathbf{U} \times \mathbf{B} \right)  \label{eq:lorentz_4b}
  \end{align}
\end{subequations}

\noindent  where $\mathcal{E}$ is the scalar energy, ($U{\scriptstyle_{o}}$,$\mathbf{U}$) is the 4-vector velocity [$U{\scriptstyle_{o}}$ $=$ $\mathcal{E}$/mc], ($p{\scriptstyle_{o}}$,$\mathbf{p}$) is the 4-vector momentum [$=$ m ($U{\scriptstyle_{o}}$,$\mathbf{U}$)], and $\mathbf{E}$ and $\mathbf{B}$ are the electric and magnetic fields.  Because $\mathbf{E}$ and $\mathbf{B}$ are elements of the second-rank tensor F$^{\alpha \beta}$, their values can be expressed in different reference frames in terms of their values in other reference frames \citep[see page 558 of][]{jackson98a}.  Mathematically, this is expressed as:

\begin{equation}
  \label{eq:lorentz_5}
  F'^{\alpha \beta} = \frac{ \partial x'^{\alpha} }{ \partial x^{\gamma} } \frac{ \partial x'^{\beta} }{ \partial x^{\delta} } F^{\gamma \delta}  \text{  .}
\end{equation}

\noindent  For a general Lorentz transformation, $\mathbf{E}$ and $\mathbf{B}$ are transformed as:

\begin{subequations}
  \begin{align}
    \mathbf{E}' & = \gamma \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{E} \right)  \label{eq:lorentz_6a}  \\
    \mathbf{B}' & = \gamma \left( \mathbf{B} - \boldsymbol{\beta} \times \mathbf{E} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{B} \right)  \label{eq:lorentz_6b}
  \end{align}
\end{subequations}

\noindent  where $\boldsymbol{\beta}$ $=$ $\mathbf{v}$/c \citep[see page 558 of][]{jackson98a}.  As an aside, if we have $\mathbf{J}$' $=$ $\sigma$ $\mathbf{E}$' (Ohm's law), where primes denote the rest frame, then the covariant generalization of Ohm's law is written as:

\begin{equation}
  \label{eq:lorentz_7}
  J^{\alpha} - \frac{ 1 }{ c^{2} } \left( U{\scriptstyle_{\beta}} J^{\beta} \right) U^{\alpha} = \frac{ \sigma }{ c } F^{\alpha \beta} U{\scriptstyle_{\beta}}
\end{equation}

\noindent  where we have allowed for the possibility of convection currents and conduction currents \citep[see problem 11.16 of][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Fluid Moment Definitions
%%----------------------------------------------------------------------------------------
\section{Fluid Moment Definitions} \label{app:moments}

\indent  Let us assume we have a function, f${\scriptstyle_{s}}$(\textbf{x},\textbf{v},t), which defines the number of particles of species $s$ in the following way:

\begin{equation}
  \label{eq:app3_0a}
  dN = f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right) \thickspace d^{3}x \thickspace d^{3}v
\end{equation}

\noindent  which tells us that f${\scriptstyle_{s}}$(\textbf{x},\textbf{v},t) is the particle distribution function of species $s$ that defines a probability density in phase space.  We can define moments of the distribution function as expectation values of any dynamical function, g(\textbf{x},\textbf{v}), as:

\begin{equation}
  \label{eq:app3_0b}
  \left< g\left( \textbf{x}, \textbf{v} \right) \right> = \frac{ 1 }{ N } \int d^{3}x \thickspace d^{3}v \thickspace g\left( \textbf{x}, \textbf{v} \right) \thickspace f\left( \textbf{x}, \textbf{v}, t \right)
\end{equation}

\noindent  where $\langle$ $\rangle$ is the average, which can mean ensemble average, arithmetic mean, etc.

\indent  If we define a set of fluid moments with similar format to that of Equations \ref{eq:mean_0c} and \ref{eq:mean_0d}, then we have:

\begin{subequations}
  \begin{align}
    \text{number density:   } n{\scriptstyle_{s}} & = \int d^{3}v \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_0}  \\
    \text{average velocity:   } \textbf{U}{\scriptstyle_{s}} & = \frac{ 1 }{ n{\scriptstyle_{s}} } \int d^{3}v \thickspace \textbf{v}\thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_1}  \\
    \text{kinetic energy density:   } W{\scriptstyle_{s}} & = \frac{ m{\scriptstyle_{s}} }{ 2 } \int d^{3}v \thickspace v^{2} \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_2}  \\
    \text{pressure tensor:   } \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} & = m{\scriptstyle_{s}} \int d^{3}v \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right) \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right) \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_3}  \\
    \text{heat flux tensor:   } \left(\overleftrightarrow{\mathbb{Q}}{\scriptstyle_{s}}\right){\scriptstyle_{i,j,k}} & = m{\scriptstyle_{s}} \int d^{3}v \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{i}} \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{j}} \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{k}} \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_3b}
  \end{align}
\end{subequations}

\noindent  where the pressure tensor can be written as:

\begin{equation}
  \label{eq:app3_4}
  \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} = 
  \begin{bmatrix}
     P{\scriptstyle_{xx}} & P{\scriptstyle_{xy}} & P{\scriptstyle_{xz}}  \\
     P{\scriptstyle_{yx}} & P{\scriptstyle_{yy}} & P{\scriptstyle_{yz}}  \\
     P{\scriptstyle_{zx}} & P{\scriptstyle_{zy}} & P{\scriptstyle_{zz}}
  \end{bmatrix}
\end{equation}

\noindent  which can be reduced to a symmetric tensor (consequence of covariance symmetry, see Section \ref{subsubsec:covariance}) with the only off-diagonal elements being P${\scriptstyle_{xy}}$ $=$ P${\scriptstyle_{yx}}$, P${\scriptstyle_{xz}}$ $=$ P${\scriptstyle_{zx}}$, and P${\scriptstyle_{yz}}$ $=$ P${\scriptstyle_{zy}}$.  In a magnetized plasma, the magnetic field direction can often organize the collective particle motion so that the pressure tensor is reduced to a diagonal tensor.  In general, one can separate the pressure tensor into a diagonal part and an off-diagonal part\footnote{which is usually called the stress tensor}.  The general diagonal elements of the pressure tensor are:

\begin{equation}
  \label{eq:app3_5}
  \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} = 
  \begin{bmatrix}
     P{\scriptstyle_{\perp, 1}} &          0                 & 0  \\
              0                 & P{\scriptstyle_{\perp, 2}} & 0  \\
              0                 &          0                 & P{\scriptstyle_{\parallel}}
  \end{bmatrix}
\end{equation}

\noindent  where a gyrotropic assumption will result in P${\scriptstyle_{\perp, 1}}$ $=$ P${\scriptstyle_{\perp, 2}}$.  Thus, a gyrotropic plasma will have:

\begin{subequations}
  \begin{align}
    P{\scriptstyle_{\perp, s}}     & = n{\scriptstyle_{s}} k{\scriptstyle_{B}} T{\scriptstyle_{\perp, s}}  \label{eq:app3_6a}  \\
    P{\scriptstyle_{\parallel, s}} & = n{\scriptstyle_{s}} k{\scriptstyle_{B}} T{\scriptstyle_{\parallel, s}}  \label{eq:app3_6b}
  \end{align}
\end{subequations}

\noindent  and a non-gyrotropic plasma will have:

\begin{subequations}
  \begin{align}
    T{\scriptstyle_{\perp, s}} & = \frac{ 1 }{ 2 n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \left( P{\scriptstyle_{\perp, 1, s}} + P{\scriptstyle_{\perp, 2, s}} \right)  \label{eq:app3_7a}  \\
    T{\scriptstyle_{\parallel, s}} & = \frac{ 1 }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } P{\scriptstyle_{\parallel, s}} \text{  .}  \label{eq:app3_7b}
  \end{align}
\end{subequations}

\noindent  Therefore, if we have the following relationships:

\begin{subequations}
  \begin{align}
    \text{gyrotropic:   } V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 1 }{ 2 } \left( V{\scriptstyle_{T_{s}, \perp}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_8a}  \\
    \text{non-gyrotropic:   } V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 2 }{ 3 m{\scriptstyle_{s}} } Tr\left[ \frac{ \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \right] }  \label{eq:app3_8b}  \\
    & = \sqrt{ \frac{ 2 k{\scriptstyle_{B}} \left< T{\scriptstyle_{s}} \right> }{ m{\scriptstyle_{s}} }  }  \label{eq:app3_8c}
  \end{align}
\end{subequations}

\noindent  where we have used Tr[] as the trace and defined:

\begin{equation}
  \label{eq:app3_9}
  \left< T{\scriptstyle_{s}} \right> = \frac{ 1 }{ 3 } Tr\left[ \frac{ \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \right] \text{  .}
\end{equation}

\noindent  The average temperature of particle species $s$ shown in Equation \ref{eq:app3_9} is the one most often used when calculating temperatures from electrostatic plasma analyzers \citep[e.g.,][]{curtis89a}.  The temperature is physically a measure of the average kinetic energy density of particle species $s$, and can be represented as:

\begin{subequations}
  \begin{align}
    T{\scriptstyle_{\perp, s}} & = \frac{ 1 }{ 2 } \left(  T{\scriptstyle_{\perp, 1, s}} + T{\scriptstyle_{\perp, 2, s}} \right)  \label{eq:app3_10a}  \\
    \left< T{\scriptstyle_{s}} \right> & = \frac{ 1 }{ 3 } \left(  T{\scriptstyle_{\perp, 1, s}} + T{\scriptstyle_{\perp, 2, s}} + T{\scriptstyle_{\parallel, s}} \right)  \label{eq:app3_10b}
  \end{align}
\end{subequations}

\noindent  therefore, if we already have V${\scriptstyle_{T_{s}, \perp}}$ and V${\scriptstyle_{T_{s}, \parallel}}$ and we assume T${\scriptstyle_{\perp, 1}}$ $\neq$ T${\scriptstyle_{\perp, 2}}$ (i.e., non-gyrotropic)\footnote{In most cases, it is assumed that the electrons are gyrotropic and ions are non-gyrotropic.  Physically, this is due to the relatively long sample period ($\geq$3 s) of current particle detectors compared to $\Omega{\scriptstyle_{ce}}^{-1}$ for electrons, which causes the resulting measured distribution to appeared \textit{smeared out} in phase space.  Most non-gyrotropic features are lost due to the relatively long sample periods.  For ions, however, $\Omega{\scriptstyle_{cp}}^{-1}$ can be $\sim$1-10 s (for B${\scriptstyle_{o}}$ $\sim$ 1-10 nT).  Therefore, non-gyrotropic features (e.g., gyrophase bunching) can often be observed in ion distributions.}, then we have:

\begin{subequations}
  \begin{align}
    V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 1 }{ 3 } \left( V{\scriptstyle_{T_{s}, \perp, 1}}^{2} + V{\scriptstyle_{T_{s}, \perp, 2}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_11a}  \\
    & = \sqrt{ \frac{ 2 V{\scriptstyle_{T_{s}, \perp}}^{2} }{ 3 } + \frac{ V{\scriptstyle_{T_{s}, \parallel}}^{2} }{ 3 } }  \label{eq:app3_11b}  \\
    & \neq \sqrt{ \frac{ 1 }{ 2 } \left( V{\scriptstyle_{T_{s}, \perp}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_11c}
  \end{align}
\end{subequations}

%%  Heat Flux (3rd moment)
\indent  The heat flux tensor (or kinetic energy flux in the bulk flow reference frame), in its general form, is a 3$\times$3$\times$3-element array, which, without symmetries, would have 27 distinct elements.  However, due to symmetries imposed by math\footnote{similar covariance rules to those used to make the pressure tensor symmetric}, assumptions, and physical aspects of fluids, we can reduce this tensor to only its symmetric components (10 total).  The 10 variations of $Q{\scriptstyle_{l,m,n}}$ are:  $Q{\scriptstyle_{x,x,x}}$, $Q{\scriptstyle_{x,y,y}}$, $Q{\scriptstyle_{x,z,z}}$, $Q{\scriptstyle_{x,x,y}}$, $Q{\scriptstyle_{x,x,z}}$, $Q{\scriptstyle_{x,y,z}}$, $Q{\scriptstyle_{y,y,z}}$, $Q{\scriptstyle_{y,z,z}}$, $Q{\scriptstyle_{y,y,y}}$, and $Q{\scriptstyle_{z,z,z}}$.  The result is a simple rank-2 tensor or 3$\times$3 matrix where the sum of the i$^{th}$ row results in the i$^{th}$ component of the resultant \emph{heat flux vector}.  This is how one typically defines a heat flux in practical applications e.g., the solar wind electron heat flux, given by:

\begin{equation}
  \label{eq:hflux_3}
  \vec{\textbf{q}} = \frac{ m{\scriptstyle_{e}} }{ 2 } \int d^{3}v \thickspace f{\scriptstyle_{e}}(\vec{\textbf{x}},\vec{\textbf{v}},t) \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{i}} \right) \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{i}} \right)^{2}
\end{equation}

\noindent  where m${\scriptstyle_{e}}$ is the electron mass, \textbf{U}${\scriptstyle_{i}}$ the bulk flow velocity, and f${\scriptstyle_{e}}$($\vec{\textbf{v}}$,$\vec{\textbf{x}}$,t) represents a general form of the electron velocity distribution function.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Conservation Relations
%%----------------------------------------------------------------------------------------
\section{Conservation Relations} \label{app:rhequations}

\indent  In the case of a planar shock, we can define the conservation relations called the Rankine-Hugoniot relations across the shock ramp.  If we define $\Delta [X]$ $=$ $\langle X \rangle{\scriptstyle_{dn}}$ - $\langle X \rangle{\scriptstyle_{up}}$, where the subscript up(dn) corresponds to upstream(downstream).  Then we have from \citet{vinas86a, koval08a}:

\begin{subequations}
  \begin{align}
    \Delta \left[ G{\scriptstyle_{n}} \right] \equiv & \Delta \left[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \right] = 0  \label{eq:rhequations_0a} \\
    \Delta \left[ B{\scriptstyle_{n}} \right] \equiv & \Delta \left[ \hat{\mathbf{n}} \cdot \mathbf{B} \right] = 0  \label{eq:rhequations_0b} \\
    \Delta \left[ \textbf{S}{\scriptstyle_{t}} \right] \equiv & \Delta \left[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \mathbf{V}{\scriptstyle_{t}} - \frac{ B{\scriptstyle_{n}} }{ \mu{\scriptstyle_{o}} } \mathbf{B}{\scriptstyle_{t}} \right] = 0  \label{eq:rhequations_0c} \\
    \Delta \left[ \textbf{S}{\scriptstyle_{t}} \right] \equiv & \Delta \left[ \left( \hat{\mathbf{n}} \times \mathbf{V}{\scriptstyle_{t}} \right) B{\scriptstyle_{n}} - \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \left( \hat{\mathbf{n}} \times \mathbf{B}{\scriptstyle_{t}} \right) \right] = 0  \label{eq:rhequations_0d} \\
    \Delta \left[ S{\scriptstyle_{n}} \right] \equiv & \Delta \left[ P + \frac{ \mathbf{B}{\scriptstyle_{t}} \cdot \mathbf{B}{\scriptstyle_{t}} }{ 2 \mu{\scriptstyle_{o}} } + \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right)^{2} \right] = 0  \label{eq:rhequations_0e} \\
    \begin{split}
      \Delta \left[ \varepsilon \right] \equiv & \Delta \Bigg[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \left\{ \frac{1}{2} \left( \mathbf{V}{\scriptstyle_{sw}} - V{\scriptstyle_{shn}} \hat{\mathbf{n}} \right)^{2} + \frac{ \gamma }{ \gamma - 1 } \frac{ P }{ \rho } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ \rho \mu{\scriptstyle_{o}} } \right\} \\
      \qquad & - \frac{ B{\scriptstyle_{n}} \left( \mathbf{V}{\scriptstyle_{sw}} - V{\scriptstyle_{shn}} \hat{\mathbf{n}} \right) \cdot \mathbf{B} }{ \mu{\scriptstyle_{o}} } \Bigg] = 0  \label{eq:rhequations_0f}
    \end{split}
  \end{align}
\end{subequations}

\noindent  where we have defined:

\begin{subequations}
  \begin{align}
    Q{\scriptstyle_{n}} & = \mathbf{Q} \cdot \hat{\mathbf{n}}  \label{eq:rhequations_1aa}  \\
    \mathbf{Q}{\scriptstyle_{t}} & = \left( \hat{\mathbf{n}} \times \mathbf{Q} \right) \times \hat{\mathbf{n}}  \label{eq:rhequations_1bb}  \\
    & = \mathbf{Q} \cdot \left( \mathbb{I} - \hat{\mathbf{n}} \hat{\mathbf{n}} \right)  \label{eq:rhequations_1cc}  \\
    V{\scriptstyle_{shn}} & = \frac{ \Delta \left[ \rho \mathbf{V}{\scriptstyle_{sw}} \right] }{ \Delta \left[ \rho \right] } \cdot \hat{\mathbf{n}}  \label{eq:rhequations_1dd}
  \end{align}
\end{subequations}

\noindent  and $\rho$ is the mass density, P is scalar total (ion plus electron) thermal pressure, and $\gamma$ is the ratio of specific heats or polytrope index.  We note that P $=$ $\hat{\mathbf{n}}$ $\cdot$ $\mathbb{P}$ $\cdot$ $\hat{\mathbf{n}}$ $=$ 1/3 Tr[$\mathbb{P}$] $\sim$ n${\scriptstyle_{o}}$ k${\scriptstyle_{B}}$ (T${\scriptstyle_{e}}$ $+$ T${\scriptstyle_{i}}$).

\noindent  The more general form of the above equations are as follows:

\begin{subequations}
  \begin{align}
    \intertext{Maxwell's equations:}
    & \nabla \cdot \mathbf{E} = \frac{ \rho{\scriptstyle_{c}} }{\varepsilon{\scriptstyle_{o}}}  \label{eq:genconseq_0a}  \\
    & \nabla \cdot \mathbf{B} = 0  \label{eq:genconseq_0b}  \\
    & \nabla \times \mathbf{E} + \frac{ \partial \mathbf{B} }{ \partial t } = 0  \label{eq:genconseq_0c}  \\
    & \nabla \times \mathbf{B} = \mu{\scriptstyle_{o}} \mathbf{j} + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t }  \label{eq:genconseq_0d} \\
    \intertext{mass flux continuity equation:}
    & \frac{ \partial \rho }{ \partial t } + \nabla \left( \rho \mathbf{V} \right) = 0  \label{eq:genconseq_0e} \\
    \intertext{charge flux continuity equation:}
    & \frac{ \partial \rho{\scriptstyle_{c}} }{ \partial t } = - \nabla  \cdot \left[ e \left( n{\scriptstyle_{i}} \mathbf{V}{\scriptstyle_{i}} - n{\scriptstyle_{e}} \mathbf{V}{\scriptstyle_{e}} \right) \right] = - \nabla  \cdot \mathbf{j} \label{eq:genconseq_0f} \\
    \intertext{momentum flux continuity equation:}
    & \frac{ \partial }{ \partial t } \left[ \rho \mathbf{V} + \frac{ 1 }{ c^{2} } \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) \right] + \nabla  \cdot \left[ \rho \mathbf{V} \mathbf{V} + \mathbb{P} + \left( \frac{ \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \mathbf{E} }{ 2 } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } \right) \mathbb{I} - \varepsilon{\scriptstyle_{o}} \mathbf{E} \mathbf{E} - \frac{  \mathbf{B} \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right] = 0 \label{eq:genconseq_0g} \\
    \intertext{energy flux continuity equation:}
    & \frac{ \partial }{ \partial t } \left[ \frac{ 1 }{ 2 } \rho \mathbf{V} \cdot \mathbf{V} + \frac{ 3 }{ 2 } P + \left( \frac{ \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \mathbf{E} }{ 2 } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } \right) \right] + \nabla  \cdot \left[ \left( \frac{ 1 }{ 2 } \rho \mathbf{V} \cdot \mathbf{V} + \frac{ 3 }{ 2 } P \right) \mathbf{V} + \mathbb{P} \cdot \mathbf{V} + \mathbf{q} + \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) \right] = 0 \label{eq:genconseq_0h} \\
    \intertext{and generalized Ohm's law:}
    & \frac{ \partial \mathbf{j} }{ \partial t } + \nabla  \cdot \sum_{\alpha} \left( q{\scriptstyle_{\alpha}} n{\scriptstyle_{\alpha}} \mathbf{V}{\scriptstyle_{\alpha}} \mathbf{V}{\scriptstyle_{\alpha}} + \frac{ q{\scriptstyle_{\alpha}} }{ m{\scriptstyle_{\alpha}} } \mathbb{P}{\scriptstyle_{\alpha}} \right) = \sum_{\alpha} \frac{ q{\scriptstyle_{\alpha}}^{2} n{\scriptstyle_{\alpha}} }{ m{\scriptstyle_{\alpha}} } \left( \mathbf{E} + \mathbf{V}{\scriptstyle_{\alpha}} \times \mathbf{B} \right) \label{eq:genconseq_0i}
%%    \frac{  }{  }
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

\clearpage
\setcounter{secnumdepth}{-1}  %%  --> Shut off section counters
%%----------------------------------------------------------------------------------------
%%  Bibliography
%%----------------------------------------------------------------------------------------
\renewcommand{\bibsep}{0pt}  %%  Tighten spacing between references
\addtocontents{toc}{}         %%  Tighten spacing in Table of Contents
\phantomsection               %%  Fix reference link
\section{\emph{References}} \label{sec:References}
\renewcommand{\refname}{}     %%  Shut off default section title for bibliography
\bibliography{/Users/lbwilson/Desktop/Lynn_B_Wilson_III/LaTeX/Bibliographies/my_bib_maker}

%%########################################################################################
%%%%######################################################################################
%%%%%%####################################################################################
%%%%%%%%  End the document
%%%%%%####################################################################################
%%%%######################################################################################
%%########################################################################################
\end{document}