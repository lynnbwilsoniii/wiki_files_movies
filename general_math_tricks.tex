\documentclass[letterpaper,fleqn,11pt]{article}

%%
%%  Graphics packages
%%
\pdfoutput=1
\usepackage[dvips]{graphicx,color}  %%  allow colored text
%\usepackage[dvips,pdftex]{graphicx,color}  %%  allow colored text
\usepackage{array}
\usepackage{epstopdf}  %%  convert EPS --> PDF
\usepackage[tableposition=top]{caption}
\usepackage{wrapfig}
%%
%%  Math/Symbol packages
%%
\usepackage{amsmath}   %%  contains advanced math extensions
\usepackage{latexsym}  %%  adds other symbols in to be used in math mode
\usepackage{amssymb}   %%  adds new symbols in to be used in math mode
\usepackage{mathtools} %%  allows the use of the dcases environment
%%
%%  Bibliography packages
%%
%%  \citep{} --> [Wilson et al., YYYY]
%%  \citet{} --> Wilson et al., [YYYY]
\usepackage[square,authoryear,compress]{natbib}
%%
%%  Color packages
%%
\usepackage{color}
\usepackage[rgb,dvipsnames]{xcolor}
%%  Define the color grey
\definecolor{grey}{gray}{0.5}
%%
%%  Referencing packages
%%
\usepackage{hyperref}     %%  Allow internal and external references
\usepackage{lastpage}     %%  Allow user to know total # of pages (e.g. page 5 of 9)
%%  Setup hyper references
%%    colorlinks, urlcolor, citecolor, and linkcolor --> define appearance of links
%%    the rest --> define options for the PDF viewer when opened
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black, linkcolor=black, bookmarksopen=true, bookmarks=true, pdftoolbar=true, pdfmenubar=true, bookmarksopenlevel=\maxdimen, pdftex, bookmarksnumbered=true, bookmarkstype=toc}
%%
%%  Page Formatting packages
%%
\usepackage{enumitem}
\usepackage[compact]{titlesec}
\usepackage[paperwidth=8.5in, paperheight=11.0in, top=1.25in, bottom=1.25in, left=0.875in, right=0.875in, marginparsep=0pt, textwidth=6.5in, textheight=8.5in, headheight=20.0pt, headsep=35.0pt, footnotesep=5pt, footskip=20pt, bindingoffset=0pt, showframe=false]{geometry}
\usepackage{setspace}
\linespread{1.00}         %%  Set line spacing to single space
\usepackage{indentfirst}  %%  Force first paragraph to indent
\usepackage{fancyhdr}     %%  Allow the use of custom headers and footers
%%
%%  Extra Formatting packages
%%
\usepackage{tocloft}      %%  Allow the user to format the Table of Contents
\usepackage{lastpage}     %%  Allow user to know total # of pages (e.g. page 5 of 9)
%%########################################################################################
%%  Length Formatting Settings
%%########################################################################################
%%  -> Define the margin sizes
\setlength{\topmargin}{-0.25in}
%%  Adjust spacing between last top float or first bottom float and the text
\setlength{\textfloatsep}{1pt}
%%  Adjust spacing before/after an in-text float
\setlength{\intextsep}{7pt}
%%  Adjust spacing before/after each item in enumerate
\setlength{\itemsep}{0pt}
%%  Adjust spacing before/after a paragraph in each item in enumerate
\setlength{\parsep}{0pt}
%%  Adjust spacing before/after a list and paragraph in text
\setlength{\parskip}{0pt}
%%  Adjust spacing before an enumerate environment
\setlength{\topsep}{0pt}
%%  Adjust spacing above/below captions
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{1pt}
%%  Adjust horizontal spacing before each item in enumerate
\setlist{leftmargin=3.5mm}
%%########################################################################################
%%  Table of Contents (ToC) Formatting Settings
%%########################################################################################
%%  Adjust space between lines in TOC
\setlength{\cftbeforesecskip}{-1pt}         %%  Section spacing
\setlength{\cftbeforesubsecskip}{-2pt}      %%  Subsection
\setlength{\cftbeforesubsubsecskip}{-2pt}   %%  Subsubsection
%%  Adjust font size in ToC
\renewcommand{\cftsecfont}{\normalsize\bfseries}
\renewcommand{\cftsubsecfont}{\small}
\renewcommand{\cftsubsubsecfont}{\footnotesize}
%%  Define # of levels labeled in TOCs
%%    1  =  show down to Sections
%%    2  =  show down to Subsections
%%    3  =  show down to Subsubsections
\setcounter{tocdepth}{3}
%%  Define new ToC section name
\renewcommand{\cftmarktoc}{{\bf \Large Table of Contents}}
%%  Define new List of Tables section name
\renewcommand{\cftmarklot}{{\bf \Large List of Tables}}
%%  Define new List of Figures section name
\renewcommand{\cftmarklof}{{\bf \Large List of Figures}}
%%########################################################################################
%%  Section Formatting options
%%########################################################################################
%%  \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before>}[<after>]
%%     (1 em = N points, where N = font size
%%       [e.g., 1 em = 10 pt for 10 pt font ])
\titleformat{\section}[hang]{\Large\bfseries}{\thesection}{10pt}{}  %% horizontal spacing
\titleformat{\subsection}[hang]{\large\bfseries}{\thesubsection}{7pt}{}
\titleformat{\subsubsection}[hang]{\normalsize\bfseries}{\thesubsubsection}{5pt}{}
%%########################################################################################
%%  -> Define some new commands
%%########################################################################################
%%  Define a date command
\newcommand{\thedate}{\today}
%%  Define my name
\newcommand{\myname}{Lynn B. Wilson III}
%%  -> Define the full title
\newcommand{\fulltitle}{General Math Tricks and Other Notes}
%%  -> Define the short title
\newcommand{\shorttitle}{Math and Notes}
%%  -> Define right-header as the section name (actual section name)
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}
\newcommand{\hdrnoter}{{\it \leftmark}}
%%  -> Define the footer notes
\newcommand{\ftrnotel}{\textcolor{grey}{lynn.b.wilsoniii@gmail.com} \\ \textcolor{grey}{lynn.b.wilson@nasa.gov}}
\newcommand{\ftrnotec}{\textcolor{grey}{\myname}}
\newcommand{\ftrnoter}{\thepage\ of \pageref{LastPage}}
%%  -> Define line thickness for header/footer lines
\renewcommand{\headrulewidth}{0.5mm}  %%  set to 0mm if no line desired
\renewcommand{\footrulewidth}{0.5mm}
%%  -> Change header/footer line color to teal
%%     Header Line
\renewcommand{\headrule}{{\color{teal}\hrule width\headwidth height\headrulewidth \vskip-\headrulewidth}}
%%     Footer Line
\renewcommand{\footrule}{{\color{teal}\vskip-\footruleskip\vskip-\footrulewidth
\hrule width\headwidth height\footrulewidth\vskip\footruleskip}}
%%########################################################################################
%%  Headers and Footers
%%########################################################################################
%%  -> Set the page style to use these custom headers/footers
\pagestyle{fancy}
%%  -> Define the headers [Left, Center, Right]
\fancyhead[L]{ \shorttitle }
\chead[]{}
\fancyhead[R]{ \hdrnoter }
%%  -> Define the footers [Left, Center, Right]
\fancyfoot[L]{ \ftrnotel }
\fancyfoot[C]{ \ftrnotec }
\fancyfoot[R]{ \ftrnoter }
%%########################################################################################
%%  Change Table and Figure Labels
%%########################################################################################
%%  -> Force figure labels to be alphanumeric
\renewcommand{\thefigure}{\Alph{figure}}
%%  Change the separator between Figure and the counters to :
\DeclareCaptionLabelSeparator{vline}{: }
%%########################################################################################
%%  Footnotes
%%########################################################################################
%%  Change footnote marks from symbol to numbers
\makeatletter  %%  Allow the use of @-symbol in the followin TeX definition
\let\@fnsymbol\@arabic
\makeatother   %%  Shut-off @-symbol functionality
%%  Change footnote marks
\renewcommand{\thefootnote}{\arabic{footnote}}
%%
%%  Reference Footnotes
%%
%%\usepackage{fmtcount}  %%  Allow user to keep track of counters/labels
%%\renewcommand{\fmtord}[1]{}  %%  remove st, nd, rd, th, etc. from ordinal suffixes
%%########################################################################################
%%  Math formatting options
%%########################################################################################
%%  -> Define horizontal spacing for equations
\setlength{\mathindent}{15pt}
%%########################################################################################
%%%%######################################################################################
%%%%%%####################################################################################
%%%%%%%%  Begin the document
%%%%%%####################################################################################
%%%%######################################################################################
%%########################################################################################
\begin{document}
%%  Define style of bibliography
\bibliographystyle{agufull08}  %%  agu08.bst is distributed by AGU
%%
%%       \titlespacing{ command }{ left }{ before }{ after }[ right ]
%%         before = X plus Y minus Z
%%           => This means that:
%%                X  =  value we would like
%%                Y  =  max value TeX can increase X
%%                Z  =  max value TeX can decrease X
%%
%%  -> Define spacing for floats
\titlespacing{\enumerate}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\figure}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\table}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\threeparttable}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\tablenotes}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
\titlespacing{\wrapfigure}{0pt}{0pt plus 1pt minus 1pt}{0pt plus 1pt minus 1pt}
%%  -> Define spacing for sections
\titlespacing{\section}{0pt}{3pt plus 2pt minus 2pt}{3pt plus 2pt minus 2pt}
\titlespacing{\subsection}{0pt}{2pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}
\titlespacing{\subsubsection}{0pt}{1pt plus 2pt minus 2pt}{1pt plus 2pt minus 2pt}
%%
%%  Format Table/Figure captions
%%
%%  \captionsetup[figure]{figurename=Figure,skip=1pt,font=footnotesize,position=below,justification=RaggedRight,labelsep=vline}
%%  \captionsetup[wrapfigure]{figurename=Figure,labelfont=bf,skip=1pt,font=footnotesize,justification=RaggedRight,labelsep=vline}
%%----------------------------------------------------------------------------------------
%%  Title Page
%%----------------------------------------------------------------------------------------
%%  Page Format and Numbering
\pagestyle{plain}
\pagenumbering{arabic}
%%  Define Title
\title{\bf \fulltitle}
%%  Define Author
\author{\myname}
%%  Define current date
\date{\today}
\maketitle  %%  necessary to make LaTeX print title

%%  Shut off page numbering here
\thispagestyle{empty}
\clearpage
%%----------------------------------------------------------------------------------------
%%  Auto-generated Table of Contents (ToC)
%%----------------------------------------------------------------------------------------
%%  Shut off default ToC section name
\renewcommand{\contentsname}{}
\tableofcontents  %%  Output ToC

%%  Shut off page numbering here
\thispagestyle{empty}
\clearpage


%%  -> Set the page style to use these custom headers/footers
\pagestyle{fancy}
\pagenumbering{arabic}  %%  start page numbering here
\setcounter{page}{1}    %%  set page number to 1
%%  -> Set the equation numbers to number by section
\numberwithin{equation}{section}
%%----------------------------------------------------------------------------------------
%%  Section: Conversion Factors
%%----------------------------------------------------------------------------------------
\section{Conversion Factors}
\indent  The following are useful conversion factors:
\begin{subequations}
  \begin{align}
    1 \frac{ km \thickspace T }{ s } & = 10^{-3} \thickspace \frac{ mV }{ m } = 1 \frac{ \mu V }{ m } \\
    1 \frac{ eV }{ cm^{3} } & = 1.60217646 \times 10^{-4} \thickspace \frac{ J }{ km^{3} } \\
    1 \frac{ \mu W }{ m^{2} } & = 6.24150974 \times 10^{0} \thickspace \frac{ keV \thickspace km }{ s \thickspace cm^{3} } \\
    1 \frac{ keV \thickspace km }{ s \thickspace cm^{3} } & = 1.60217646 \times 10^{-7} \thickspace \frac{ erg }{ s \thickspace cm^{2} } \\
    1 \frac{ erg }{ s \thickspace cm^{2} } & = 10^{3} \thickspace \frac{ \mu W }{ m^{2} } \\
    1 \frac{ s^{3} }{ km^{3} \thickspace cm^{3} } & = 10^{-3} \thickspace \frac{ s^{3} }{ m^{6} } \\
    1 \frac{ \mu V }{ cm } & = 10^{-1} \thickspace \frac{ mV }{ m }
  \end{align}
\end{subequations}

\indent  Let us define q${\scriptstyle_{s}}$ $=$ Z e [$\equiv$ charge of particle species $s$], Z $\equiv$ number of unit charges, m${\scriptstyle_{e}}$(M${\scriptstyle_{s}}$) $\equiv$ mass of electron(ion species $s$), $\mu$ $\equiv$ M${\scriptstyle_{i}}$/M${\scriptstyle_{p}}$, n${\scriptstyle_{s}}$ $\equiv$ number density of particle species $s$, T${\scriptstyle_{s}}$ $\equiv$ average temperature of particle species $s$, and B${\scriptstyle_{o}}$ $\equiv$ magnitude of the quasi-static magnetic field.  In addition, let us define $\omega{\scriptstyle_{ps}}$ $=$ 2$\pi$f${\scriptstyle_{ps}}$ $=$ $\sqrt{ n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}/(m{\scriptstyle_{s}} \varepsilon{\scriptstyle_{o}}) }$ [$\equiv$ plasma frequency of particle species $s$], $\Omega{\scriptstyle_{cs}}$ $=$ 2$\pi$f${\scriptstyle_{cs}}$ $=$ q${\scriptstyle_{s}}$B${\scriptstyle_{o}}$/m${\scriptstyle_{s}}$ [$\equiv$ cyclotron frequency of particle species $s$], V${\scriptstyle_{Ts}}$ $=$ $\sqrt{ (2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})/m{\scriptstyle_{s}} }$ [$\equiv$ average thermal speed of particle species $s$], $\lambda{\scriptstyle_{s}}$ $=$ c/$\omega{\scriptstyle_{ps}}$ [$\equiv$ inertial length (or skin depth) of particle species $s$], $\lambda{\scriptstyle_{Ds}}$ $=$ $\sqrt{ (\varepsilon{\scriptstyle_{o}} k{\scriptstyle_{B}} T{\scriptstyle_{s}})/(n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}) }$ [$\equiv$ Debye length of particle species $s$], $\rho{\scriptstyle_{cs}}$ $=$ V${\scriptstyle_{Ts}}$/$\omega{\scriptstyle_{ps}}$ [$\equiv$ thermal gyroradius of particle species $s$], and V${\scriptstyle_{A}}$ $=$ $\sqrt{ B{\scriptstyle_{o}}^{2}/( \mu{\scriptstyle_{o}} M{\scriptstyle_{i}} n{\scriptstyle_{i}} ) }$ [$\equiv$ Alfv\'{e}n speed]\footnote{we also refer to an electron Alfv\'{e}n speed, V${\scriptstyle_{Ae}}$, on occasion, but it does not have the same physical significance as V${\scriptstyle_{A}}$}.

\indent  Below, the units are defined as follows:  all frequencies are in Hz; distances in meters; speeds in km/s; temperatures in eV; magnetic fields in nT; and densities in cm$^{-3}$.  The approximate factors are:

%%  plasma frequencies
\begin{equation}
  f{\scriptstyle_{pe}} \cong 8.9787 \times 10^{3} \thickspace \sqrt{ n{\scriptstyle_{e}} }
\end{equation}

\begin{equation}
  f{\scriptstyle_{pi}} \cong 209.5353 \thickspace \sqrt{ \frac{ Z^{2} n{\scriptstyle_{i}} }{ \mu } }
\end{equation}

%%  cyclotron frequencies
\begin{equation}
  f{\scriptstyle_{ce}} \cong 27.9925 \thickspace B{\scriptstyle_{o}}
\end{equation}

\begin{equation}
  f{\scriptstyle_{ci}} \cong 1.5245 \times 10^{-2} \thickspace \frac{ Z }{ \mu } B{\scriptstyle_{o}}
\end{equation}

%%  gyroradii
\begin{equation}
  \rho{\scriptstyle_{ce}} \cong 3.3721 \thickspace \frac{\sqrt{ T{\scriptstyle_{e}} }}{ B{\scriptstyle_{o}} }
\end{equation}

\begin{equation}
  \rho{\scriptstyle_{ci}} \cong 144.4970 \thickspace \frac{ \sqrt{ \mu T{\scriptstyle_{i}} } }{ Z B{\scriptstyle_{o}} }
\end{equation}

%%  inertial lengths
\begin{equation}
  \lambda{\scriptstyle_{e}} \cong 5.3141 \times 10^{3} \thickspace n{\scriptstyle_{e}}^{-1/2}
\end{equation}

\begin{equation}
  \lambda{\scriptstyle_{i}} \cong 2.2771 \times 10^{5} \thickspace \sqrt{ \frac{ \mu }{ Z^{2} n{\scriptstyle_{i}} } }
\end{equation}

%%  Debye lengths
\begin{equation}
  \tilde{\lambda}{\scriptstyle_{De}} = \frac{ V{\scriptstyle_{Te}} }{ \sqrt{2} \omega{\scriptstyle_{pe}} } \cong 7.4339 \thickspace \sqrt{ \frac{ T{\scriptstyle_{e}} }{ n{\scriptstyle_{e}} } }
\end{equation}

\begin{equation}
  \lambda{\scriptstyle_{Ds}} = \frac{ V{\scriptstyle_{Ts}} }{ \omega{\scriptstyle_{pe}} } \cong 10.5132 \thickspace \sqrt{ \frac{ T{\scriptstyle_{s}} }{ Z{\scriptstyle_{s}}^{2} n{\scriptstyle_{s}} } }
\end{equation}

%%  thermal speeds
\begin{equation}
  V{\scriptstyle_{Te}} \cong 593.0970 \thickspace \sqrt{ T{\scriptstyle_{e}} }
\end{equation}

\begin{equation}
  V{\scriptstyle_{Ti}} \cong 13.8411 \thickspace \sqrt{ \frac{ T{\scriptstyle_{i}} }{ \mu } }
\end{equation}

\begin{equation}
  \frac{\omega_{pe}}{\Omega_{ce}} \cong 3.21 \times 10^{2} \thickspace \frac{\sqrt{n_{e}(cm^{-3})}}{B(nT)}
\end{equation}

\begin{equation}
  \beta{\scriptstyle_{s}} \cong 0.403 \thickspace \frac{ n{\scriptstyle_{s}} T{\scriptstyle_{s}} }{ B{\scriptstyle_{o}}^{2} }
\end{equation}

\indent  The following are useful relationships:

\begin{subequations}
  \begin{align}
    \omega{\scriptstyle_{pe}} & = \left( \frac{ c }{ V{\scriptstyle_{Ae}} } \right) \Omega{\scriptstyle_{ce}}  \\
    \eta & = \frac{ \nu }{ \varepsilon{\scriptstyle_{o}} \omega{\scriptstyle_{pe}}^{2} }
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section:  General Mathematical Rules
%%----------------------------------------------------------------------------------------
\section{General Mathematical Rules}
%%----------------------------------------------------------------------------------------
%%  Subsection:  The Dirac Delta Function
%%----------------------------------------------------------------------------------------
\subsection{The Dirac Delta Function}  \label{subsec:diracdelta}
Definition = a mathematically \emph{improper} function having the properties :
\begin{enumerate}
  \item $\delta\bigl($x - a$\bigr)$ = 0 for x $\ne$ a
  \item $\smallint$ $\delta\bigl($x - a$\bigr)$ dx = 1 (\emph{if region includes x = a which we'll assume from here on, otherwise it is zero})
  \item $\smallint$ dx f(x) $\delta\bigl($x - a$\bigr)$ = f(a)
  \item $\smallint$ dx f(x) $\delta$'$\bigl($x - a$\bigr)$ = -f'(a)
  \item The delta function transforms according to the rule seen in Equation \ref{eq:dirac_delta1}, assuming f(x) only has simple zeros located at x = x$_{i}$.
  \item In more than one dimension, the delta function can be written as seen in Equation \ref{eq:dirac_delta2}
  \item The delta function has the inverse units of whatever the delta function happens to be a function of $\Rightarrow$ the delta function in Equation \ref{eq:dirac_delta2} has the units of an inverse volume
  \item One can expand a delta function in a Taylor series according to the rules defined in Equations (\ref{eq:delta_func_expan1} - \ref{eq:delta_func_expan4})
  \item Typically one assumes that $\nabla^{2}$(1/r) = 0, assuming r $\ne$ 0 and its volume integral is equal to -4$\pi$.  One can then use the properties of the delta function to say $\nabla^{2}$(1/r) = -4$\pi$ $\delta$(\textbf{x}).  A more general version can be seen in Equation \ref{eq:dirac_delta3}.
\end{enumerate}
\begin{equation}
  \label{eq:dirac_delta0}
    \delta\left(x' - x\right) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} dk e^{ik(x' - x)}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_a}
    \frac{1}{k}\delta\left(k - k'\right) =  \int_{0}^{\infty} d\rho \rho J_{\nu}\bigl(k \rho\bigr) J_{\nu}\bigl(k' \rho\bigr)
\end{equation}
where J$_{\nu}$ are \emph{Bessel Functions} and Re$\bigl\{\nu\bigr\}$ $>$ -1.
\begin{equation}
  \label{eq:dirac_delta0_1}
    \frac{d^{n} \delta\left(x' - x\right)}{dx^{n}} = \delta\left(x' - x\right) \frac{d^{n}}{dx^{n}}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_2}
    \delta\left(xa\right) = \frac{\delta\left(x\right)}{\lvert a \rvert}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_3}
    \delta ' \left(x' - x\right) = \frac{d}{dx} \delta\left(x' - x\right) = -\frac{d}{dx'} \delta\left(x' - x\right)
\end{equation}
\begin{equation}
  \label{eq:dirac_delta0_4}
    \delta\left(x' - x\right) = \frac{d}{dx} \Theta\left(x' - x\right)
\end{equation}
where $\Theta \bigl($x' - x$\bigr)$ is the \emph{Theta Function} which has the properties:
\begin{equation}
  \label{eq:dirac_delta0_5}
\Theta \bigl(x' - x\bigr) = \begin{cases}
  0 & \text{ if (x' - x) $<$ 0}, \\
  1 & \text{ if (x' - x) $>$ 0}.
\end{cases}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta1}
  \delta\bigl(f(x)\bigr) = \sum_{i} \frac{1}{\lvert \frac{df}{dx_{i}} \rvert} \delta\left(x - x_{i}\right) \text{ [x$_{i}$ are the zeros of f(x)]}
\end{equation}
\begin{equation}
  \label{eq:dirac_delta2}
  \delta\left(\textbf{x} - \textbf{x}'\right) = \delta\left(x_{1} - x_{1}'\right) \delta\left(x_{2} - x_{2}'\right) \delta\left(x_{3} - x_{3}'\right) 
\end{equation}
\begin{equation}
  \label{eq:dirac_delta3}
  \nabla^{2} \Biggl(\frac{1}{\lvert \textbf{x} - \textbf{x}' \rvert}\Biggr) = -4\pi \delta \bigl(\textbf{x} - \textbf{x}'\bigr)
\end{equation}
\begin{subequations}
  \begin{align}
    \vec{r}_{i+\delta} & = \vec{r}_{i} + \vec{r}_{\delta i} \label{eq:delta_func_expan1} \\
    \frac{\lvert \vec{r}_{\delta i} \rvert}{\lvert \vec{r}_{i + \delta} \rvert} & \ll 1 \label{eq:delta_func_expan2} \\
    \delta\left(\vec{r} - \vec{r}_{i+\delta}\right) & \rightarrow \delta\left(\vec{r} - \vec{r}_{i} - \vec{r}_{\delta i}\right) \label{eq:delta_func_expan3} \\
    & \approx \delta\left(\vec{r} - \vec{r}_{i}\right) - \vec{r}_{\delta i} \cdot \nabla_{\vec{r}} \Bigl(\delta\left(\vec{r} - \vec{r}_{i}\right) \Bigr) \label{eq:delta_func_expan4} 
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Vector and Tensor Calculus
%%----------------------------------------------------------------------------------------
\subsection{Vector and Tensor Calculus}  \label{subsec:vectortensors}
\indent  If we have an arbitrary vector that is not coplanar with a plane that has a unit normal $\hat{\textbf{n}}$, we can define the vector along the normal (subscript n) and transverse to the normal (subscript t) as:
\begin{subequations}
  \begin{align}
    Q{\scriptstyle_{n}} & = \textbf{Q} \cdot \hat{\textbf{n}}  \label{eq:rhequations_1a}  \\
    \textbf{Q}{\scriptstyle_{t}} & = \left( \hat{\textbf{n}} \times \textbf{Q} \right) \times \hat{\textbf{n}}  \label{eq:rhequations_1b}  \\
    & = \textbf{Q} \cdot \left( \mathbb{I} - \hat{\textbf{n}} \hat{\textbf{n}} \right)  \label{eq:rhequations_1c}
  \end{align}
\end{subequations}

\indent  Note that advection is not necessarily the same as convection.  Convection is the sum of the advective and diffusive effects of a fluid flow.  Diffusion describes the spread of particles through random motion from regions of higher concentration to regions of lower concentration.  Mathematically, this can be shown by considering the advection term, $\nabla$ $\times$ \textbf{V}, separately from the convection term, \textbf{V} $\cdot$ ($\nabla$ \textbf{V}), because the convection term can be rewritten in the following form:
\begin{equation}
  \label{eq:cattell96a_1}
  \textbf{V} \cdot \left(\nabla \textbf{V}\right) = \left[ \nabla \frac{\mid \textbf{V} \mid^{2}}{2} - \left(\nabla \textbf{V}\right) \cdot \textbf{V} \right] + \left(\textbf{V} \cdot \nabla\right) \textbf{V}
\end{equation}
where we have used the vector identity:
\begin{equation}
  \label{eq:cattell96a_2}
  \textbf{A} \times \left( \nabla \times \textbf{B} \right) = \left(\nabla \textbf{B}\right) \cdot \textbf{A} - \left(\textbf{A} \cdot \nabla\right)\textbf{B} 
\end{equation}

\begin{enumerate}
  \item Assume that the vector \textbf{A} and the scalars, $\psi$ and $\phi$, are well behaved vector functions
  \item V $\equiv$ 3D volume with volume element d$^{3}$x
  \item S $\equiv$ is a closed 2D surface bounding volume \emph{V}, with area element \emph{da}
  \item \textbf{n} $\equiv$ unit \emph{outward} normal vector at surface element \emph{da}
\end{enumerate}
\begin{subequations}
  \begin{align}
  \int_{V} d^{3}x \thickspace \nabla \cdot \textbf{A} & = \int_{S} da \thickspace \textbf{n} \cdot \textbf{A} \label{eq:vec_calc1} \\
  \int_{V} d^{3}x \thickspace \nabla \psi & = \int_{S} da \thickspace \textbf{n} \psi \label{eq:vec_calc2} \\
  \int_{V} d^{3}x \thickspace \nabla \times \textbf{A} & = \int_{S} da \thickspace \textbf{n} \times \textbf{A} \label{eq:vec_calc3} \\
  \int_{V} d^{3}x \thickspace \Bigl[\textbf{A} \cdot \Bigl(\nabla \times \bigl(\nabla \times \textbf{B}\bigr) - \textbf{B} \cdot \Bigl(\nabla \times \bigl(\nabla \times \textbf{A}\bigr)\Bigr)\Bigr] & = \int_{S} da \thickspace \textbf{n} \cdot \Bigl[\textbf{B} \times \Bigl(\nabla \times \textbf{A}\Bigr) - \textbf{A} \times \Bigl(\nabla \times \textbf{B}\Bigr)\Bigr]  \label{eq:vec_calc3_2} \\
  \int_{V} d^{3}x \thickspace \Bigl(\phi \nabla^{2} \psi + \nabla \phi \cdot  \nabla \psi\Bigr) & = \int_{S} da \thickspace \phi \bigl(\textbf{n} \cdot  \nabla \psi\bigr) \text{  (Green's 1}^{st}\text{ Identity)} \label{eq:vec_calc4} \\
  \int_{V} d^{3}x \thickspace \Bigl(\phi \nabla^{2} \psi + \psi \nabla^{2} \phi\Bigr) & = \int_{S} da \thickspace \phi \bigl(\phi \nabla \psi - \psi \nabla \phi\bigr) \text{  (Green's Theorem)} \label{eq:vec_calc5}
  \end{align}
\end{subequations}
\begin{enumerate}
  \item In the following equations, we define \emph{S} $\equiv$ open surface
  \item \emph{C} $\equiv$ contour bounding the open surface S, with line element d\textbf{l}
  \item \textbf{n} $\equiv$ normal to the surface \emph{S} with the direction defined by the \emph{right-hand-screw rule} in relation to the direction of d\textbf{l} (i.e. the line integral around contour \emph{C})
\end{enumerate}
\begin{subequations}
  \begin{align}
  \int_{S} da \thickspace \Bigl(\nabla \times \textbf{A} \Bigr) \cdot \textbf{n} & = \oint_{C} \textbf{A} \cdot d\textbf{l} \text{  (Stokes's Theorem)} \label{eq:vec_calc6} \\
  \int_{S} da \thickspace \Bigl(\textbf{n} \times \nabla\Bigr) \psi & = \oint_{C} \psi d\textbf{l} \label{eq:vec_calc7} \\
  \int_{S} da \thickspace \Bigl(\textbf{n} \times \nabla\Bigr) \times \textbf{A} & = \oint_{C} \thickspace d\textbf{l} \times \textbf{A} \label{eq:vec_calc7_2} \\
  \int_{S} da \thickspace \textbf{n} \cdot \Bigl(\nabla f \times \nabla g\Bigr) & = \oint_{C} dg \thickspace f = - \oint_{C} df \thickspace g \label{eq:vec_calc7_3}
  \end{align}
\end{subequations}
\begin{enumerate}
  \item In the following equations, we define \textbf{x} $\equiv$ coordinate of some point with respect to some origin
  \item r $\equiv$ the magnitude of \textbf{x} (= $\lvert$\textbf{x}$\rvert$)
  \item \textbf{k} $\equiv$ \textbf{x}/r = unit radial vector
  \item f(r) $\equiv$ a well-behaved function of r
  \item \textbf{a} $\equiv$ an arbitrary vector
  \item \textbf{L} $\equiv$ the angular momentum operator defined in Equation \ref{eq:vec_calc14}
\end{enumerate}
\begin{subequations}
  \begin{align}
    \nabla \cdot \textbf{x} & = 3 \label{eq:vec_calc8} \\
    \nabla \times \textbf{x} & = 0 \label{eq:vec_calc9} \\
    \nabla \cdot \Bigl[\textbf{n} f(r) \Bigr] & = \frac{2}{r} f(r) + \frac{\partial f}{\partial r} \label{eq:vec_calc10} \\
    \nabla \times \Bigl[\textbf{n} f(r) \Bigr] & = 0 \label{eq:vec_calc11} \\
    \Bigl(\textbf{a} \cdot \nabla\Bigr) \textbf{n} f(r) & = \frac{f(r)}{r} \Bigl[\textbf{a} - \textbf{n}\Bigl(\textbf{a} \cdot \textbf{n} \Bigr) \Bigr] + \textbf{n}\Bigl(\textbf{a} \cdot \textbf{n} \Bigr) \frac{\partial f}{\partial r} \label{eq:vec_calc12} \\
    \nabla \Bigl(\textbf{x} \cdot \textbf{a} \Bigr) & = \textbf{a} + \textbf{x} \Bigl( \nabla \cdot \textbf{a}\Bigr) + i \Bigl(\textbf{L} \times \textbf{a}\Bigr) \label{eq:vec_calc13} \\
    \textbf{L} & = -i \Bigl(\textbf{x} \times \nabla \Bigr) \label{eq:vec_calc14}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \textbf{a} \cdot \bigl(\textbf{b} \times \textbf{c}\bigr) & = \textbf{b} \cdot \bigl(\textbf{c} \times \textbf{a}\bigr) = \textbf{c} \cdot \bigl(\textbf{a} \times \textbf{b}\bigr) \label{eq:vec_calc15} \\
    \textbf{a} \times \bigl(\textbf{b} \times \textbf{c}\bigr) & = \bigl(\textbf{a} \cdot \textbf{c}\bigr)\textbf{b} - \bigl(\textbf{a} \cdot \textbf{b}\bigr)\textbf{c} \label{eq:vec_calc16} \\
    \bigl(\textbf{a} \times \textbf{b}\bigr) \cdot \bigl(\textbf{c} \times \textbf{d}\bigr) & = \bigl(\textbf{a} \cdot \textbf{c}\bigr)\bigl(\textbf{b} \cdot \textbf{d}\bigr) - \bigl(\textbf{a} \cdot \textbf{d}\bigr)\bigl(\textbf{b} \cdot \textbf{c}\bigr) \label{eq:vec_calc17} \\
    \nabla \times \nabla \psi & = 0 \label{eq:vec_calc18} \\
    \nabla \cdot \bigl(\nabla \times \textbf{a}\bigr) & = 0 \label{eq:vec_calc19} \\
    \nabla \times \bigl(\nabla \times \textbf{a}\bigr) & = \nabla \bigl(\nabla \cdot \textbf{a} \bigr) - \nabla^{2}\textbf{a} \label{eq:vec_calc20} \\
    \nabla \cdot \bigl(\psi \textbf{a} \bigr) & = \textbf{a} \cdot \nabla \psi + \psi \nabla \cdot \textbf{a} \label{eq:vec_calc21} \\
    \nabla \times \bigl(\psi \textbf{a} \bigr) & = \nabla \psi \times \textbf{a} + \psi \nabla \times \textbf{a} \label{eq:vec_calc22} \\
    \nabla \bigl(\textbf{a} \cdot \textbf{b}\bigr) & = \bigl(\textbf{a} \cdot \nabla \bigr)\textbf{b} + \bigl(\textbf{b} \cdot \nabla \bigr)\textbf{a} + \textbf{a} \times \bigl(\nabla \times \textbf{b}\bigr) + \textbf{b} \times \bigl(\nabla \times \textbf{a}\bigr) \label{eq:vec_calc23} \\
    \nabla \cdot \bigl(\textbf{a} \times \textbf{b}\bigr) & = \textbf{b} \cdot \bigl(\nabla \times \textbf{a}\bigr) - \textbf{a} \cdot \bigl(\nabla \times \textbf{b}\bigr) \label{eq:vec_calc24} \\
    \nabla \times \bigl(\textbf{a} \times \textbf{b}\bigr) & = \textbf{a}\bigl(\nabla \cdot \textbf{b}\bigr) - \textbf{b}\bigl(\nabla \cdot \textbf{a}\bigr) + \bigl(\textbf{b} \cdot \nabla\bigr)\textbf{a} - \bigl(\textbf{a} \cdot \nabla\bigr)\textbf{b} \label{eq:vec_calc25} \\
    \bigl(\nabla \textbf{b}\bigr) \cdot \textbf{a} & = \textbf{a} \times \bigl(\nabla \times \textbf{b}\bigr) + \bigl(\textbf{a} \cdot \nabla\bigr) \textbf{b} \label{eq:vec_calc26} \\
    \nabla^{2}\textbf{a} & = \nabla \bigl( \nabla \cdot \textbf{a}\bigr) - \nabla \times \bigl(\nabla \times \textbf{a}\bigr) \label{eq:vec_calc27} \\
  \end{align}
\end{subequations}
\begin{equation}
  \label{eq:laplacian_general} 
    \nabla^{2} \equiv \frac{1}{h_{1} h_{2} h_{3}}\Biggl[\frac{\partial}{\partial u_{1}} \Bigl(\frac{h_{2}h_{3}}{h_{1}}\frac{\partial}{\partial u_{1}}\Bigr) +  \frac{\partial}{\partial u_{2}} \Bigl(\frac{h_{1}h_{3}}{h_{2}}\frac{\partial}{\partial u_{2}}\Bigr) + \frac{\partial}{\partial u_{3}} \Bigl(\frac{h_{1}h_{2}}{h_{3}}\frac{\partial}{\partial u_{3}}\Bigr) \Biggr]
\end{equation}
\begin{table}[htp]
  \begin{center}
  \caption{Scale factors of the Laplacian}
  \label{tab:laplacian}
  \begin{tabular}{| l | c | c | c | c | c | c |}
    \hline \hline
    Coord.        & u$_{1}$ & u$_{2}$  & u$_{3}$ & h$_{1}$ & h$_{2}$ &      h$_{3}$   \\
    \hline
    Cartesian     &    x    &    y     &    z    &    1    &    1    &         1      \\
    \hline
    Cylindrical   &    r    &  $\phi$  &    z    &    1    &    r    &         1      \\
    \hline
    Spherical     &    z    & $\theta$ & $\phi$  &    1    &    r    &  r$\sin{\phi}$ \\
    \hline \hline
    Oblate Sph.   &  $\xi$  &  $\eta$  & $\phi$  & a$\sqrt{\sinh^{2}{\xi} + \sin^{2}{\eta}}$ & a$\sqrt{\sinh^{2}{\xi} + \sin^{2}{\eta}}$ & a$\cosh{\xi}\cos{\eta}$ \\
    \hline \hline
    Elliptic Cyl. &    u    &  $\nu$   &    z    & a$\sqrt{\sinh^{2}{u} + \sin^{2}{\nu}}$ & a$\sqrt{\sinh^{2}{u} + \sin^{2}{\nu}}$ &         1      \\
    \hline
  \end{tabular}
  \end{center}
\end{table}
\begin{enumerate}
  \item In the following equations, \emph{dA} $\equiv$ unit surface area
  \item \emph{dV} $\equiv$ unit volume
  \item \emph{ds}$^{2}$ $\equiv$ 1$^{st}$ Fundamental Form of a Line Element or Geodesic Equation of Free Motion
  \item h$_{i}$ $\equiv$ scale factors in the coordinate system metric
  \item g$_{\mu \nu}$ $\equiv$ coordinate system metric
  \item $\Gamma^{\lambda}_{\mu \nu}$ $\equiv$ Christoffel Symbol of the Second Kind
\end{enumerate}
\begin{equation}
  \label{eq:metrics_1}
  h_{i} \equiv \sqrt{g_{ii}} = \sqrt{\sum_{k=1}^{n} \Biggl(\frac{\partial X_{k}}{\partial q_{i}} \Biggr)^{2}}
\end{equation}
\begin{equation}
  \label{eq:metrics_2}
  g_{ij} = g_{ii} \delta_{ij} \text{ (Diagonal Metric)}
\end{equation}
\begin{subequations}
  \begin{align}
    ds^{2} & = g_{11} dx_{1}^{2} + \dotsc + g_{nn} dx_{n}^{2} \label{eq:metrics_3} \\
    & = h_{1}^{2} dx_{1}^{2} + \dotsc + h_{n}^{2} dx_{n}^{2} \label{eq:metrics_4} 
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
  \nabla^{2} \phi & = g^{\mu \nu} \partial_{\mu} \partial_{\nu} \phi - \Gamma^{\mu} \partial_{\nu} \phi \label{eq:metrics_5} \\
  & = g^{\mu \nu} \frac{\partial}{\partial_{\mu}} \Bigl(\frac{\partial \phi}{\partial_{\nu}} \Bigr) - \Gamma^{\mu} \frac{\partial \phi}{\partial_{\nu}} \label{eq:metrics_6}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Gamma^{\lambda} \thinspace _{\mu \nu} & \equiv \frac{\partial^{2} \zeta^{\alpha}}{\partial x^{\mu} \partial x^{\nu}} \frac{\partial x^{\lambda}}{\partial \zeta^{\alpha}} \label{eq:metrics_7} \\
    & = \frac{1}{2} g^{\lambda \alpha} \Bigl[\partial_{\nu} g_{\alpha \mu} + \partial_{\mu} g_{\nu \alpha} - \partial_{\alpha} g_{\mu \nu} \Bigr] \label{eq:metrics_8} \\
    & = g^{\alpha \lambda} \Bigl[\mu \nu , \lambda\Bigr] \label{eq:metrics_9}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Gamma_{\lambda \mu \nu} & = 0 \text{ for } \lambda \ne \mu \ne \nu \label{eq:metrics_10} \\
    \Gamma_{\lambda \lambda \nu} & = -\frac{1}{2} \frac{\partial g_{\lambda \lambda}}{\partial x^{\nu}} \text{ for } \lambda \ne \nu \label{eq:metrics_11} \\
    \Gamma_{\lambda \mu \lambda} & = \Gamma_{\mu \lambda \lambda} = \frac{1}{2} \frac{\partial g_{\lambda \lambda}}{\partial x^{\mu}} \label{eq:metrics_12} \\
    \Gamma^{\nu} \thinspace _{\lambda \mu} & = 0 \text{ for } \lambda \ne \mu \ne \nu \label{eq:metrics_13} \\
    \Gamma^{\nu} \thinspace _{\lambda \lambda} & = -\frac{1}{2 g_{\nu \nu}} \frac{\partial g_{\lambda \lambda}}{\partial x^{\nu}}  \text{ for } \lambda \ne \nu \label{eq:metrics_14} \\
    \Gamma^{\lambda} \thinspace _{\lambda \mu} & = \Gamma^{\lambda} \thinspace _{\mu \lambda} = \frac{1}{2 g_{\lambda \lambda}} \frac{\partial g_{\lambda \lambda}}{\partial x^{\mu}} = \frac{1}{2} \frac{\partial \ln{g_{\lambda \lambda}}}{\partial x^{\mu}} \label{eq:metrics_15}
  \end{align}
\end{subequations}
\begin{equation}
  \label{eq:metrics_16}
  d\tau^{2} \equiv - g_{\mu \nu} dx^{\mu}dx^{\nu}
\end{equation}
\begin{equation}
  \label{eq:metrics_17}
  \nabla_{\mu} V^{\nu} \equiv \partial_{\mu} V^{\nu} + \Gamma^{\nu} \thinspace _{\mu \lambda} V^{\lambda}
\end{equation}
Let x$^{\mu}$ = x$^{\mu}$($\lambda$) then:
\begin{equation}
  \label{eq:metrics_18}
  \frac{d^{2} x^{\mu}}{d\lambda^{2}} + \Gamma^{\mu} \thinspace _{\rho \sigma} \frac{dx^{\rho}}{d\lambda} \frac{dx^{\sigma}}{d\lambda} = 0
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Mean, Variance, Covariance, and Correlation
%%----------------------------------------------------------------------------------------
\subsection{Mean, Variance, Covariance, and Correlation}  \label{subsec:meanvarcovar}

\indent  We will use $\langle$ $\rangle{\scriptstyle_{\alpha}}$ to denote the \emph{arithmetic mean} (or \emph{expectation value} or \emph{average}) with respect to the variable $\alpha$ (e.g., time or space).  These angle brackets act like an operator and can be defined by:
\begin{equation}
  \label{eq:mean_0a}
  \left\langle f \left( x \right) \right\rangle{\scriptstyle_{\alpha}} = \frac{ \int \thickspace d\alpha \thickspace f \left( x \right) }{ \int \thickspace d\alpha }
\end{equation}
for a continuous function\footnote{in general, the denominator is not present and the $\langle$ $\rangle{\scriptstyle_{\alpha}}$ is an unnormalized average} or
\begin{equation}
  \label{eq:mean_0b}
  \left\langle x \right\rangle = \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}}
\end{equation}
for discrete variates, $x{\scriptstyle_{i}}$.  We will use $\mu{\scriptstyle_{2}}$, $\sigma^{2}$, or $var()$ to denote the \emph{variance} and $cov(x,y)$ to denote the \emph{covariance}.  Finally, we will denote the \emph{correlation} as $cor(x,y)$.  \\
\indent  The $\mu{\scriptstyle_{n}}$ notation denotes the $n$-th moment of some probability distribution, P(x), for some function, f(x).  In general, the moments are called \emph{raw moments} ($\mu{\scriptstyle_{n}}$), as they are not centered on any significant value of $x$.  Herein, we will use \emph{central moments} ($\bar{\mu}{\scriptstyle_{n}}$), which are centered on the \emph{mean}.  The general form of the $n$-th central moment is:
\begin{subequations}
  \begin{align}
    \bar{\mu}{\scriptstyle_{n}} & \equiv \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{n} \right\rangle  \label{eq:mean_0c}  \\
    & = \int \thickspace dx \thickspace \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{n} \thickspace P\left( x \right)  \label{eq:mean_0d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

where the integral is changed to a summation for discrete f(x).  The various moments of a distribution are defined as:

\begin{subequations}
  \begin{align}
    \text{  normalization  } & \equiv \mu{\scriptstyle_{0}}  \label{eq:moms_0a}  \\
    & \equiv \text{  density for particle velocity distributions  }  \notag \\
    \text{  mean  } & \equiv \mu{\scriptstyle_{1}} = \left\langle f\left( x \right) \right\rangle  \label{eq:moms_0b}  \\
    & \equiv \text{  bulk flow velocity for particle velocity distributions  }  \notag \\
    \bar{\mu}{\scriptstyle_{1}} & = \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right] \right\rangle = \left\langle f\left( x \right) \right\rangle - \left\langle f\left( x \right) \right\rangle = 0  \label{eq:moms_0c}  \\
    \text{  variance  } & \equiv \bar{\mu}{\scriptstyle_{2}} = \left\langle \left[ f\left( x \right) - \left\langle f\left( x \right) \right\rangle \right]^{2} \right\rangle  \label{eq:moms_0d}  \\
    & \equiv \text{  pressure tensor for particle velocity distributions  }  \notag \\
    \text{  skewness  } & \equiv \frac{ \bar{\mu}{\scriptstyle_{3}} }{ \bar{\mu}{\scriptstyle_{2}}^{3/2} } = \text{  measure of asymmetry of a distribution  }  \label{eq:moms_0e}  \\
    & \equiv \text{  heat flux tensor for particle velocity distributions  }  \notag \\
    \text{  kurtosis  } & \equiv \frac{ \bar{\mu}{\scriptstyle_{4}} }{ \bar{\mu}{\scriptstyle_{2}}^{2} } = \text{  degree of peakedness of a distribution  }  \label{eq:moms_0f}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Mean
%%----------------------------------------------------------------------------------------
\subsubsection{Mean}  \label{subsubsec:mean}
\noindent  The \emph{mean} satisfies the following relations:
\begin{subequations}
  \begin{align}
    \langle a x + b y \rangle & = a \langle x \rangle + b \langle y \rangle  \label{eq:mean_1a}  \\
    \langle f \left( x \right) + g \left( x \right) \rangle & = \langle f \left( x \right) \rangle + \langle g \left( x \right) \rangle  \label{eq:mean_1b}  \\
    \langle f \left( x \right) \cdot g \left( x \right) \rangle & = \langle f \left( x \right) \rangle \cdot \langle g \left( x \right) \rangle  \label{eq:mean_1c}  \\
    \langle a x + b \rangle & = a \langle x \rangle + b  \label{eq:mean_1d}  \\
    \langle \bar{x} \rangle & = \left\langle \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1e}  \\
    & = \frac{ 1 }{ N } \left\langle \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1f}  \\
    & = \frac{ 1 }{ N } \sum_{i = 1}^{N} \left\langle x{\scriptstyle_{i}} \right\rangle  \label{eq:mean_1g}  \\
    & = \frac{ 1 }{ N } \sum_{i = 1}^{N} \mu  \label{eq:mean_1h}  \\
    & = \frac{ 1 }{ N } \left( N \mu \right)  \label{eq:mean_1i}  \\
    & \equiv \mu  \label{eq:mean_1j}
%%    \langle  \rangle & = 
  \end{align}
\end{subequations}
where $\mu$ is the \emph{population mean} or the first moment of the central moments defined by:
\begin{equation}
  \label{eq:mean_2}
  \bar{\mu}{\scriptstyle_{n}} = \left\langle \left( x - \left\langle x \right\rangle \right)^{n} \right\rangle  \text{  .}
\end{equation}
\noindent  The \emph{mean} of a bivariate function satisfies the following relationship:
\begin{subequations}
  \begin{align}
    \left\langle \left( x - \mu{\scriptstyle_{x}} \right) \left( y - \mu{\scriptstyle_{y}} \right) \right\rangle & = \left\langle  x y + \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} - x \mu{\scriptstyle_{y}} - y \mu{\scriptstyle_{x}} \right\rangle  \label{eq:mean_3a}  \\
    & = \left\langle x y \right\rangle + \left\langle \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} \right\rangle - \left\langle x \mu{\scriptstyle_{y}} \right\rangle - \left\langle y \mu{\scriptstyle_{x}} \right\rangle  \label{eq:mean_3b}  \\
    & = \left\langle x y \right\rangle + \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}} - \mu{\scriptstyle_{y}} \left\langle x \right\rangle - \mu{\scriptstyle_{x}} \left\langle y \right\rangle  \label{eq:mean_3c}  \\
    & = \left\langle x y \right\rangle - \mu{\scriptstyle_{x}} \mu{\scriptstyle_{y}}  \label{eq:mean_3d}  \\
    & = \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle  \text{  .}  \label{eq:mean_3e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Variance
%%----------------------------------------------------------------------------------------
\subsubsection{Variance}  \label{subsubsec:variance}
\noindent  The \emph{variance} is given by:
\begin{subequations}
  \begin{align}
    var\left( x \right) & = \left\langle \left( x - \left\langle x \right\rangle \right)^{2} \right\rangle  \label{eq:variance_0a}  \\
    & = \left\langle \left( x^{2} + \left\langle x \right\rangle^{2} - 2 x \left\langle x \right\rangle \right) \right\rangle  \label{eq:variance_0b}  \\
    & = \left\langle x^{2} \right\rangle + \left\langle \left\langle x \right\rangle^{2} \right\rangle - 2 \left\langle x \left\langle x \right\rangle \right\rangle  \label{eq:variance_0c}  \\
    & = \left\langle x^{2} \right\rangle - \left\langle x \right\rangle^{2}  \text{  .}  \label{eq:variance_0d}
%%    \left\langle  \right\rangle & = 
  \end{align}
\end{subequations}
The \emph{variance} satisfies the following relationship:
\begin{subequations}
  \begin{align}
    var\left( \bar{x} \right) & = var\left\{ \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\}  \label{eq:variance_1a}  \\
    & = \left\langle \left[ \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} - \left\langle \frac{ 1 }{ N } \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1b}  \\
    & = \frac{ 1 }{ N^{2} } \left\langle \left[ \sum_{i = 1}^{N} x{\scriptstyle_{i}} - \left\langle \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1c}  \\
    & = \frac{ 1 }{ N^{2} } var\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right\}  \label{eq:variance_1d}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} var\left\{ x{\scriptstyle_{i}} \right\}  \label{eq:variance_1e}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} \left\langle \left[ x{\scriptstyle_{i}} - \left\langle x{\scriptstyle_{i}} \right\rangle \right]^{2} \right\rangle  \label{eq:variance_1f}  \\
    & = \frac{ 1 }{ N^{2} } \sum_{i = 1}^{N} \sigma^{2} = \frac{ \sigma^{2} }{ N }  \text{  .}  \label{eq:variance_1g}
%%    \left\langle  \right\rangle & = 
  \end{align}
\end{subequations}
The \emph{variance} also satisfies the following relationship:
\begin{subequations}
  \begin{align}
    var\left( a x + b \right) & = \left\langle \left[ \left( a x + b \right) - \left\langle a x + b \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2a}  \\
    & = \left\langle \left[ \left( a x \right) - a \left\langle x \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2b}  \\
    & = \left\langle a^{2} \left[ x - \left\langle x \right\rangle \right]^{2} \right\rangle  \label{eq:variance_2c}  \\
    & = a^{2} var\left( x \right)  \text{  .}  \label{eq:variance_2d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
The \emph{variance} of two random variates is given by:
\begin{subequations}
  \begin{align}
    var\left( x + y \right) & = var\left( x \right) + var\left( y \right) + 2 cov\left( x, y \right)  \label{eq:variance_3a}  \\
    & = \sigma{\scriptstyle_{x}}^{2} + \sigma{\scriptstyle_{y}}^{2} + 2 cov\left( x, y \right)  \label{eq:variance_3b}  \\
    var\left( y - b x \right) & = var\left( y \right) + var\left( - b x \right) + 2 cov\left( y, - b x \right)  \label{eq:variance_3c}  \\
    & = var\left( y \right) + b^{2} var\left( x \right) - 2 b cov\left( y, x \right)  \label{eq:variance_3d}  \\
    & = \sigma{\scriptstyle_{y}}^{2} + b^{2} \sigma{\scriptstyle_{x}}^{2} - 2 b \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} cor\left( y, x \right)  \label{eq:variance_3e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
where we have used $\sigma{\scriptstyle_{x}}$ $\equiv$ $\sqrt{ var\left( x \right) }$ (also known as the \emph{standard deviation}), $cov\left( y, x \right)$ $\equiv$ the \emph{covariance} (see Section \ref{subsubsec:covariance} for more), and $cor\left( y, x \right)$ $\equiv$ the \emph{correlation} (see Section \ref{subsubsec:correlation} for more).
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Covariance
%%----------------------------------------------------------------------------------------
\subsubsection{Covariance}  \label{subsubsec:covariance}
\indent  The \emph{covariance} is given by:
\begin{equation}
  \label{eq:covariance_0a}
  cov\left( x, y \right) \equiv \left\langle \left( x - \mu{\scriptstyle_{x}} \right) \left( y - \mu{\scriptstyle_{y}} \right) \right\rangle = \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle 
\end{equation}
where we note that:
\begin{equation}
  \label{eq:covariance_1a}
  cov\left( x, x \right) = \left\langle x^{2} \right\rangle - \left\langle x \right\rangle^{2} = var\left( x \right)  \text{  .}
\end{equation}
\noindent  The \emph{covariance} can also be related to the \emph{variance} through:
\begin{subequations}
  \begin{align}
    var\left( \sum_{i = 1}^{N} x{\scriptstyle_{i}} \right) & = cov\left( \sum_{i = 1}^{N} x{\scriptstyle_{i}}, \sum_{j = 1}^{N} x{\scriptstyle_{j}} \right)  \label{eq:covariance_2a}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2b}  \\
    & = \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j = i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right) + \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j \neq i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2c}  \\
    & = \sum_{i = 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{i}} \right) + \sum_{i = 1}^{N} \sum_{\substack{ j = 1 \\ j \neq i }}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_2d}  \\
    & = \sum_{i = 1}^{N} var\left( x{\scriptstyle_{i}} \right) + 2 \sum_{i = 1}^{N} \sum_{j = i + 1}^{N} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \text{  .}  \label{eq:covariance_2e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
\noindent  The \emph{covariance} has the following property for linear sums:
\begin{subequations}
  \begin{align}
    var\left( \sum_{i = 1}^{N} a{\scriptstyle_{i}} x{\scriptstyle_{i}} \right) & = cov\left( \sum_{i = 1}^{N} a{\scriptstyle_{i}} x{\scriptstyle_{i}}, \sum_{j = 1}^{N} a{\scriptstyle_{j}} x{\scriptstyle_{j}} \right)  \label{eq:covariance_3a}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{N} a{\scriptstyle_{i}} a{\scriptstyle_{j}} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \label{eq:covariance_3b}  \\
    & = \sum_{i = 1}^{N} a{\scriptstyle_{i}}^{2} var\left( x{\scriptstyle_{i}} \right) + 2 \sum_{i = 1}^{N} \sum_{j = i + 1}^{N} a{\scriptstyle_{i}} a{\scriptstyle_{j}} cov\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right)  \text{  .}  \label{eq:covariance_3c}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
We use the results for the \emph{variance} of two random variates is given by Equations \ref{eq:variance_3a} -- \ref{eq:variance_3e}, which allows us to calculate:
\begin{subequations}
  \begin{align}
    cov\left( x + z, y \right) & = \left\langle \left[ \left( x + z \right) - \left\langle x + z \right\rangle \right] \cdot \left[ \left( y \right) - \left\langle y \right\rangle \right] \right\rangle  \label{eq:covariance_5a}  \\
    & = \left\langle \left( x + z \right) y \right\rangle - \left\langle x + z \right\rangle \left\langle y \right\rangle  \label{eq:covariance_5b}  \\
    & = \left\langle x y + z y \right\rangle - \left[ \left\langle x \right\rangle + \left\langle z \right\rangle \right] \left\langle y \right\rangle  \label{eq:covariance_5c}  \\
    & = \left\langle x y \right\rangle + \left\langle z y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle - \left\langle z \right\rangle \left\langle y \right\rangle  \label{eq:covariance_5d}  \\
    & = \left[ \left\langle x y \right\rangle - \left\langle x \right\rangle \left\langle y \right\rangle \right] + \left[ \left\langle z y \right\rangle - \left\langle z \right\rangle \left\langle y \right\rangle \right]  \label{eq:covariance_5e}  \\
    & = cov\left( x, y \right) + cov\left( z, y \right)  \text{  .}  \label{eq:covariance_5f}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
\noindent  Finally, the \emph{covariance} satisfies the following:
\begin{subequations}
  \begin{align}
    cov\left( x, a \right) & = 0  \label{eq:covariance_6a}  \\
    cov\left( a x, b y \right) & = a b \thickspace cov\left( x, y \right)  \label{eq:covariance_6b}  \\
    cov\left( x + a, y + b \right) & = cov\left( x, y \right)  \label{eq:covariance_6c}  \\
    cov\left( a x + b y, c w + d z \right) & = a c \thickspace cov\left( x, w \right) + a d \thickspace cov\left( x, z \right) + b c \thickspace cov\left( y, w \right) + b d \thickspace cov\left( y, z \right)  \label{eq:covariance_6d}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
where the proofs are just a few lines of algebra following the rules for the \emph{variance} and the \emph{mean}.  The \emph{covariance matrix} is given by:
\begin{equation}
  \label{eq:covariance_7a}
  V{\scriptstyle_{ij}} = cor\left( x{\scriptstyle_{i}}, x{\scriptstyle_{j}} \right) \equiv \left\langle \left( x{\scriptstyle_{i}} - \mu{\scriptstyle_{i}} \right) \cdot \left( x{\scriptstyle_{j}} - \mu{\scriptstyle_{j}} \right) \right\rangle
\end{equation}
where an individual matrix element, $V{\scriptstyle_{ij}}$, is called the covariance of $x{\scriptstyle_{i}}$ and $x{\scriptstyle_{j}}$.  \\
\indent  The \emph{covariance}, in the form of Equation \ref{eq:covariance_0a}, is similar, physically, to the \emph{pressure} in kinetic theory.  More generally, the \emph{covariance matrix} is analogous to the \emph{pressure tensor} in kinetic theory\footnote{just replace $x{\scriptstyle_{i}}$ with a component of the particle momentum(velocity) and $\mu{\scriptstyle_{i}}$ with the first moment, or bulk flow momentum(velocity)}.  Recall that the \emph{pressure tensor} is symmetric, which is the result of the following:
\begin{subequations}
  \begin{align}
    cov\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}}, y \right\} & = \sum_{i = 1}^{N} cov\left\{ x{\scriptstyle_{i}}, y \right\}  \label{eq:covariance_8a}  \\
    cov\left\{ \sum_{i = 1}^{N} x{\scriptstyle_{i}}, \sum_{j = 1}^{M} y{\scriptstyle_{j}} \right\} & = \sum_{i = 1}^{N} cov\left\{ x{\scriptstyle_{i}}, \sum_{j = 1}^{M} y{\scriptstyle_{j}} \right\}  \label{eq:covariance_8b}  \\
    & = \sum_{i = 1}^{N} cov\left\{ \sum_{j = 1}^{M} y{\scriptstyle_{j}}, x{\scriptstyle_{i}} \right\}  \label{eq:covariance_8c}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{M} cov\left\{ y{\scriptstyle_{j}}, x{\scriptstyle_{i}} \right\}  \label{eq:covariance_8d}  \\
    & = \sum_{i = 1}^{N} \sum_{j = 1}^{M} cov\left\{ x{\scriptstyle_{i}}, y{\scriptstyle_{j}} \right\}  \label{eq:covariance_8e}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
%%    \left\{  \right\}
  \end{align}
\end{subequations}
which shows that $V{\scriptstyle_{ij}}$ $=$ $V{\scriptstyle_{ji}}$ $\Rightarrow$ the \emph{pressure tensor} is symmetric.
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  Correlation
%%----------------------------------------------------------------------------------------
\subsubsection{Correlation}  \label{subsubsec:correlation}
\noindent  The \emph{correlation} is given by:
\begin{equation}
  \label{eq:correlation_0a}
  cor\left( x, y \right) \equiv \frac{ cov\left( x, y \right) }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} }
\end{equation}
where $\sigma{\scriptstyle_{x}}$ is defined as $\sqrt{ var\left( x \right) }$.  We can use a rule for the \emph{variance}, given by:
\begin{subequations}
  \begin{align}
    var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \pm \frac{ y }{ \sigma{\scriptstyle_{y}} } \right\} & = var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \right\} + var\left\{ \frac{ \pm y }{ \sigma{\scriptstyle_{y}} } \right\} + 2 var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} }, \frac{ \pm y }{ \sigma{\scriptstyle_{y}} } \right\}  \label{eq:correlation_1a}  \\
    & = \frac{ 1 }{ \sigma{\scriptstyle_{x}}^{2} } var\left( x \right) + \frac{ 1 }{ \sigma{\scriptstyle_{y}}^{2} } var\left( y \right) \pm \frac{ 2 }{ \sigma{\scriptstyle_{x}} \sigma{\scriptstyle_{y}} } cov\left( x, y \right)  \label{eq:correlation_1b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}
combined with the knowledge that:
\begin{equation}
  \label{eq:correlation_2a}
  var\left\{ \frac{ x }{ \sigma{\scriptstyle_{x}} } \pm \frac{ y }{ \sigma{\scriptstyle_{y}} } \right\} \geq 0
\end{equation}
to prove the following:
\begin{equation}
  \label{eq:correlation_3a}
  -1 \leq cor\left( x, y \right) \leq 1  \text{  .}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Linear Algebra
%%----------------------------------------------------------------------------------------
\subsection{Linear Algebra} \label{subsec:LA}

Let $\langle \mathbf{x} \rangle$ be defined as the \emph{sample mean}, which mathematically means:

\begin{equation}
  \label{eq:linalg_0}
  \langle \mathbf{x} \rangle \equiv \frac{1}{N} \sum_{i=1}^{N} \textbf{x}_{i}
\end{equation}

where N is the number of samples in your data set.  Let us define the following:

\begin{equation}
  \label{eq:linalg_1}
  \hat{\textbf{x}}_{k} \equiv \textbf{x}_{k} - \langle \mathbf{x} \rangle
\end{equation}

which leads us to a matrix whose columns have a zero sample mean, defined as:

\begin{equation}
  \label{eq:linalg_2}
  \textbf{B} = \Bigl[\hat{\textbf{x}}_{1} \hat{\textbf{x}}_{2} \dotsc \hat{\textbf{x}}_{n}\Bigr] \text{ .}
\end{equation}

The \emph{sample covariance matrix} is thus defined by:

\begin{equation}
  \label{eq:linalg_3}
  \textbf{S} \equiv \frac{\textbf{B} \textbf{B}^{T}}{N-1} \text{ .}
\end{equation}

If we now define a vector, \textbf{X}, which varies over the set of observed vectors and denote the coordinates by \emph{x}$_{j}$, then the diagonal entry, \emph{s}$_{jj}$ in \textbf{S} is called the variance of \emph{x}$_{j}$.  Thus, \emph{s}$_{jj}$ measures the \emph{spread} of the values of \emph{x}$_{j}$.  The \emph{total variance} is defined as:

\begin{equation}
  \label{eq:linalg_4}
  \Bigl\{Total Variance\Bigr\} \equiv Tr\Bigl[\textbf{S}\Bigr]
\end{equation}

The \emph{covariance}, \emph{s}$_{ij}$ for i $\neq$ j, is equal to zero when \emph{x}$_{i}$ and \emph{x}$_{j}$ are uncorrelated.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Principle Component Analysis
%%----------------------------------------------------------------------------------------
\subsection{Principle Component Analysis} \label{subsec:PCA}

The main goal here is to find an orthogonal \emph{n}$\times$\emph{n} matrix, \textbf{P} = [\textbf{u}$_{1}$ $\dotsc$ \textbf{u}$_{n}$], such that \textbf{X} = \textbf{P} \textbf{Y}, with the property that the components of \textbf{Y}, \emph{y}$_{j}$, are uncorrelated and arranged in order of decreasing variance.  This implies that each individual observed vector, \textbf{X}$_{k}$, goes to a new \emph{name}, \textbf{Y}$_{k}$.  This results in the following relationship:

\begin{equation}
  \label{eq:linalg_5}
  \textbf{Y}_{k} = \textbf{P}^{-1} \textbf{X}_{k} = \textbf{P}^{T} \textbf{X}_{k} \text{  for k = 1,$\dotsc$,N .}
\end{equation}

A direct result of this \emph{re-naming} is that the covariance matrix for \textbf{Y}$_{k}$ is:

\begin{equation}
  \label{eq:linalg_6}
  \textbf{S}_{2} = \textbf{P}^{T} \textbf{S} \textbf{P}
\end{equation}

which forces \textbf{S}$_{2}$ to be diagonal (since \textbf{P} is an orthogonal matrix).  Now if we allow \textbf{D} to be a diagonal matrix with eigenvalues of \textbf{S}, $\lambda_{k}$ on the diagonal arranged so that $\lambda_{1}$ $\geq$ $\lambda_{2}$ $\geq$ $\dotsc$ $\lambda_{n}$ $\geq$ 0, then if \textbf{P} is an orthogonal matrix of corresponding eigenvectors we have:

\begin{subequations}
  \begin{align}
    \textbf{S} & = \textbf{P} \textbf{D} \textbf{P}^{T}  \label{eq:linalg_7} \\
    \textbf{D} & = \textbf{P}^{T} \textbf{S} \textbf{P} \label{eq:linalg_8} \text{ .}
  \end{align}
\end{subequations}

The eigenvectors, \textbf{u}$_{i}$, of the covariance matrix, \textbf{S}, are called the \emph{principal components} of the data.  The \emph{first principal component}, \textbf{u}$_{1}$, is the eigenvector corresponding to the largest eigenvalue of \textbf{S} and the \emph{second principal component}, \textbf{u}$_{2}$, corresponds to the second largest eigenvalue and so on.  If we allow \emph{c}$_{i}$ to be entries of \textbf{u}$_{1}$, then \textbf{Y} $=$ \textbf{P}$^{T}$ \textbf{X} gives:

\begin{equation}
  \label{eq:linalg_9}
  y_{1} = \textbf{u}_{1}^{T} \textbf{X} = c_{1} x_{1} + c_{2} x_{2} + \dotsc + c_{n} x_{n}
\end{equation}

which means y$_{1}$ is a linear combination of the original variables x$_{1}$ $\dotsc$ x$_{n}$.  One thing to note, the orthogonal change of variables, \textbf{X} $=$ \textbf{P} \textbf{Y}, does NOT change the total variance of the data, or in other words:

\begin{subequations}
  \begin{align}
    \Bigl\{Total \: Variance \: of \: x_{i}\Bigr\} & = \Bigl\{Total \: Variance \: of \: y_{i}\Bigr\}\label{eq:linalg_10} \\
    \Bigl\{Total \: Variance \: of \: y_{i}\Bigr\} & = Tr\Bigl[\textbf{D}\Bigr] \label{eq:linalg_11} \\
    & = \lambda_{1} + \dotsc + \lambda_{n} \label{eq:linalg_12}
  \end{align}
\end{subequations}

$\Rightarrow$
where the variance of \emph{y}$_{i}$ is $\lambda_{i}$, and $\lambda_{i}$/Tr$ \bigl[$\textbf{S}$\bigr]$ measures the fraction of the total variace that is \emph{explained} or \emph{captured} by \emph{y}$_{i}$.  Thus if \textbf{u} satisfies, \emph{y} $=$ \textbf{u}$^{T}$ \textbf{X}, then the variance of the values of \emph{y} as \textbf{X} varies over the original data, \textbf{X}$_{i}$, is \textbf{u}$^{T}$\textbf{S}\textbf{u}.

\begin{enumerate}
  \item The maximum value of \textbf{u}$^{T}$\textbf{S}\textbf{u} occurs for $\lambda_{1}$ and \textbf{u}$_{1}$
  \item \emph{y}$_{2}$ has a maximum variance among all variables \emph{y} $=$ \textbf{u}$^{T}$\textbf{X} that are uncorrelated with \emph{y}$_{1}$
  \item Likewise, \emph{y}$_{3}$ has a maximum variance among all variables that are uncorrelated with BOTH \emph{y}$_{1}$ and \emph{y}$_{2}$.
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Minimum Variance Analysis
%%----------------------------------------------------------------------------------------
\subsection{Minimum Variance Analysis} \label{subsec:MVA}

Minimum variance analysis, or MVA, is the utilization of a property of plane polarized linear electromagnetic waves which allows one to assume that fluctuations in the electric ($\delta \mathbf{E}$) and magnetic ($\delta \mathbf{B}$) fields are are in a plane orthogonal to the direction of propagation ($\hat{\mathbf{k}}$) \citet{khrabrov98}.  If the wave is truly a plane polarized wave, then $\hat{\mathbf{k}}$ $\cdot$ $\delta \mathbf{B}$ $=$ 0, which is a linear approximation of the Maxwell equation, $\nabla$ $\cdot$ $\mathbf{B}$ $=$ 0.  The analysis is performed by minimizing the variance matrix of the magnetic field given by:

\begin{equation}
      \label{eq:appmv_1}
      \textbf{S}{\scriptstyle_{pq}} = \Bigl< \Bigl( B{\scriptstyle_{p}} - \bigl<B{\scriptstyle_{p}}\bigr> \Bigr) \Bigl( B{\scriptstyle_{q}} - \bigl<B{\scriptstyle_{q}}\bigr> \Bigr) \Bigr>
\end{equation}

where $\langle B{\scriptstyle_{p}} \rangle$ is the average of the $p{\scriptstyle^{th}}$ component of the magnetic field.  We assume $\mathbf{S}{\scriptstyle_{pq}}$ to be a non-degenerate matrix with three distinct eigenvalues, $\lambda{\scriptstyle_{3}}$ $<$ $\lambda{\scriptstyle_{2}}$ $<$ $\lambda{\scriptstyle_{1}}$, and three corresponding eigenvectors, $\mathbf{e}{\scriptstyle_{3}}$, $\mathbf{e}{\scriptstyle_{2}}$, $\mathbf{e}{\scriptstyle_{1}}$.  Thus the minimum variance eigenvalue and eigenvector are $\lambda{\scriptstyle_{3}}$ and $\mathbf{e}{\scriptstyle_{3}}$.  The propagation direction is said to be along $\hat{\textbf{e}} {\scriptstyle_{3}}$ if one assumes small isotropic noise and the condition $\lambda{\scriptstyle_{2}}$/$\lambda{\scriptstyle_{3}}$ $\geq$ 10 is satisfied.  Then the uncertainty in this direction is given by \citet{kawano95a}:

\begin{equation}
  \label{eq:appmv_2}
  \delta \hat{\textbf{k}} = \pm \Bigl(\hat{\textbf{e}}{\scriptstyle_{1}} \sqrt{\frac{\delta \lambda{\scriptstyle_{3}}}{\lambda{\scriptstyle_{1}} - \lambda{\scriptstyle_{3}}}} + \hat{\textbf{e}}{\scriptstyle_{2}} \sqrt{\frac{\delta \lambda{\scriptstyle_{3}}}{\lambda{\scriptstyle_{2}} - \lambda{\scriptstyle_{3}}}} \Bigr)
\end{equation}

where \emph{K} is the number vectors used and $\delta \lambda{\scriptstyle_{3}}$, the uncertainty in the $\lambda{\scriptstyle_{3}}$ eigenvalue, is given by:

\begin{equation}
  \label{eq:appmv_3}
  \delta \lambda{\scriptstyle_{3}} = \pm \lambda{\scriptstyle_{3}} \sqrt{ \frac{2}{(K - 1)} }  \text{  .}
\end{equation}

In general, the uncertainty of $\delta \lambda{\scriptstyle_{i}}$ is given by:

\begin{equation}
  \label{eq:appmv_4}
  \delta \lambda{\scriptstyle_{i}} = \pm \sqrt{\frac{2 \lambda{\scriptstyle_{3}} (\lambda{\scriptstyle_{i}} - \lambda{\scriptstyle_{3}}) }{(K - 1)}}  \text{  .}
\end{equation}

Another useful quantity to know is the angle between the local ambient magnetic field and the propagation direction, $\theta{\scriptstyle_{kB}}$.  This can be calculated in the typical manner, $\theta{\scriptstyle_{kB}} \equiv \cos^{-1}{\left( \hat{\textbf{k}} \cdot \hat{\textbf{b}} \right)}$, with associated uncertainties of:

\begin{equation}
  \label{eq:appmv_5}
  \delta \theta{\scriptstyle_{kB}} = \pm \sqrt{ \frac{ \lambda{\scriptstyle_{3}} \lambda{\scriptstyle_{2}} }{(K - 1) (\lambda{\scriptstyle_{2}} - \lambda{\scriptstyle_{3}})^{2} } }  \text{  .}
\end{equation}  

\citet{khrabrov98} found analytical estimates to the error analysis of statistical noise in a vector field (i.e., B-field) with the application of minimum/maximum variance analysis.  They consider two special cases of signal-to-noise ratios: 1) large and 2) small, for arbitrary noise distributions.

\begin{enumerate}
  \item \textbf{The Ideal Case} $\equiv$ small errors and isotropic Gaussian noise
  \item For the ideal case, one can determine \emph{uncertainty cones} with elliptic cross sections for all three eigenvectors: $\mathbf{x}_{1}$,$\mathbf{x}_{2}$, $\mathbf{x}_{3}$, and \emph{uncertainty intervals} for all three eigenvalues: $\lambda_{1}$, $\lambda_{2}$, $\lambda_{3}$
  \item Note: $\lambda_{3}$ $<$ $\lambda_{2}$ $<$ $\lambda_{1}$ by definition
  \item \textbf{Anisotropic Noise, No Signal:} 1) $\lambda_{3}$ $\approx$ $\lambda_{2}$ $\equiv$ \textbf{Linearly Polarized IF} $\lambda_{3}$ $\ll$ $\lambda_{1}$ \textbf{AND} the non-fluctuating part of the signal is negligible (i.e., only measuring noise due to wave packets which are broadband or spatially unresolved), 2) $\lambda_{1}$ $\approx$ $\lambda_{2}$ $\equiv$ \textbf{Circularly Polarized} IF $\lambda_{3}$ $\ll$ $\lambda_{2}$
  \item \textbf{Small Anisotropic Noise:}  If amplitude of noise $\ll$ amplitude of signal, then $\lambda_{3}$ can be said to be entirely due to noise
  \item \textbf{Isotropic Gaussian Noise:}  Equations \ref{eq:khrabrov_stdev2} and \ref{eq:khrabrov_vecuncert1} implicitly assume isotropic Gaussian noise
  \item In Equation \ref{eq:khrabrov_dphi1}, $\Delta \phi_{i,j}$ $\equiv$ the angular standard deviation (radians) of the $i^{th}$ vector's ($\vec{x}_{i}$) direction towards/away from the $j^{th}$ vector's ($\vec{x}_{j}$) direction
  \item The \emph{Variance} of any quantity is defined as in Equation \ref{eq:khrabrov_variance1}.  The use of Minimum Variance Analysis (MVA) on magnetic fields derives from the Maxwell Equation $\nabla \cdot \mathbf{B}$ = 0.  From this equation, one can convert the divergence into a dot product between a vector, \textbf{n}, and the B-field.  If this vector \textbf{n} exists, the field does not vary along it.  Thus we say, $B_{n}$ = $\mathbf{n} \cdot \mathbf{B}$ = constant!  So we vary the B-field in each of it's component directions and the variance is described by Equation \ref{eq:khrabrov_variance2}, where K $\equiv$ number of measurements/vectors.
  \item \emph{Rule of Thumb:}  for K $<$ 50, REQUIRE $\lambda_{2}$/$\lambda_{3}$ $\ge$ 10, UNLESS one knows \emph{a priori} that the noise is truly random, which then implies that 1/K is a relevant, small parameter
  \item The Variance Matrix:  see Equation \ref{eq:khrabrov_variance3}
  \item \textbf{Ensemble Average} $\equiv$ $\bigl<\bigl<$  $\bigr>\bigr>$ $\equiv$ average over the ensemble of all realizations of data
  \item \textbf{Average of Data} $\equiv$ $\bigl<$  $\bigr>$ $\equiv$ average of data in a given realization
\end{enumerate}

If we have a set of functions given by:

\begin{equation}
  \label{eq:khrabrov_fset1}
  \Bigl\{f_{j}\Bigr\} = \Bigl\{f\left(x_{j}\right)\Bigr\}
\end{equation}

and we let $x_{j}$ go to $\langle x \rangle + e_{j}$, then we have:

\begin{subequations}
 \begin{align}
  \bigl<f\bigr> & = \frac{1}{N} \sum_{j} f\left(x_{j}\right) \label{eq:khrabrov_fset2} \\
                & = \frac{1}{N} \sum_{j} f\left(\left<x\right> + e_{j}\right) \label{eq:khrabrov_fset3} \\
                & = f\left(\left<x\right>\right) + \frac{1}{N} f'\left(\left<x\right>\right) \sum_{j} e_{j} + \frac{1}{2N} f''\left(\left<x\right>\right) \sum_{j} \left(e_{j}\right)^{2} + \dotsc \label{eq:khrabrov_fset4} \\
                & = f\left(\left<x\right>\right) + \frac{\sigma^{2}}{2} f''\left(\left<x\right>\right) \label{eq:khrabrov_fset5}
 \end{align}
\end{subequations}

where $\sigma^{2}$ is defined by:

\begin{equation}
  \label{eq:khrabrov_variance1}
  \sigma^{2} \equiv \frac{1}{N} \sum_{i=1}^{N} \left(\textbf{x}_{i} - \langle \textbf{x}\rangle \right)^{2}
\end{equation}

thus, it can be shown that the fluctuations of two eigenvalues, treated as distinct and uncorrelated, have an the standard deviation of their difference, ($\lambda_{i}$ - $\lambda_{j}$), as: 

\begin{equation}
  \label{eq:khrabrov_stdev1}
  \sigma_{ij} = \sqrt{\left<\left< \bigl(\Delta \lambda_{i} \bigr)^{2} \right>\right> + \left<\left< \bigl(\Delta \lambda_{j} \bigr)^{2} \right>\right>}
\end{equation}

where, we have

\begin{equation}
\label{eq:khrabrov_stdev2}
  \left<\left< \bigl(\Delta \lambda_{i} \bigr)^{2} \right>\right> = \frac{2 \lambda_{3} \left(2 \lambda_{i} - \lambda_{3}\right)}{\left(K - 1\right)} \text{ .}
\end{equation}

The uncertainty in the vector, $\mathbf{x}_{1}$ (The maximum variance direction.), is then given by:

\begin{equation}
\label{eq:khrabrov_vecuncert1}
  \Delta \phi_{1j} = \sqrt{\frac{\lambda_{j} \lambda_{1}}{\left(K - 1\right) \left(\lambda_{1} - \lambda_{j}\right)} }
\end{equation}

if $\lambda_{2}$ $\ll$ $\lambda_{1}$ \textbf{AND} $\lambda_{3}$ $\ll$ $\lambda_{1}$.

\begin{equation}
\label{eq:khrabrov_variance2}
  Var(\textbf{B} \cdot \textbf{x}) \equiv \frac{1}{K} \sum_{k=1}^{K} \Bigl[ \left( \textbf{B}^{(k)} - \langle \textbf{B}\rangle \right) \cdot \textbf{x} \Bigr]^{2} \equiv \Bigl< \Bigl[ \left( \textbf{B}^{(k)} - \langle \textbf{B}\rangle \right) \cdot \textbf{x} \Bigr]^{2} \Bigr>
\end{equation}

\begin{equation}
\label{eq:khrabrov_variance3}
  M_{ij} = \Bigl< \Bigl( B_{i}^{(k)} - \langle B_{i}^{(k)} \rangle \Bigr) \Bigl( B_{j}^{(k)} - \langle B_{j}^{(k)} \rangle   \Bigr) \Bigr> \equiv \Bigl< \delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr>
\end{equation}

now replace $\mathbf{B}^{(k)}$ by $\mathbf{B}^{*(k)} + \delta \mathbf{b}^{(k)}$, where $\mathbf{B}^{*(k)}$ $\equiv$ signal and $\delta \mathbf{b}^{(k)}$ $\equiv$ noise.  One should note that $\mathbf{B}^{*(k)}$, k = 1, 2, $\dotsc$, K are the same in all realizations, while the K-offset noise components, $\delta \mathbf{b}^{(k)}$, contain $\bigl< \mathbf{b} \bigr>$, therefore are functions of all K noise vectors, $\mathbf{b}^{(k)}$, k = 1, 2, $\dotsc$,K, in the realization.  By definition, the latter has the property:

\begin{equation}
\label{eq:khrabrov_variance4}
  \Bigl<\Bigl< \textbf{b}^{(k)} \Bigr>\Bigr> \equiv 0  \Rightarrow \Bigl<\Bigl< \delta\textbf{b}^{(k)} \Bigr>\Bigr> \equiv 0
\end{equation}

which allows us to define the following:

\begin{equation}
\label{eq:khrabrov_variance5}
   \delta \textbf{B}^{(k)} = \delta \textbf{B}^{*(k)} +  \delta \textbf{b}^{(k)}
\end{equation}

where

\begin{subequations}
 \begin{align}
   \delta \textbf{B}^{*(k)} & \equiv \textbf{B}^{*(k)} - \bigl< \textbf{B}^{*} \bigr> \label{eq:khrabrov_definition1} \\
   \delta \textbf{b}^{(k)} & \equiv \textbf{b}^{(k)} - \bigl< \textbf{b} \bigr> \label{eq:khrabrov_definition2}
 \end{align}
\end{subequations}

 so that Equation \ref{eq:khrabrov_variance3} goes to:

\begin{subequations}
 \begin{align}
   \Bigl<\delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr> & = \Bigl<\bigl(\delta B_{i}^{*(k)} + \delta b_{i}^{(k)} \bigr) \bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> \label{eq:khrabrov_variance6} \\
   & = \Bigl<\delta B_{i}^{*(k)}\bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> + \Bigl<\delta b_{i}^{(k)}\bigl(\delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \bigr) \Bigr> \label{eq:khrabrov_variance7} \\
   & = \Bigl<\delta B_{i}^{*(k)} \delta B_{j}^{*(k)} \Bigr> + \Bigl<\delta B_{i}^{*(k)} \delta b_{j}^{(k)} \Bigr> + \Bigl<\delta b_{i}^{(k)} \delta B_{j}^{*(k)} \Bigr> + \Bigl<\delta b_{i}^{(k)} \delta b_{j}^{(k)}\Bigr> = M_{ij} \label{eq:khrabrov_variance8} \text{ .}
 \end{align}
\end{subequations}

For the next step we have to realize that the following rule is valid:

\begin{equation}
\label{eq:khrabrov_definition3} 
   \Biggl< \Bigl[\bigl<\bigl< A \bigr>\bigr>\Bigr] \Biggr> = \Biggl<\Biggl< \Bigl[\bigl< A \bigr>\Bigr] \Biggr>\Biggr> \text{ .}
\end{equation}

If we take the \emph{ensemble average} of our variance matrix, we get:

\begin{subequations}
  \begin{align}
  \Biggl<\Biggl< \Bigl(\Delta M_{ij} \Bigr)^{2}\Biggr>\Biggr> & = \Biggl<\Biggl< \Bigl[M_{ij} + \bigl<\bigl<M_{ij}\bigr>\bigr> \Bigr]^{2}\Biggr>\Biggr> \label{eq:khrabrov_variance9} \\
  \begin{split}
  & = \Biggl<\Biggl< \Biggl\{ \frac{1}{K}\sum_{k} \Bigl( \delta B_{i}^{*(k)} + \delta b_{i}^{(k)} \Bigr) \Bigl( \delta B_{j}^{*(k)} + \delta b_{j}^{(k)} \Bigr) - \\
  & \qquad \frac{1}{K}\sum_{m} \Bigl( \delta B_{i}^{*(m)}\delta B_{j}^{*(m)} + \bigl<\bigl<\delta b_{i} \delta b_{j} \bigr>\bigr> \Bigr) \Biggr\}^{2} \Biggr>\Biggr> \label{eq:khrabrov_variance10}
 \end{split}
  \end{align}
\end{subequations}

where the second term on the R.H.S. of Equation \ref{eq:khrabrov_variance9} is:

\begin{subequations}
  \begin{align}
    \Biggl<\Biggl< M_{ij}\Biggr>\Biggr> & = \Biggl<\Biggl< \Bigl[ \Bigl<\delta B_{i}^{(k)} \delta B_{j}^{(k)} \Bigr> \Bigr]\Biggr>\Biggr> \label{eq:khrabrov_variance11} \\
    & = \Biggl<\Bigl[\Bigl<\Bigl< \delta B_{i}^{(k)} \delta B_{j}^{(k)}\Bigr>\Bigr>\Bigr]\Biggr> \label{eq:khrabrov_variance12} \\
    \begin{split}
    & = \Biggl<\Bigl[\Bigl<\Bigl<\delta B_{i}^{*(k)}\delta B_{j}^{*(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \Biggl<\Bigl[\Bigl<\Bigl< \delta B_{i}^{*(k)} \delta b_{j}^{(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \\
    & \qquad \Biggl<\Bigl[\Bigl<\Bigl< \delta b_{i}^{(k)} \delta B_{j}^{*(k)} \Bigr>\Bigr>\Bigr]\Biggr> + \Biggl<\Bigl[\Bigl<\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Bigr>\Bigr]\Biggr>  \label{eq:khrabrov_variance13}
   \end{split}
  \end{align}
\end{subequations}

which is highly simplified by realizing that the middle two terms can be canceled when the ensemble average is taken due to the properties assumed in Equation \ref{eq:khrabrov_variance4}.  The first term on the R.H.S. is just defined as:

\begin{equation}
  \label{eq:khrabrov_variance14}
  M_{ij}^{*} \equiv \Biggl<\Biggl\{\Bigl<\Bigl<\delta B_{i}^{*(k)}\delta B_{j}^{*(k)} \Bigr>\Bigr>\Biggr\}\Biggr>
\end{equation}

which is the variance matrix of the nonfluctuating part of the field.  The final result is written as:

\begin{equation}
  \label{eq:khrabrov_variance15}
   \Biggl<\Biggl< M_{ij}\Biggr>\Biggr> = M_{ij}^{*} + \Biggl<\Biggl<\Biggl\{\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Biggr\}\Biggr>\Biggr> \text{ .}
\end{equation}

The second term on the R.H.S. of Equation \ref{eq:khrabrov_variance15} can be dealt with in the following manner:

\begin{subequations}
  \begin{align}
    \Biggl<\Biggl<\Biggl\{\Bigl< \delta b_{i}^{(k)} \delta b_{j}^{(k)} \Bigr>\Biggr\}\Biggr>\Biggr> & = \Biggl<\Biggl<\Biggl\{\Bigl< \Bigl(b_{i}^{(k)} - \bigl<b_{i}^{(k)}\bigr> \Bigr) \Bigl(b_{j}^{(k)} - \bigl<b_{j}^{(k)}\bigr> \Bigr) \Bigr>\Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance16} \\
    & = \Biggl<\Biggl<\Biggl\{\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr> - \Bigl<b_{i}^{(k)} \bigl<b_{j}^{(k)}\bigr>\Bigr> - \Bigl<\bigl<b_{i}^{(k)}\bigr> b_{j}^{(k)}\Bigr> + \Bigl<\bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr>\Bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance17} \\
    & = \Biggl<\Biggl<\Biggl\{\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr> - \bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance18} \\
    & = \Biggl<\Biggl\{\Bigl<\Bigl< b_{i}^{(k)}b_{j}^{(k)}\Bigr>\Bigr>\Biggr\}\Biggr> - \Biggl<\Biggl<\Biggl\{ \bigl<b_{i}^{(k)}\bigr>\bigl<b_{j}^{(k)}\bigr> \Biggr\}\Biggr>\Biggr> \label{eq:khrabrov_variance19} 
  \end{align}
\end{subequations}

The uncertainty in the direction between any two eigenvectors is given by:

\begin{equation}
\label{eq:khrabrov_dphi1}
  \Delta \phi_{i,j} = \pm \sqrt{ \left(\frac{\lambda_{3} (\lambda_{i} + \lambda_{j} - \lambda_{3})}{(K - 1)(\lambda_{i} - \lambda_{j})^{2}} \right)}
\end{equation}

with an uncertainty in eigenvalues given by:

\begin{equation}
\label{eq:khrabrov_dlam1}
  \Delta \lambda_{i} = \pm \sqrt{ \left(\frac{2 \lambda_{3} (2 \lambda_{i} - \lambda_{3})}{(K - 1)} \right)}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Trigonometric Identities
%%----------------------------------------------------------------------------------------
\subsection{Trigonometric Identities}  \label{subsec:trigidentities}
\noindent  The following are trigonometric identities for complex functions of $x$:
\begin{subequations}
  \begin{align}
    sinh \thickspace \left( \pm i x \right) & = \pm i \thickspace sin \thickspace x  \label{eq:trigidentities_0a}  \\
    cosh \thickspace \left( \pm i x \right) & = cos \thickspace x  \label{eq:trigidentities_0b}  \\
    tanh \thickspace \left( \pm i x \right) & = \pm i \thickspace tan \thickspace x  \label{eq:trigidentities_0c}  \\
    sinh \thickspace \left( y \pm i x \right) & = \pm i \thickspace cosh \left( y \right) \thickspace sin \left( x \right) + cos \left( x \right) \thickspace sinh \left( y \right)  \label{eq:trigidentities_0d}  \\
    cosh \thickspace \left( y \pm i x \right) & = cos \left( x \right) \thickspace cosh \left( y \right) \pm i \thickspace sin \left( x \right) \thickspace sinh \left( y \right)  \label{eq:trigidentities_0e}  \\
    tanh \thickspace \left( y \pm i x \right) & = \frac{ \pm i \thickspace cosh \left( y \right) \thickspace sin \left( x \right) + cos \left( x \right) \thickspace sinh \left( y \right) }{ cos \left( x \right) \thickspace cosh \left( y \right) \pm i \thickspace sin \left( x \right) \thickspace sinh \left( y \right) }  \label{eq:trigidentities_0f}
  \end{align}
\end{subequations}
where we can note that letting $i x$ $\rightarrow$ $z$ gives:
\begin{subequations}
  \begin{align}
    sinh \thickspace \left( y \pm z \right) & = cosh \left( z \right) \thickspace sinh \left( y \right) \pm cosh \left( y \right) \thickspace sinh \left( z \right)  \label{eq:trigidentities_1a}  \\
    cosh \thickspace \left( y \pm z \right) & = cosh \left( y \right) \thickspace cosh \left( z \right) \pm sinh \left( y \right) \thickspace sinh \left( z \right)  \label{eq:trigidentities_1b}  \\
    tanh \thickspace \left( y \pm z \right) & = \frac{ cosh \left( z \right) \thickspace sinh \left( y \right) \pm cosh \left( y \right) \thickspace sinh \left( z \right) }{ cosh \left( y \right) \thickspace cosh \left( z \right) \pm sinh \left( y \right) \thickspace sinh \left( z \right) }  \text{  .}  \label{eq:trigidentities_1c}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Taylor Series
%%----------------------------------------------------------------------------------------
\subsection{Taylor Series}  \label{subsec:taylorseries}
\noindent  The following are Taylor series expansions for general functions of $x$:
\begin{subequations}
  \begin{align}
    \sqrt{\frac{1}{1 + x^{2}}} & \approx 1 - \frac{x^{2}}{2} + \frac{3 x^{4}}{8} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0a}  \\
    \sqrt{\frac{1}{1 + \left(x/a\right)^{2}}} & \approx 1 - \frac{x^{2}}{2 a^{2}} + \frac{3 x^{4}}{8 a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0b}  \\
    \sqrt{1 + x^{2}} & \approx 1 + \frac{x^{2}}{2} - \frac{x^{4}}{8} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0c}  \\
    \sqrt{1 + \left(x/a\right)^{2}} & \approx 1 - \frac{x^{2}}{2 a^{2}} + \frac{x^{4}}{8 a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0d}  \\
    \sqrt{\frac{x^{2}}{1 + x^{2}}} & \approx x - \frac{x^{3}}{2} + \frac{3 x^{5}}{8} + \mathcal{O}\left( x^{7} \right)  \label{eq:series_0e}  \\
    \sqrt{\frac{1 + x^{2}}{x^{2}}} & \approx \frac{1}{x} + \frac{x}{2} - \frac{x^{3}}{8} + \mathcal{O}\left( x^{5} \right)  \label{eq:series_0f}  \\
    \frac{1}{1 + x^{2}} & \approx 1 - x^{2} + x^{4} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0g}  \\
    \frac{1}{1 + \left(x/a\right)^{2} } & \approx 1 - \frac{x^{2}}{a^{2}} + \frac{x^{4}}{a^{4}} + \mathcal{O}\left( x^{6} \right)  \label{eq:series_0h}
  \end{align}
\end{subequations}
\noindent  The following are Taylor series expansions for exponential functions of $x$:
\begin{subequations}
  \begin{align}
    e^{ ^{\displaystyle \pm x } } & \approx 1 \pm x + \frac{ x^{2} }{ 2! } \pm \frac{ x^{3} }{ 3! } + \frac{ x^{4} }{ 4! } \pm \frac{ x^{5} }{ 5! } + \mathcal{O}\left( x^{6} \right)  \label{eq:series_1a}  \\
    e^{ ^{\displaystyle \pm i x } } & \approx 1 \pm \left( i x \right) - \frac{ x^{2} }{ 2! } \mp \frac{ i x^{3} }{ 3! } + \frac{ x^{4} }{ 4! } \pm \frac{ i x^{5} }{ 5! } + \mathcal{O}\left( x^{6} \right)  \label{eq:series_1b}  \\
    e^{ ^{\displaystyle \pm x^{2} } } & \approx 1 \pm x^{2} + \frac{ x^{4} }{ 2! } \pm \frac{ x^{6} }{ 3! } + \frac{ x^{8} }{ 4! } + \mathcal{O}\left( x^{10} \right)  \label{eq:series_1c}
  \end{align}
\end{subequations}
where we know that:
\begin{equation}
  \label{eq:series_1d}
  e^{ ^{\displaystyle \pm i x } } = \cos{x} \pm i \sin{x}  \text{  .}
\end{equation}
\noindent  The following are Taylor series expansions for trigonometric functions of $x$:
\begin{subequations}
  \begin{align}
    \sin{x} & \approx x - \frac{ x^{3} }{ 3! } + \frac{ x^{5} }{ 5! } - \frac{ x^{7} }{ 7! } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_2a}  \\
    \cos{x} & \approx 1 - \frac{ x^{2} }{ 2! } + \frac{ x^{4} }{ 4! } - \frac{ x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_2b}  \\
    \tan{x} & \approx x + \frac{ x^{3} }{ 3 } + \frac{ 2 x^{5} }{ 15 } + \frac{ 17 x^{7} }{ 315 } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_2c}  \\
    \sec{x} & \approx 1 + \frac{ x^{2} }{ 2! } + \frac{ 5 x^{4} }{ 4! } + \frac{ 61 x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_2d}  \\
    \csc{x} & \approx \frac{ 1 }{ x } + \frac{ x }{ 6 } + \frac{ 7 x^{3} }{ 360 } + \frac{ 31 x^{5} }{ 15120 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_2e}  \\
    \cot{x} & \approx \frac{ 1 }{ x } - \frac{ x }{ 3 } - \frac{ x^{3} }{ 45 } - \frac{ 2 x^{5} }{ 945 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_2f}
  \end{align}
\end{subequations}
\noindent  The following are Taylor series expansions for hyperbolic functions of $x$:
\begin{subequations}
  \begin{align}
    sinh \thickspace x & \approx x + \frac{ x^{3} }{ 3! } + \frac{ x^{5} }{ 5! } + \frac{ x^{7} }{ 7! } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_3a}  \\
    cosh \thickspace x & \approx 1 + \frac{ x^{2} }{ 2! } + \frac{ x^{4} }{ 4! } + \frac{ x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_3b}  \\
    tanh \thickspace x & \approx x - \frac{ x^{3} }{ 3 } + \frac{ 2 x^{5} }{ 15 } - \frac{ 17 x^{7} }{ 315 } + \mathcal{O}\left( x^{9} \right)  \label{eq:series_3c}  \\
    sech \thickspace x & \approx 1 - \frac{ x^{2} }{ 2! } + \frac{ 5 x^{4} }{ 4! } - \frac{ 61 x^{6} }{ 6! } + \mathcal{O}\left( x^{8} \right)  \label{eq:series_3d}  \\
    csch \thickspace x & \approx \frac{ 1 }{ x } - \frac{ x }{ 6 } + \frac{ 7 x^{3} }{ 360 } - \frac{ 31 x^{5} }{ 15120 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_3e}  \\
    coth \thickspace x & \approx \frac{ 1 }{ x } + \frac{ x }{ 3 } - \frac{ x^{3} }{ 45 } + \frac{ 2 x^{5} }{ 945 } + \mathcal{O}\left( x^{7} \right)  \label{eq:series_3f}
  \end{align}
\end{subequations}


\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Variational Principle
%%----------------------------------------------------------------------------------------
\subsection{Variational Principle}  \label{subsec:VariationalPrinciple}
\indent  Recall that for an arbitrary function, $\mathcal{F}$ $=$ $\mathcal{F} \left( t, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, ..., x{\scriptstyle_{n - 1}}, x{\scriptstyle_{n}} \right)$, the \emph{exact derivative} or \emph{total derivative} is given by:
\begin{equation}
  \label{eq:varprincp_0a}
  \frac{ d \mathcal{F} }{ dt } = \frac{ \partial \mathcal{F} }{ \partial t } + \sum_{i = 1}^{n} \frac{ \partial \mathcal{F} }{ \partial x{\scriptstyle_{i}} } \thickspace \frac{ d x{\scriptstyle_{i}} }{ dt }  \text{   .}
\end{equation}
If we have $\mathcal{W}$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \textbf{x}, t \right)$, then the variation is given by:
\begin{equation}
  \label{eq:varprincp_1a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \omega, \textbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \textbf{x} } \cdot \delta \textbf{x} + \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \omega + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa}
\end{equation}
\indent  Now let us consider the dispersion relation, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \textbf{x}, t \right)$, then variation can be shown to be:
\begin{equation}
  \label{eq:varprincp_2a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \textbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \textbf{x} } \cdot \delta \textbf{x} + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Bessel Functions
%%----------------------------------------------------------------------------------------
\subsection{Bessel Functions}  \label{subsec:BesselFunctions}
\indent  If we define $\mathit{J}{\scriptstyle_{n}}$ and $\mathit{Y}{\scriptstyle_{n}}$ as Bessel functions of the first and second kind, respectively, and we let $\mathit{I}{\scriptstyle_{n}}$ and $\mathit{K}{\scriptstyle_{n}}$ be the modified Bessel functions of the first and second kind, respectively, then:
\begin{subequations}
  \begin{align}
    \mathit{J}{\scriptstyle_{n}}(x) & = \sum_{j=0}^{\infty} \frac{ \left( -1 \right)^{j} }{ j \! \left( n + j \right) \! } \left(\frac{x}{2}\right)^{2 j + n} \label{eq:stix62a_9} \\
    \mathit{Y}{\scriptstyle_{n}}(x) & = \frac{ \mathit{J}{\scriptstyle_{n}}(x) \cos\left( n \pi \right) - \mathit{J}{\scriptstyle_{-n}}(x) }{\sin\left( n \pi \right)}  \label{eq:stix62a_10} \\
    \mathit{I}{\scriptstyle_{n}}(x) & = \left( i \right)^{-n} \mathit{J}{\scriptstyle_{n}}(i x)  \label{eq:stix62a_11a} \\
    & = e^{-i n \pi/2} \mathit{J}{\scriptstyle_{n}}\left( x e^{i \pi/2} \right) \label{eq:stix62a_11b} \\
    & = \sum_{m=0}^{\infty} \frac{1}{ m! \left( m + \mid n \mid \right)! } \left( \frac{x}{2} \right)^{ 2 m + \mid n \mid } \label{eq:stix62a_11c}  \\
    \mathit{K}{\scriptstyle_{n}}(x) & = \frac{\pi}{2} \frac{\mathit{I}{\scriptstyle_{-n}}(x) - \mathit{I}{\scriptstyle_{n}}(x)}{\sin\left( n \pi \right)} \label{eq:stix62a_12}
  \end{align}
\end{subequations}

\indent  Some Bessel function relationships are:
\begin{subequations}
  \begin{align}
    \int_{0}^{\infty} \thickspace du \thickspace J{\scriptstyle_{n}}^{2}\left( au \right) \thickspace u \thickspace e^{ - \beta u^{2} } & = \frac{1}{ 2 \beta } e^{ -a^{2}/2 \beta } I{\scriptstyle_{n}}\left( \frac{ a^{2} }{ 2 \beta } \right)  \label{eq:bessel_0a}  \\
    e^{ i \left[ \alpha{\scriptstyle_{s}} \phi + \beta{\scriptstyle_{s}} \sin{\phi} \right] } & = \sum_{m=0}^{\infty} \thickspace J{\scriptstyle_{m}}\left( \beta{\scriptstyle_{s}} \right) e^{ i \left[ \alpha{\scriptstyle_{s}} + m \right] \phi } \label{eq:bessel_0d}  \\
    J{\scriptstyle_{n+1}}\left( x \right) + J{\scriptstyle_{n-1}}\left( x \right) & = \frac{ 2 n }{ x } J{\scriptstyle_{n}}\left( x \right) \label{eq:bessel_0e}  \\
    J{\scriptstyle_{n+1}}\left( x \right) - J{\scriptstyle_{n-1}}\left( x \right) & = -2 \frac{ d J{\scriptstyle_{n}}\left(x\right) }{ dx } \label{eq:bessel_0f}  \\
    \int_{\phi} \thickspace d\phi' \thickspace e^{-i \left[ \alpha{\scriptstyle_{s}} \phi' + \beta{\scriptstyle_{s}} \sin{\phi'} \right] } & = \sum_{n=0}^{\infty} \thickspace J{\scriptstyle_{n}}\left( \beta{\scriptstyle_{s}} \right) \int_{\phi} \thickspace d\phi' \thickspace e^{ -i \left[ \alpha{\scriptstyle_{s}} + n \right] \phi' } \label{eq:bessel_0g}  \\
    & = i \sum_{n=0}^{\infty} \thickspace \frac{ J{\scriptstyle_{n}}\left( \beta{\scriptstyle_{s}} \right) }{ \alpha{\scriptstyle_{s}} + n } e^{ -i \left[ \alpha{\scriptstyle_{s}} + n \right] \phi' } \label{eq:bessel_0h}  \\
    e^{ i x  \sin{\theta} } & = \sum_{ n = -\infty }^{\infty} \thickspace J{\scriptstyle_{n}}\left( x \right) e^{ i n \theta } \label{eq:bessel_0i}
  \end{align}
\end{subequations}
and we can also note that:
\begin{subequations}
  \begin{align}
    \int_{0}^{2 \pi} d\phi e^{i (m - n) \phi} & = 2 \pi \delta{\scriptstyle_{m,n}}  \label{eq:bessel_1a}  \\
    \lim_{x \rightarrow 0} n J{\scriptstyle_{n}}^{2}\left(x\right) & = 0  \label{eq:bessel_1b}
  \end{align}
\end{subequations}

\indent  Bessel function identities:
\begin{subequations}
  \begin{align}
    \sum_{ n = -\infty }^{\infty} \thickspace J{\scriptstyle_{n}}^{2}\left( x \right) & = 1  \label{eq:bessel_2a}  \\
    \sin{\phi} \sum_{n} \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi} & = - i \thickspace \sum_{n} \thickspace J'{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi}  \label{eq:bessel_2b}  \\
    \cos{\phi} \sum_{n} \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi} & = \sum_{n} \thickspace \frac{ n \Omega }{ k{\scriptstyle_{\perp}} V{\scriptstyle_{\perp}}  } \thickspace J{\scriptstyle_{n}}\left( k{\scriptstyle_{\perp}} r \right) \thickspace e^{i n \phi}  \label{eq:bessel_2c}  \\
    \sum_{n} \thickspace J{\scriptstyle_{n}}\left( x \right) \thickspace J'{\scriptstyle_{n}}\left( x \right) & = 0  \label{eq:bessel_2d}  \\
    \sum_{n} \thickspace n^{2} \thickspace J{\scriptstyle_{n}}^{2}\left( x \right) & = \frac{ x^{2} }{ 2 }  \label{eq:bessel_2e}
%%    J{\scriptstyle_{n}}\left( x \right) & = \frac{ 1 }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ \left( x \sin{ \theta } - n \theta \right) }  \label{eq:bessel_2f}  \\
%%    & = \frac{ i^{-n} }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ n \theta } \thickspace e^{ i x \cos{ \theta } }  \label{eq:bessel_2g}  \\
%%    I{\scriptstyle_{n}}\left( x \right) & = \frac{ 1 }{ \pi } \int_{0}^{\pi} \thickspace d\theta \thickspace \cos{ n \theta } \thickspace e^{ x \cos{ \theta } }  \label{eq:bessel_2h}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  The Plasma Dispersion Function
%%----------------------------------------------------------------------------------------
\subsection{The Plasma Dispersion Function}  \label{subsec:plasmadispersion}
\indent  In this section, we will discuss the plasma dispersion function \citep[e.g.,][]{gurnett05}.

\noindent  If we let $\zeta{\scriptstyle_{s}}$ $=$ $\sqrt{m{\scriptstyle_{s}}/(2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})}$ (i p/k), then we define:

\begin{equation}
  \label{eq:pdispf_0}
  Z\left( \zeta \right) = \frac{ 1 }{ \sqrt{\pi} } \int_{C} \thickspace dz \thickspace \frac{ e^{ -z^{2} } }{ z - \zeta }
\end{equation}

\noindent  where the contour C is understood to be along the real z-axis, passing under the pole at z $=$ $\zeta$.  To alter this, we need to consider the Plemelj relation given by:

\begin{equation}
  \label{eq:pdispf_1}
  \lim_{\epsilon \rightarrow 0} \int_{-\infty}^{\infty} \thickspace dx \thickspace \frac{ f\left( x \right) }{ x - \left( x{\scriptstyle_{o}} \pm i \varepsilon \right) } = P \int_{-\infty}^{\infty} \thickspace dx \thickspace \frac{ f\left( x \right) }{ x - x{\scriptstyle_{o}} } \pm i\pi f\left( x{\scriptstyle_{o}} \right)
\end{equation}

\noindent  where $\epsilon$ $>$ 0 and the P refers to the principal value integral defined by:

\begin{equation}
  \label{eq:pdispf_2}
  P \int_{-\infty}^{\infty} ... dx = \lim_{\delta \rightarrow 0}[\int_{-\infty}^{x_{o} - \delta} ...dx + \int_{x_{o} + \delta}^{\infty} ...dx] .
\end{equation}

\noindent  We can define the derivative of the plasma dispersion function as:

\begin{equation}
  \label{eq:pdispf_3}
  \frac{ d Z }{ d \zeta } \equiv Z'\left( \zeta \right) = - 2 \left[ 1 + \zeta Z\left( \zeta \right) \right]
\end{equation}

\noindent  We can expand $Z\left( \zeta \right)$ and $Z'\left( \zeta \right)$ in the limits $\lvert \zeta \rvert$ $\gg$ 1 and $\lvert \zeta \rvert$ $\ll$ 1, which are given by:

\begin{subequations}
  \begin{align}
  Z\left( \zeta \right) & = i \sqrt{\pi} \frac{ k }{ \mid k \mid } e^{ - \zeta^{2} } - \left[ \frac{1}{ \zeta } + \frac{1}{ 2 \zeta^{3} } + \frac{3}{ 4 \zeta^{5} } + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\gg$ 1)}  \label{eq:pdispf_4a} \\
  Z\left( \zeta \right) & = i \sqrt{\pi} \frac{ k }{ \mid k \mid } e^{ - \zeta^{2} } - \left[ 2 \zeta - \frac{4}{3} \zeta^{3} + \frac{8}{15} \zeta^{5} + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\ll$ 1)} \label{eq:pdispf_4b} \\
  Z'\left( \zeta \right) & = -2 i \sqrt{\pi} \frac{ k }{ \mid k \mid } \zeta e^{ - \zeta^{2} } +  \left[ \frac{1}{ \zeta^{2} } + \frac{3}{ 2 \zeta^{4} } + \frac{15}{ 4 \zeta^{6} } + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\gg$ 1)} \label{eq:pdispf_4c} \\
  Z'\left( \zeta \right) & = -2 i \sqrt{\pi} \frac{ k }{ \mid k \mid } \zeta e^{ - \zeta^{2} } - \left[ 2 - 4 \zeta^{2} + \frac{8}{3} \zeta^{4} + \dotsm \right] \text{   (for $\lvert \zeta \rvert$ $\ll$ 1)}  \label{eq:pdispf_4d}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Deriving the Quadratic Equation
%%----------------------------------------------------------------------------------------
\subsection{Deriving the Quadratic Equation} \label{subsec:BasicAlgebra}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: Basic Algebra
%%----------------------------------------------------------------------------------------
\subsubsection{Basic Algebra} \label{subsubsec:BasicAlgebra}

\indent  So before we get too ahead of ourselves, let us review a few important properties of algebraic manipulation.  The first is multiplication by one, which can be seen as:

\begin{subequations}
  \begin{align}
    a & = a \left(\frac{b}{b}\right) \label{eq:multone0}  \\
    \frac{1}{(x - y)} & = \frac{1}{\frac{x}{x}(x - y)} \label{eq:multone10}  \\
    & = \frac{1}{x (1 - \frac{y}{x})} \label{eq:multone11}  \\
    \frac{1}{(x - y)^{n}} & = \frac{1}{(\frac{x}{x})^{n}(x - y)^{n}} \label{eq:multone20}  \\
    & = \frac{1}{x^{n} (1 - \frac{y}{x})^{n}} \label{eq:multone21}
  \end{align}
\end{subequations}

\noindent  and so on and so forth.  The point is, there are a multitude of ways to multiply any factor by the number one.  Note that in the above examples, I am simply doing everything with arbitrary variables and for the anal retentive mathematicians, we'll assume that \textbf{ALL} of those variables are definite real numbers not equal to zero.

\indent  Now let's review a few properties of quotients.  The two most important ones to remember, at least in my mind, are the following:

\begin{subequations}
  \begin{align}
    \left(\frac{a}{\frac{b}{c}}\right) & = \left(\frac{a c}{b}\right)  \label{eq:quotient0}  \\
    \left(\frac{\frac{a}{b}}{c}\right) & =\left(\frac{a}{b c}\right)  \label{eq:quotient1}
  \end{align}
\end{subequations}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: Quadratic Equation
%%----------------------------------------------------------------------------------------
\subsubsection{Quadratic Equation} \label{subsubsec:QuadraticEquation}

\indent  If we let the following variables be defined as constants, \textit{a}, \textit{b}, \textit{c}, where \textit{a} $\neq$ 0 and \textit{b} and \textit{c} are $\in$ $\Re$\footnote{Note that $\in$ is one of the fancy mathematician ways of saying \textit{element of...} while $\Re$ pertains to the set of numbers known as \textit{Reals}.  Also, I should be careful to point out that the ONLY real requirement on any of the constants is that \textit{a} $\neq$ 0, but we'll throw in the real number thing to avoid imaginaries, which tend to obfuscate things.}.  Thus we start with a general second-order\footnote{the highest power of our undetermined variable, \textit{x}, is 2} polynomial equation of the form:

\begin{equation}
  \label{eq:quadratic0}
    a x^{2} + b x + c = 0
\end{equation}

\noindent  where our undetermined variable, \textit{x}, is the unknown we seek to solve for.  Now when one is faced with a general second-order polynomial that cannot be factored, it is typically useful to do something called \textit{completing the square}.  To do so, we first divide both sides of Equation \ref{eq:quadratic0} by \textit{a}\footnote{Were \textit{a} allowed even the slightest possibility to be $=$ 0, mathematicians would have their panties all in a bunch over this step...}:

\begin{subequations}
  \begin{align}
    a x^{2} + b x + c & = 0  \label{eq:quadratic1}  \\
    x^{2} + \left(\frac{b}{a}\right) x + \left(\frac{c}{a}\right) & = 0 \label{eq:quadratic2}
  \end{align}
\end{subequations}

\indent  Now the next step in completing the square requires that we move \textbf{ALL} terms with \textbf{ONLY} constants in them to the opposite side of the equation from that of our unknown variable, \textit{x}.  This changes Equation \ref{eq:quadratic2} to:

\begin{equation}
  \label{eq:quadratic3}
    x^{2} + \left(\frac{b}{a}\right) x = -\left(\frac{c}{a}\right)
\end{equation}

\noindent  and now are trying to change our original equation to something of the form of:  \\

\begin{equation}
  \label{eq:quadratic4}
    \left(x + B \right)^{2} = C
\end{equation}

\noindent  where \textit{B} and \textit{C} are \textbf{NOT} the same as their lower case counterparts.  To see this, we expand the left hand side of Equation \ref{eq:quadratic4} to find:

\begin{equation}
  \label{eq:quadratic5}
    \left(x + B \right)^{2} = x^{2} + 2 B x + B^{2}
\end{equation}

\noindent  where we see that there is a factor of 2 which must be taken into account.  So let's expand the following:

\begin{equation}
  \label{eq:quadratic6}
    \left(x + \left(\frac{b}{a}\right) \right)^{2} = x^{2} + 2 \left(\frac{b}{a}\right) x + \left(\frac{b}{a}\right)^{2}
\end{equation}

\noindent  which still leaves that pesky factor of 2 in our equation, so let's try a different approach.  Instead of Equation \ref{eq:quadratic6}, let's try the following:

\begin{equation}
  \label{eq:quadratic7}
    \left(x + \left(\frac{b}{2 a}\right) \right)^{2} = \left[x^{2} + \left(\frac{b}{a}\right) x \right] + \left(\frac{b}{2 a}\right)^{2}
\end{equation}

\noindent  where we can see the the terms in [ ] are the same as those on the left hand side of Equation \ref{eq:quadratic3}.  Thus, we rearrange Equation \ref{eq:quadratic7} in the following manner to find:

\begin{subequations}
  \begin{align}
    \left\{x + \left(\frac{b}{2 a}\right) \right\}^{2} - \left(\frac{b}{2 a}\right)^{2} & = \left[x^{2} + \left(\frac{b}{a}\right) x \right]  \label{eq:quadratic8}  \\
    & = -\left(\frac{c}{a}\right) \label{eq:quadratic9}
  \end{align}
\end{subequations}

\noindent  Thus we have the following equation of the following form:

\begin{equation}
  \label{eq:quadratic10}
    \left\{x + \left(\frac{b}{2 a}\right) \right\}^{2} - \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right] = 0
\end{equation}

\noindent  which allows us to see that \textit{B} and \textit{C} in Equation \ref{eq:quadratic4} are:

\begin{subequations}
  \begin{align}
    B & \equiv \left(\frac{b}{2 a}\right)  \label{eq:quadratic11} \\
    C & \equiv \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]  \label{eq:quadratic12}
  \end{align}
\end{subequations}

\noindent  Now we return to Equation \ref{eq:quadratic4} and solve for \textit{x} by first taking the square-root of both sides finding:

\begin{subequations}
  \begin{align}
    \left(x + B \right) & = \pm \sqrt{C}  \label{eq:quadratic13} \\
    x & = - B \pm \sqrt{C}  \label{eq:quadratic14}
  \end{align}
\end{subequations}

\noindent  and now substitute in our definitions of \textit{B} and \textit{C} from Equations \ref{eq:quadratic11} and \ref{eq:quadratic12} to find:

\begin{subequations}
  \begin{align}
    x & = - \left(\frac{b}{2 a}\right) \pm \sqrt{\left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]}  \label{eq:quadratic15}  \\
      & = - \left(\frac{b}{2 a}\right) \pm \sqrt{ \left(\frac{2 a}{2 a}\right)^{2} \left[ \left(\frac{b}{2 a}\right)^{2} - \left(\frac{c}{a}\right) \right]} \label{eq:quadratic16}  \\
      & = - \left(\frac{b}{2 a}\right) \pm \left(\frac{1}{2 a}\right) \sqrt{b^{2} - \left(\frac{4 a^{2} c}{a}\right)} \label{eq:quadratic17}  \\
    x & = \frac{-b \pm \sqrt{b^{2} - 4 a c}}{2 a} \label{eq:quadratic18}
  \end{align}
\end{subequations}

\noindent  where Equation \ref{eq:quadratic18} is the commonly seen form of the general solution to any second-order polynomial of the form of Equation \ref{eq:quadratic0}.


\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Heat flux from 10 Moment Diffusive Term
%%----------------------------------------------------------------------------------------
\section{Heat flux from 10 Moment Diffusive Term}
\indent  Let us define the following quantities:
\begin{enumerate}
  \item $\mathbb{Q}$   $\equiv$ heat flux $=$ $\mathbb{Q}{\scriptstyle_{0}}$ $+$ $\mathbb{Q}{\scriptstyle_{1}}$
  \item $\mathbb{T}$($\mathbb{P}$) $\equiv$ temperature(pressure) tensor
  \item $\mathfrak{T}$ $\equiv$ $\mathbb{T}^{-1}$ (inverse of the temperature tensor)
  \item $\mathbb{I}$   $\equiv$ identity tensor
  \item $\tilde{\mathbb{A}}$   $\equiv$ arbitrary 3-rank tensor
  \item $\mathbb{A}$\textbf{:}$\mathbb{B}$ $\equiv$ Frobenius inner product $=$ $\sum_{i,j}$ $\mathbb{A}{\scriptstyle_{ij}}$ $\mathbb{B}{\scriptstyle_{ij}}$
  \item Tr[]           $\equiv$ trace
  \item Sym[]          $\equiv$ tensor symmetrization operator
\end{enumerate}
where for example, we define:
\begin{subequations}
  \begin{align}
    Sym\left[ \nabla \textbf{u} \right] & = \frac{1}{2} \left[ \nabla \textbf{u} + \left( \nabla \textbf{u} \right)^{T} \right]  \label{eq:symmetric_0} \\
    Sym\left[ \mathbb{P} \cdot \nabla \textbf{u} \right] & = Sym\left[ \nabla \cdot \left( \mathbb{P} \textbf{u} \right) - \textbf{u} \nabla \cdot \mathbb{P} \right]  \label{eq:symmetric_1}
  \end{align}
\end{subequations}
which we have chosen because in general, diffusion terms have the form $\nabla$ $\cdot$ ($\tilde{\mathbb{A}}$\textbf{:}$\nabla$\textbf{h}).  Thus we can write:
\begin{subequations}
  \begin{align}
    \mathbb{Q} &= 3 A{\scriptstyle_{1}} Sym\left[ \nabla \mathfrak{T} \right] + 3 A{\scriptstyle_{0}} Sym\left[ \mathbb{I} \cdot Tr\left[ \nabla \mathfrak{T} \right] \right]  \label{eq:symmetric_2}  \\
    \left[ \mathbb{Q}{\scriptstyle_{1}} \right]{\scriptstyle_{ijk}} & = A{\scriptstyle_{1}} \left( \partial{\scriptstyle_{i}} \mathfrak{T}{\scriptstyle_{jk}} + \partial{\scriptstyle_{k}} \mathfrak{T}{\scriptstyle_{ij}} + \partial{\scriptstyle_{j}} \mathfrak{T}{\scriptstyle_{ik}}  \right)   \label{eq:symmetric_3}  \\
    \left[ \mathbb{Q}{\scriptstyle_{0}} \right]{\scriptstyle_{ijk}} & = A{\scriptstyle_{0}} \left[ \delta{\scriptstyle_{ij}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{nk}} + \partial{\scriptstyle_{k}} \mathfrak{T}{\scriptstyle_{nn}} \right) + \delta{\scriptstyle_{ik}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{nj}} + \partial{\scriptstyle_{j}} \mathfrak{T}{\scriptstyle_{nn}} \right) +   \delta{\scriptstyle_{jk}} \cdot \left( 2 \partial{\scriptstyle_{n}} \mathfrak{T}{\scriptstyle_{ni}} + \partial{\scriptstyle_{i}} \mathfrak{T}{\scriptstyle_{nn}} \right) \right] \label{eq:symmetric_4}
  \end{align}
\end{subequations}
where A${\scriptstyle_{0,1}}$ are positive parameters determined by a collisional integral. 

\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: Rotations and Transformations
%%----------------------------------------------------------------------------------------
\section{Rotations and Transformations} \label{sec:RotationsandTransformations}

\subsection{Constructing Rotation Matrices} \label{subsec:ConstructingRotationMatrices}

Let's assume we have two arbitrary vectors, \textbf{A} and \textbf{B}.  Let their unit vectors be denoted by: \textbf{$\hat{a}$} and \textbf{$\hat{b}$}.  If we want to find the parts of vector \textbf{A} which are parallel and perpendicular to \textbf{B}, we can do a couple of things:
\begin{enumerate}
  \item We can find \textbf{A}$_{\perp}$ and \textbf{A}$_{\parallel}$ with dot and cross products, but leave the the resultant vectors in the original coordinate basis
  \item We can find \textbf{A}$_{\perp}$ and \textbf{A}$_{\parallel}$ by rotating both vectors to a new coordinate basis where \textbf{B}' is now the Z'-Axis and \textbf{A}' is in the X'Z'-Plane.
\end{enumerate}
The method to deal with the first method is the following:  1) First find the unit vectors in the typical manner:
\begin{subequations}
  \begin{align}
    \textbf{a} & \equiv \frac{\textbf{A}}{\lvert \textbf{A} \rvert} \label{eq:rotate_1} \\
    \textbf{b} & \equiv \frac{\textbf{B}}{\lvert \textbf{B} \rvert} \label{eq:rotate_2} \text{ ,}
  \end{align}
\end{subequations}
2) then we find the parallel vector by the following method:
\begin{subequations}
  \begin{align}
    \textbf{a}_{\parallel} & = \Bigl(\textbf{a} \cdot \textbf{b}\Bigr)\textbf{b} = \lvert\textbf{a}\rvert \lvert\textbf{a}\rvert \cos{\theta_{ab}} \textbf{b} \label{eq:rotate_3} \\
    \textbf{a}_{\perp}     & \equiv \Bigl(\textbf{b} \times \textbf{a} \Bigr) \times \textbf{a} = \textbf{a} - \Bigl(\textbf{b} \cdot \textbf{a} \Bigr)\textbf{b} \label{eq:rotate_4} \text{ ,}
  \end{align}
\end{subequations}
which only need to be multiplied by the magnitude of the vector, \textbf{A}, to be turned back into vectors.  It should be noted that these two vectors, \textbf{A}$_{\parallel}$ and \textbf{A}$_{\perp}$, satisfy the following condition:
\begin{equation}
  \label{eq:rotate_5}
  \lvert \textbf{A} \rvert = \sqrt{\Bigl(\textbf{A}_{\parallel}\Bigr)^{2} + \Bigl(\textbf{A}_{\perp}\Bigr)^{2}} \equiv \sqrt{\Biggl(\sum_{i}^{3} A_{i}^{2} \Biggr)} \text{ .}
\end{equation}
The second method to find these vectors is by constructing a matrix which can rotate both vectors into a new coordinate system where \textbf{b}' is parallel to the new Z'-Axis and \textbf{a}' is in the X'Z'-Plane.  To do this, we start with the unit vectors again.  The first thing we do is define the following two vectors:
\begin{subequations}
  \begin{align}
    \textbf{c} & \equiv \textbf{b} \times \textbf{a} \label{eq:rotate_6} \\
    \textbf{d} & \equiv \textbf{c} \times \textbf{b} \label{eq:rotate_7}
  \end{align}
\end{subequations}
which we use to construct the following matrix:
\begin{equation}
  \label{eq:rotate_8}
 \textbf{R} = \left[
  \begin{array}{ c c c }
     d_{1} & d_{2} & d_{3}  \\
     c_{1} & c_{2} & c_{3}  \\
     a_{1} & a_{2} & a_{3}
  \end{array} \right]
\end{equation}
The original vectors can now be rotated into a new coordinate system.  Let's consider an example for illustrative purposes.  Let the following be true:
\begin{subequations}
  \begin{align}
    \textbf{A} & = \bigl\{0.2,0.3,0.4 \bigr\} \label{eq:rotate_9} \\
    \textbf{B} & = \bigl\{0.1,0.5,0.7 \bigr\} \label{eq:rotate_10} \\
    \lvert \textbf{A} \rvert & = 0.5385165215 \label{eq:rotate_11} \\
    \lvert \textbf{B} \rvert & = 0.8660253882 \label{eq:rotate_12} \\
    \textbf{a} & = \bigl\{0.37139,0.55709,0.74278 \bigr\} \label{eq:rotate_13} \\
    \textbf{b} & = \bigl\{0.11547,0.57735,0.80829 \bigr\} \label{eq:rotate_14} \\
    \textbf{c} & = \bigl\{-0.02144,0.21442,-0.15010 \bigr\} \label{eq:rotate_15} \\
    \textbf{d} & = \bigl\{0.25997,1.30385\times10^{-8},-0.03714 \bigr\} \label{eq:rotate_16}
  \end{align}
\end{subequations}
where the Y-component of \textbf{d} is a consequence of rounding errors, which I'll show turn out to actually matter.  Thus our matrix is:
\begin{equation}
  \label{eq:rotate_17}
 \textbf{R} = \left[
  \begin{array}{ c c c }
     0.25997 & 1.3038\times10^{-8} & -0.03714  \\
     -0.02144 & 0.21442 & -0.15010  \\
     0.11547 & 0.57735 & 0.80829
  \end{array} \right]
\end{equation}
which produces the following new vectors:
\begin{subequations}
  \begin{align}
    \textbf{a}' & = \bigl\{0.06897,-1.84871\times10^{-9},0.964901 \bigr\} \label{eq:rotate_18} \\
    \textbf{b}' & = \bigl\{-1.87482\times10^{-9},-7.16156\times10^{-9},1.00000 \bigr\} \label{eq:rotate_19} \text{ .}
  \end{align}
\end{subequations}
One can see that \textbf{a}' and \textbf{b}' are not normalized, nor are they what we \emph{expected} them to be.  Meaning, I claimed that \textbf{b}' should be PURELY in the Z'-direction, but this has small, finite values in the X'Y'-Plane.  Before we complain too much about this atrocity, let's normalize the unit vectors, which makes them now:
\begin{subequations}
  \begin{align}
    \textbf{a}' & = \bigl\{0.07129,-1.91108\times10^{-9},0.99746 \bigr\} \label{eq:rotate_20} \\
    \textbf{b}' & = \bigl\{-1.87482\times10^{-9},-7.16156\times10^{-9},1.00000 \bigr\} \label{eq:rotate_21} \text{ .}
  \end{align}
\end{subequations}
Recall that I claimed these \emph{small} rounding errors made a difference in your final answer, so let's go back to our first set of rotated unit vectors in Equations \ref{eq:rotate_18} and \ref{eq:rotate_19} and intentionally force those \emph{small} rounding errors to zero before we renormalize the unit vectors.  Let's define these new ones as \textbf{w}' and \textbf{u}' to avoid confusion with our vectors in Equations \ref{eq:rotate_20} and \ref{eq:rotate_21}, and they become (after renormalizing):
\begin{subequations}
  \begin{align}
    \textbf{w}' & = \bigl\{0.07129,0.00000,0.99746 \bigr\} \label{eq:rotate_22} \\
    \textbf{u}' & = \bigl\{0.00000,0.00000,1.00000 \bigr\} \label{eq:rotate_23} \text{ .}
  \end{align}
\end{subequations}
We now take the magnitudes of our original vectors and multiply that by these unit vectors to get the new vectors:
\begin{subequations}
  \begin{align}
    \lvert \textbf{A} \rvert * \textbf{a}' & \equiv \textbf{A}' = \bigl\{0.0383921,-1.02915\times10^{-9},0.537146 \bigr\} \label{eq:rotate_24} \\
    \lvert \textbf{B} \rvert * \textbf{b}' & \equiv \textbf{B}' = \bigl\{-1.62364\times10^{-9},-6.20209\times10^{-9},0.866025 \bigr\} \label{eq:rotate_25} \\
    \lvert \textbf{A} \rvert * \textbf{w}' & \equiv \textbf{W}' = \bigl\{0.0383921,0.00000,0.537146 \bigr\} \label{eq:rotate_26} \\
    \lvert \textbf{B} \rvert * \textbf{u}' & \equiv \textbf{U}' = \bigl\{0.00000,0.00000,0.866025 \bigr\} \label{eq:rotate_27} \text{ .}
  \end{align}
\end{subequations}
If we use double precision instead of single, our rotation matrix is now:
\begin{equation}
  \label{eq:rotate_28}
 \textbf{R}_{d} = \left[
  \begin{array}{ c c c }
     0.25997347 & -6.5919492\times10^{-17} & -0.037139068  \\
     -0.021442251 & 0.21442251 & -0.15009575  \\
     0.11547005 & 0.57735027 & 0.80829038
  \end{array} \right]
\end{equation}
which produces the following new vectors (after normalization):
\begin{subequations}
  \begin{align}
    \textbf{a}'_{d} & = \bigl\{0.071292300580,9.63019443558\times10^{-18},0.997455466614 \bigr\} \label{eq:rotate_29} \\
    \textbf{b}'_{d} & = \bigl\{-3.88116288919\times10^{-18},1.40180631841\times10^{-18},1.000000000000 \bigr\} \label{eq:rotate_30} \text{ .}
  \end{align}
\end{subequations}
Again we step back and intentionally remove the rounding errors before renormalizing to get (keep the same names this time):
\begin{subequations}
  \begin{align}
    \textbf{a}'_{d} & = \bigl\{0.071292300580,0.000000000000,0.997455466614 \bigr\} \label{eq:rotate_31} \\
    \textbf{b}'_{d} & = \bigl\{0.000000000000,0.000000000000,1.000000000000 \bigr\} \label{eq:rotate_32} \text{ .}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection: Normal Incidence Frame and Coordinate Basis
%%----------------------------------------------------------------------------------------
\subsection{Normal Incidence Frame and Coordinate Basis} \label{subsec:NormalIncidenceFrameandCoordinateBasis}

%%----------------------------------------------------------------------------------------
%%  Subsubsection: The Normal Incidence Frame
%%----------------------------------------------------------------------------------------
\subsubsection{The Normal Incidence Frame} \label{subsubsec:TheNormalIncidenceFrame}

\indent  In this section, we will define our reference frame transformation into the Normal Incidence Frame (NIF) and coordinate basis rotations into the Normal incidence frame Coordinate Basis (NCB).  We will present the transformations/rotations in a generalized manner, but for the purposes of this manuscript the measurements are in the SpaceCraft Frame (SCF) and GSE coordinate basis.  We define the generalized basis as the Input Coordinate Basis (ICB).  In the following, we will use the notation $\mathbf{V}{\scriptstyle_{Coord}}^{Ref}$ to represent a 3-vector in the coordinate basis, \emph{Coord}, and reference frame, \emph{Ref}.

\indent  We can define the velocity transformation from any arbitrary frame of reference (e.g., SCF) to the shock frame of reference (SHF) as:

\begin{equation}
  \label{eq:fieldtrans_0}
  \mathbf{V}{\scriptstyle_{ICB}}^{SHF} = \mathbf{V}{\scriptstyle_{ICB}}^{arb.} - \left( \mathbf{V}{\scriptstyle_{sh, ICB}}^{arb.} \cdot \hat{\mathbf{n}} \right) \hat{\mathbf{n}}
\end{equation}

\noindent  where $\hat{\mathbf{n}}$ is the vector normal to the assumed planar shock front (see Appendix \ref{app:rhequations}).  For an experimentalist's purposes with spacecraft observations, $\mathbf{V}{\scriptstyle_{ICB}}^{arb.}$ $\rightarrow$ $\mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF}$ $\equiv$ the bulk flow solar wind velocity in the SCF and ICB.  Let us define $\left( \mathbf{V}{\scriptstyle_{sh, ICB}}^{arb.} \cdot \hat{\mathbf{n}} \right)$ $=$ $V{\scriptstyle_{sh,n}}^{SCF}$ as the shock speed along the unit normal vector, $\hat{\textbf{n}}$, in the SCF in the upstream region and $U{\scriptstyle_{j,n}}^{SHF}$ as the shock normal speed in the SHF, determined from the numerical Rankine-Hugoniot solution techniques \citep[e.g.,][]{vinas86a, koval08a}, in the $j^{th}$ region.  Let us also define $\langle Q \rangle{\scriptstyle_{region}}$ as the spatial ensemble average of any parameter, $Q$, over a given space (i.e., upstream or downstream)\footnote{Note that $\hat{\textbf{n}}$, $V{\scriptstyle_{sh,n}}^{SCF}$, and $U{\scriptstyle_{j,n}}^{SHF}$ are, by definition, assumed to be averages over the upstream or downstream regions.  I did not include $\langle  \rangle$'s out of laziness.  Note I have also omitted the fact that $\hat{\textbf{n}}$ is generally defined in the ICB in the SCF.}.

\indent  Therefore, we can define the average upstream incident bulk flow velocity in the SHF, which is given by:

\begin{equation}
  \label{eq:fieldtrans_2}
  \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right)  \text{  .}
\end{equation}

\noindent  From the relationship for $\langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{j}}$, we can show that:

\begin{equation}
  \label{eq:fieldtrans_1}
  U{\scriptstyle_{j,n}}^{SHF} = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{j}} \cdot \hat{\mathbf{n}}  \text{  .}
\end{equation}

\indent  There are two physically significant frames of reference:  the Normal Incidence Frame (NIF) and the de Hoffmann-Teller frame (dHT).  The NIF is useful because the upstream flow velocity is entirely along the shock normal vector\footnote{assuming a locally planar discontinuity, as in Appendix \ref{app:rhequations}}.  The dHT frame is useful because the upstream flow velocity is entirely along the upstream averaged quasi-static magnetic field ($\mathbf{B}{\scriptstyle_{u}}$)\footnote{which results in the convective electric field $\rightarrow$ 0}.  The transformation velocity from the SHF to the NIF or dHT are given by:

\begin{subequations}
  \begin{align}
    \mathbf{V}{\scriptstyle_{ICB}}^{NIF} & = \hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)  \label{eq:fieldtrans_3a}  \\
    \mathbf{V}{\scriptstyle_{ICB}}^{dHT} & = \frac{ \hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \right) }{ \hat{\mathbf{n}} \cdot \langle \mathbf{B}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} }  \label{eq:fieldtrans_3b}
  \end{align}
\end{subequations}

\noindent  so that the upstream flow velocity in each reference frame is given by:

\begin{subequations}
  \begin{align}
    \langle \mathbf{V}{\scriptstyle_{ICB}}^{NIF} \rangle{\scriptstyle_{up}} & = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} - \mathbf{V}{\scriptstyle_{ICB}}^{NIF}  \label{eq:fieldtrans_4a}  \\
    \langle \mathbf{V}{\scriptstyle_{ICB}}^{dHT} \rangle{\scriptstyle_{up}} & = \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} - \mathbf{V}{\scriptstyle_{ICB}}^{dHT}  \text{  .}  \label{eq:fieldtrans_4b}
  \end{align}
\end{subequations}

\noindent  Note that $\mathbf{V}{\scriptstyle_{ICB}}^{NIF}$ $=$ $\hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{ICB}}^{SHF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)$ $=$ $\hat{\mathbf{n}} \times \left( \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} \times \hat{\mathbf{n}} \right)$ because $\hat{\mathbf{n}} \times \hat{\mathbf{n}}$ $=$ 0.  Since the change in velocity between any shock rest frame the local SC frame satisfies $\mid$$\boldsymbol{\beta}$$\mid$ $\equiv$ $\mid$$\Delta$$\mathbf{V}\mid$/c $\ll$ 1 for any shock within the heliosphere, the Lorentz transformations of the electric and magnetic fields \citep[page 558 of][]{jackson98a} can be given by:

\begin{subequations}
  \begin{align}
    \mathbf{E}' & = \gamma \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{E} \right)  \label{eq:fieldtrans_5a}  \\
    \lim_{\gamma \rightarrow 1} \mathbf{E}' & \approx \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right)  \label{eq:fieldtrans_5b}  \\
    \mathbf{B}' & = \gamma \left( \mathbf{B} - \boldsymbol{\beta} \times \mathbf{E} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{B} \right)  \label{eq:fieldtrans_5c}  \\
    \lim_{\gamma \rightarrow 1} \mathbf{B}' & \approx \mathbf{B}  \text{  .}  \label{eq:fieldtrans_5d}
  \end{align}
\end{subequations}

\noindent  The difference in flow velocity, $\Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)}$, between the SCF and relevant shock rest frames, i.e., NIF and dHT, is given by:

\begin{subequations}
  \begin{align}
    \Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)} & = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \langle \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)} \rangle{\scriptstyle_{up}}  \label{eq:fieldtrans_6a}  \\
    & = \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left[ \langle \mathbf{V}{\scriptstyle_{bulk, ICB}}^{SCF} \rangle{\scriptstyle_{up}} - \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right) \right] + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)}  \label{eq:fieldtrans_6b}  \\
    & = \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} \right) + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)}  \label{eq:fieldtrans_6c}
  \end{align}
\end{subequations}

\noindent  which allows us to show that the electric field in a relevant shock rest frame, $\mathbf{E}{\scriptstyle_{ICB}}^{NIF,dHT}$, can be determined from the electric field observed in the SCF, $\mathbf{E}{\scriptstyle_{ICB}}^{SCF}$, through the following:

\begin{subequations}
  \begin{align}
    \mathbf{E}{\scriptstyle_{ICB}}^{NIF(dHT)} & = \mathbf{E}{\scriptstyle_{ICB}}^{SCF} + \left( \Delta \mathbf{V}{\scriptstyle_{ICB}}^{SCF2NIF(dHT)} \times \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \right)  \label{eq:fieldtrans_7a}  \\
     & = \mathbf{E}{\scriptstyle_{ICB}}^{SCF} + \left[ \left( V{\scriptstyle_{sh,n}}^{SCF} \hat{\textbf{n}} + \mathbf{V}{\scriptstyle_{ICB}}^{NIF(dHT)} \right) \times \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \right]  \text{  .}  \label{eq:fieldtrans_7b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsubsection:  NIF Coordinate Basis
%%----------------------------------------------------------------------------------------
\subsubsection{NIF Coordinate Basis}  \label{subsubsec:NIFCoordinateBasis}

\indent  We can rotate into the Normal incidence frame Coordinate Basis (NCB) from the Input Coordinate Basis (ICB) by defining a rotation matrix, $\mathbb{A}$ \citep{scudder86a}, given by:

\begin{equation}
  \label{eq:nifbasis_1}
 \mathbb{A} = \left[
  \begin{array}{ c c c }
    n{\scriptstyle_{x}}     & n{\scriptstyle_{y}}     & n{\scriptstyle_{z}}      \\
    \beta{\scriptstyle_{x}} & \beta{\scriptstyle_{y}} & \beta{\scriptstyle_{z}}  \\
    \zeta{\scriptstyle_{x}} & \zeta{\scriptstyle_{y}} & \zeta{\scriptstyle_{z}}
  \end{array} \right]
\end{equation}

\noindent  where $\hat{\mathbf{n}}$ is the shock normal vector and $\boldsymbol{\beta}$ and $\boldsymbol{\zeta}$ are given by:

\begin{subequations}
  \begin{align}
    \hat{\textbf{y}} = \boldsymbol{\beta} & = \frac{ \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{dn}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up}} }{ \mid \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up}} \times \langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{dn}} \mid }  \label{eq:nifbasis_2a}  \\
    \hat{\textbf{z}} = \boldsymbol{\zeta} & = \frac{ \hat{\mathbf{n}} \times \boldsymbol{\beta} }{ \mid \hat{\mathbf{n}} \times \boldsymbol{\beta} \mid }  \label{eq:nifbasis_2b}
  \end{align}
\end{subequations}

\noindent  where $\langle \mathbf{B}{\scriptstyle_{ICB}}^{SCF} \rangle{\scriptstyle_{up(dn)}}$ is the average upstream(downstream) magnetic field vector.  If the vectors $\hat{\mathbf{n}}$, $\boldsymbol{\beta}$, and $\boldsymbol{\zeta}$ start in the ICB (e.g., GSE), then one would expect that $\mathbb{A}$ acting on $\hat{\mathbf{n}}$, $\boldsymbol{\beta}$, or $\boldsymbol{\zeta}$ should give the corresponding NCB axis unit vector.  Meaning, we expect the following to be true:

\begin{subequations}
  \begin{align}
    \mathbb{A} \cdot \hat{\mathbf{n}} & = \langle 1, 0, 0 \rangle  \label{eq:nifbasis_3a}  \\
    \mathbb{A} \cdot \boldsymbol{\beta} & = \langle 0, 1, 0 \rangle  \label{eq:nifbasis_3b}  \\
    \mathbb{A} \cdot \boldsymbol{\zeta} & = \langle 0, 0, 1 \rangle \text{  .}  \label{eq:nifbasis_3c}
  \end{align}
\end{subequations}

\noindent  Thus, $\mathbb{A}$ should rotate any ICB vector into the NCB.

%%  Caveat
\indent  If the coordinate vectors used to create $\mathbb{A}$ are not orthogonal, then the correct rotation tensor is given by $\mathbb{R}$ $=$ ($\mathbb{A}^{T}$)$^{-1}$, or the inverse transpose of $\mathbb{A}$.  The need to perform the inverse transpose of $\mathbb{A}$ arises from the non-orthogonal nature of the NIF basis.  If the NIF were created from an orthogonal basis, then $\mathbb{A}$ would be an orthogonal matrix, which means $\mathbb{A}^{T}$ $=$ $\mathbb{A}^{-1}$.  For any invertible matrix, the following is true:  ($\mathbb{A}^{T}$)$^{-1}$ $=$ ($\mathbb{A}^{-1}$)$^{T}$.  Thus, an orthogonal NIF basis would imply $\mathbb{R}$ $=$ ($\mathbb{A}^{T}$)$^{-1}$ $=$ ($\mathbb{A}^{T}$)$^{T}$ $=$ $\mathbb{A}$.  In general, however, the NIF basis vectors are not orthogonal and thus $\mathbb{R}$ $\neq$ $\mathbb{A}$.


\clearpage
%%----------------------------------------------------------------------------------------
%%  Section: David Oliver's book
%%----------------------------------------------------------------------------------------
\section{Notes from the Shaggy Steed of Physics Book}
%%----------------------------------------------------------------------------------------
%%  Subsection: The Action Principle
%%----------------------------------------------------------------------------------------
\subsection{The Action Principle}
Action \citep{oliver04a} is mathematically defined by the integral:
\begin{equation}
\label{eq:oliver1}
S = \int_{t_{1}}^{t_{2}} L dt
\end{equation}
where, \emph{L} is defined as the Lagrangian\footnote{Note: Oliver refers to the Lagrangian as the \textit{the gene of motion}.}.  The action principle can be stated as follows: \emph{of all the possible paths the particles may take between any two given points in space and time, they take those paths for which the action, S, has the least possible value}.  The Lagrangian is really nothing more than the difference between kinetic and potential energy, in Galilean space-time, but in its evolution, nature seeks to minimize any deviation between kinetic and potential energy, regardless of the continual interchange between the two.  In relativistic space-time, the action becomes the dominating factor in what path, for any given particle, the universe will conspire to create.  
If we define the total energy as:
\begin{equation}
\label{eq:oliver2}
  H = \sum_{\alpha} \textbf{p}_{\alpha} \cdot \dot{\textbf{x}}_{\alpha} - L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)
\end{equation}
where \textbf{p}$_{\alpha}$ is the momentum of a particle defined by:
\begin{equation}
\label{eq:oliver3}
  \textbf{p}_{\alpha} \equiv \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \text{ .}
\end{equation}
The kinetic energy can be defined as:
\begin{equation}
\label{eq:oliver4}
  T \equiv \frac{1}{2} \sum_{\alpha} \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \cdot \dot{\textbf{x}}_{\alpha} \text{ ,}
\end{equation}
and the angular momentum is:
\begin{equation}
\label{eq:oliver5}
  \textbf{J} \equiv \textbf{x}_{\alpha} \times \frac{\partial L\left(\textbf{x}_{\alpha},\dot{\textbf{x}}_{\alpha}\right)}{\partial \dot{\textbf{x}}_{\alpha}} \text{ .}
\end{equation}
We are also met with another way of describing what is meant when one claims \emph{the action must be minimized:}  think of the bottom of a valley as minimum in potential energy.  It can also be said that if the valley is a smoothly varying ''bowl,'' if you will, then one might claim that the bottom of the valley has an approximately zero slope.  What does it mean to have no \emph{slope}?  For any given function, f(q$_{1}$,q$_{2}$,q$_{3}$,$\dotsc$,q$_{n}$), the following statement defines what it means to have \emph{no slope} at some point, q$_{o}$, in space-time:
\begin{equation}
\label{eq:oliver6}
  \frac{\partial f}{\partial q_{i}} \Bigr\rvert_{q_{i} = q_{o}} = 0 \text{ ,}
\end{equation}
or one could also say that the \emph{variation} of a function must vanish at some point, q$_{o}$, in space-time:
\begin{equation}
\label{eq:oliver7}
  \delta f = \frac{\partial f}{\partial q_{i}} \delta q_{i} = 0 \text{ ,}
\end{equation}
where $\delta$q$_{i}$ are the \emph{arguments} of the function, f.  The variation of the function, f, illustrate how small changes in its arguments, q$_{i}$, cause changes in the function itself.  That means, if the partial derivative of one of the arguments vanishes, the variation in \emph{f} suffers no change (e.g. $\partial$f/$\partial$q$_{i}$ = 0, thus $\delta$f = 0 regardless of $\delta$q$_{i}$).  Since the variation of the arguments, $\delta$q$_{i}$, are arbitrary, the vanishing variation of \emph{f} requires that every partial derivative, $\partial$f/$\partial$q$_{i}$, vanishes at the arbitrary point, q$_{o}$.  Thus, \emph{least action} is define as the conditions for which the functions, q(t), satisfy the requirements to force $\delta$S = 0.  The action is defined as a \emph{functional} $\equiv$ \emph{a quantity that has a single value corresponding to an entire function}.  Thus, the variation of the action is defined as:
\begin{equation}
\label{eq:oliver8}
  \delta S \equiv \int_{t_{1}}^{t_{2}} \delta L dt = \int_{t_{1}}^{t_{2}} \Biggl(\frac{\partial L}{\partial q_{i}}\delta q_{i} + \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} \Biggr) dt
\end{equation}
where the second term in the equation is defined as:
\begin{subequations}
  \begin{align}
  \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} & = \frac{\partial L}{\partial \dot{q}_{i}} \frac{d}{dt} \Bigl(\delta q_{i}\Bigr) \label{eq:oliver9} \\
  & = \frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr) - \Biggl[\frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \Biggr)\Biggr] \delta q_{i} \label{eq:oliver10}\text{ .}
  \end{align}
\end{subequations}
An important thing to note is that the path variations, $\delta$q, vanish at the end points, ($\delta$q,t)$_{1}$ and ($\delta$q,t)$_{2}$.  So we look at the first term in Equation \ref{eq:oliver10} and notice the following:
\begin{equation}
  \label{eq:oliver11}
  \int_{t_{1}}^{t_{2}} \frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr) dt = \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \delta q_{i} \Biggr)\Biggr\rvert_{t_{1}}^{t_{2}} = 0\text{ .}
\end{equation}
So we then have the following from Equation \ref{eq:oliver10}:
\begin{equation}
  \label{eq:oliver12}
  \int_{t_{1}}^{t_{2}} \frac{\partial L}{\partial \dot{q}_{i}}\delta \dot{q}_{i} dt = - \int_{t_{1}}^{t_{2}}\Biggl[\frac{d}{dt} \Biggl(\frac{\partial L}{\partial \dot{q}_{i}} \Biggr)\Biggr] \delta q_{i} dt
\end{equation}
which means we've now transformed the second term in Equation \ref{eq:oliver8} into a variation of $\delta$ q$_{i}$, instead of $\delta\dot{q}_{i}$.  This implies that we can do the following:
\begin{equation}
  \label{eq:oliver13}
  \delta S = - \int_{t_{1}}^{t_{2}} \Biggl(\frac{d}{dt} \Bigl(\frac{\partial L}{\partial \dot{q}_{i}} \Bigr) -  \frac{\partial L}{\partial q_{i}}\Biggr)\delta q_{i} dt\text{ .}
\end{equation}
%%----------------------------------------------------------------------------------------
%%  Subsection: Other important definitions
%%----------------------------------------------------------------------------------------
\subsection{Other important definitions}
\textbf{The Total Time Derivative}
\begin{subequations}
  \begin{align}
  \frac{df}{dt} & = \frac{\partial f}{\partial t} + \Biggl(\frac{\partial f}{\partial q_{i}} \dot{q}_{i} + \frac{\partial f}{\partial p_{i}} \dot{p}_{i} \Biggr) \label{eq:oliver14} \\
  & = \frac{\partial f}{\partial t} + \Bigl\{f,\mathcal{H} \Bigr\} \label{eq:oliver15}
  \end{align}
\end{subequations}
where \{f,$\mathcal{H}$\} is called a \emph{Poisson bracket} and $\mathcal{H}$ is the Hamiltonian, defined by:
\begin{equation}
  \label{eq:oliver16}
  \Bigl\{f,g \Bigr\} \equiv \Biggl(\frac{\partial f}{\partial q_{i}} \frac{\partial g}{\partial p_{i}} - \frac{\partial g}{\partial q_{i}} \frac{\partial f}{\partial p_{i}}\Biggr) \text{ .}
\end{equation}
It is important to note that the position-momentum pair is an \emph{antisymmetric manifestation of a symplectic structure}.  Here are some general rules of Poisson brackets:
\begin{enumerate}
  \item If both \emph{f} and \emph{g} are scalars, \{f,g\} is a scalar
  \item If \emph{f} is a vector and \emph{g} is a scalar, \{f,g\} is a vector
  \item If both \emph{f} and \emph{g} are vectors, \{f,g\} is a second rank tensor.
\end{enumerate}
The Hamilton equations of motion allow/show an example of where this notation can be of constructive use with the following two examples/definitions:
\begin{subequations}
  \begin{align}
    \dot{q}_{i} & = \Bigl\{q_{i},\mathcal{H} \Bigr\} \label{eq:oliver17} \\
    \dot{p}_{i} & = \Bigl\{p_{i},\mathcal{H} \Bigr\} \label{eq:oliver18} \\
    \Bigl\{q_{i},q_{j} \Bigr\} & = 0 \label{eq:oliver19} \\
    \Bigl\{p_{i},p_{j} \Bigr\} & = 0 \label{eq:oliver20} \\
    \Bigl\{q_{i},p_{j} \Bigr\} & = \delta_{ij} \label{eq:oliver21}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \begin{split}
    \Bigl\{J_{i},J_{j} \Bigr\} & = \epsilon_{ijk}J_{k}  \\
    \textbf{J} \cdot \Bigl\{\textbf{J},J_{j} \Bigr\} & = J_{i} \Bigl\{J_{i},J_{j} \Bigr\} \\
    & = \epsilon_{ijk} J_{i} J_{k} \\
    & \equiv 0 \label{eq:oliver22}
   \end{split}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \begin{split}
      \Bigl\{J^{2},J_{j} \Bigr\} & = 0 \\
      & = \Bigl\{J_{i} J_{i},J_{j} \Bigr\} \\
      & = 2 J_{i} \Bigl\{J_{i},J_{j} \Bigr\} \label{eq:oliver23}
   \end{split}
  \end{align}
\end{subequations}
\begin{subequations}
  \begin{align}
    \Bigl\{x_{i},J_{j} \Bigr\} & = \epsilon_{ijk} x_{k} \label{eq:oliver24} \\
    \Bigl\{p_{i},J_{j} \Bigr\} & = \epsilon_{ijk} p_{k} \label{eq:oliver25} \\
    \Bigl\{F_{i},J_{j} \Bigr\} & = \epsilon_{ijk} F_{k} \label{eq:oliver26} \\
    \Bigl\{f,J_{j} \Bigr\}     & = 0                    \label{eq:oliver27}
  \end{align}
\end{subequations}
(where \textbf{F} is any arbitrary vector function and \emph{f} is any arbitrary scalar).  Phase space is always an even-dimensional manifold that describes the space of motion.  This motion fills phase space with the phase trajectories described by q(t) and p(t).  The essence of configuration space is Euclidean while phase space is symplectic\footnote{\emph{Symplectic structure} is named after the Greek word, $\pi \lambda \epsilon \kappa \acute{o} \varsigma$, meaning \emph{twined} or \emph{braided}.  It is an antisymmetric pairing of coordinates induced by the action principle.}.  If we define the following as the single 2s state vector of phase space:
\begin{equation}
  \label{eq:oliver28}
  \xi = \bigl(q , p \bigr)
\end{equation}
where the first s-components of $\xi$ are position coordinates of all the particles in the phase space you're trying to describe.  The \emph{symplectic} is a 2s $\times$ 2s antisymmetric matrix defined as:
\begin{equation}
  \label{eq:oliver29}
 \textit{J} = \left[
  \begin{array}{ c c }
     0 & \textit{I} \\
     -\textit{I} & 0
  \end{array} \right]
\end{equation}
where \textit{I} is the identity or unit matrix (of dimension \emph{s}).  The \textit{J} is a perfect example of symplectic space (Oliver calls it the \emph{signature} of symplectic phase space).  It has the following properties:
\begin{equation}
  \label{eq:oliver30}
   \textit{J} = - \textit{J}^{\dagger} = - \textit{J}^{-1} 
\end{equation}
where \textit{J}$^{\dagger}$ is defined by:
\begin{equation}
  \label{eq:oliver31}
   \textit{J}_{ij}^{\dagger} = -\textit{J}_{ji}^{*}
\end{equation}
or, in other words, the \emph{transpose conjugate}.  The symplectic also has a square:
\begin{equation}
  \label{eq:oliver32}
 \textit{J}^{2} = - \left[
  \begin{array}{ c c }
     0 & \textit{I} \\
     -\textit{I} & 0
  \end{array} \right] = - \textit{J}
\end{equation}
and it has the \emph{defining property} of inducing a vanishing scalar product on any phase space vector:
\begin{equation}
  \label{eq:oliver33}
   \xi_{i} \textit{J}_{ij}\xi_{j} = 0 \text{ (often seen as } \xi \textit{J} \xi = 0 \text{).}
\end{equation}
The symplectic allows us to rewrite the Poisson bracket notation in a different manner:
\begin{equation}
  \label{eq:oliver34}
   \Bigl\{f,g \Bigr\} = \frac{\partial f}{\partial \xi} \textit{J} \frac{\partial g}{\partial \xi}
\end{equation}
which allows one to look at the Poisson bracket of any function, f($\xi$), with the phase space vector, $\xi$:
\begin{equation}
  \label{eq:oliver35}
   \Bigl\{\xi,f \Bigr\} = \textit{J} \frac{\partial f}{\partial \xi} \text{ .}
\end{equation}
We are finally allowed to look at the Hamiltonian equations using the symplectic and phase space vectors:
\begin{equation}
  \label{eq:oliver36}
   \dot{\xi} = \Bigl\{\xi,\mathcal{H} \Bigr\} = \textit{J} \frac{\partial \mathcal{H}}{\partial \xi} \text{ .}
\end{equation}
A useful relationship between any two quantities, F$\bigl(\xi\bigr)$ and G$\bigl(\xi \bigr)$, can be shown to be:
\begin{equation}
  \label{eq:oliver37}
    \Bigl\{F,G \Bigr\} = \frac{\partial F}{\partial \xi} \textit{J} \frac{\partial G}{\partial \xi} = \frac{\partial F}{\partial \xi} \cdot \Bigl\{\xi,G \Bigr\} = -\frac{\partial G}{\partial \xi} \cdot \Bigl\{\xi,F \Bigr\} \text{ .}
\end{equation}
So the Poisson bracket illustrates a number of points:
\begin{enumerate}
  \item The Poisson bracket is a \emph{projection of the normals of the level surfaces of one quantity upon the tangents of the level surfaces of the other}.
  \item If $\bigl\{$F,G$\bigr\}$ $\ne$ 0, the \emph{flow of F} does NOT stay on level surfaces of G, rather it cuts ACROSS them! $\Rightarrow$ G is NOT a constant on the flow of F.
  \item If $\bigl\{$F,G$\bigr\}$ $=$ 0, the \emph{flow of F} not only stays on its own level surface, it is on the level surfaces of G too. $\Rightarrow$ The two quantities become one common surface to which both flows are confined.
  \item Every mechanical quantity, F$\bigl(\xi\bigr)$, has an image in phase space as \emph{sheets} of level surfaces filled with streamlines generated by $\bigl\{ \xi$,F$\bigr\}$ $\equiv$ the flow. $\Rightarrow$ The Poisson bracket describes the intersection of the two flows produced by F$\bigl(\xi\bigr)$ and G$\bigl(\xi \bigr)$.
  \item The \emph{phase flow} is incompressible.
\end{enumerate}
%%----------------------------------------------------------------------------------------
%%  Subsection: Hamilton-Jacobi Theory
%%----------------------------------------------------------------------------------------
\subsection{Hamilton-Jacobi Theory}
\emph{The motion of the world is imaged as a flow in phase space}.  The manner in which to find these \emph{flows} involves the integrals to Hamilton's equations.  The mathematical forms of the action principle, Hamilton's equations, and Poisson brackets are independent of the coordinate system they are expressed in.  \emph{Motion with s-degrees of freedom has 2s canonical coordinates which form s-conjugate pairs}. $\Rightarrow$ Any set of canonical coordinates is related to another set by transformations which preserve the action principle.
Consider the set of canonical coordinates (Q,P), where Q = (Q$_{1}$,Q$_{2}$,$ \dotsc$,Q$_{s}$) and P = (P$_{1}$,P$_{2}$,$ \dotsc$,P$_{s}$).  Though it may appear that Q and P are actual position and momentum coordinates, they need not be.  Now Hamilton's equations are:
\begin{subequations}
  \begin{align}
    \dot{Q}_{i} & = \quad \frac{\partial \mathcal{H}}{\partial P_{i}} \label{eq:oliver38} \\
    \dot{P}_{i} & = -\frac{\partial \mathcal{H}}{\partial Q_{i}} \label{eq:oliver39}
  \end{align}
\end{subequations}
so that now $\mathcal{H}$ $\rightarrow$ $\mathcal{H}$'$\bigl(Q,P\bigr)$ which still satisfies:
\begin{equation}
  \label{eq:oliver40}
  \delta S = \delta \int_{t_{1}}^{t_{2}} \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr) =  \delta \int_{t_{1}}^{t_{2}} \Bigl(P_{i}dQ_{i} - \mathcal{H}' dt \Bigr) \text{ .}
\end{equation}
These two integrals may differ by any function, F, which has a vanishing variation (i.e. $\delta$F = 0).  Thus the difference between the integrals in Equation \ref{eq:oliver40} must be the total differential of F:
\begin{equation}
  \label{eq:oliver41}
  dF = p_{i}dq_{i} - P_{i}dQ_{i} - \Bigl(\mathcal{H} - \mathcal{H}'\Bigr)dt
\end{equation}
which defines what is referred to as \emph{the generating function}.  The generating function, from Equation \ref{eq:oliver41}, is F = F$\bigl($q,Q,t$\bigr)$.  The relationship of any coordinate can be described as follows:
\begin{subequations}
  \begin{align}
    p_{i} & = \quad \frac{\partial F}{\partial q_{i}} \label{eq:oliver42} \\
    P_{i} & = -\frac{\partial F}{\partial Q_{i}} \label{eq:oliver43} \\
    \Bigl(\mathcal{H} - \mathcal{H}'\Bigr) & = \quad \frac{\partial F}{\partial t} \label{eq:oliver44}
  \end{align}
\end{subequations}
but the generating function can be written in a different form as:
\begin{equation}
  \label{eq:oliver45}
  dG = d\Bigl(F + P_{i}Q_{i}\Bigr) = p_{i}dq_{i} + P_{i}dQ_{i} - \Bigl(\mathcal{H} - \mathcal{H}'\Bigr)dt \text{ .}
\end{equation}
Here, G = G$\bigl($q,P,t$\bigr)$, where the coordinates are:
\begin{subequations}
  \begin{align}
    p_{i} & = \quad \frac{\partial G}{\partial q_{i}} \label{eq:oliver46} \\
    Q_{i} & = \quad \frac{\partial G}{\partial P_{i}} \label{eq:oliver47} \\
    \Bigl(\mathcal{H} - \mathcal{H}'\Bigr) & = - \frac{\partial G}{\partial t} \label{eq:oliver48}
  \end{align}
\end{subequations}
\begin{enumerate}
  \item The generating function incorporates ONE coordinate from the old pair and ONE coordinate from the new pair.
  \item The canonical transformation presents one of the coordinates explicitly and one of them implicitly.  Meaning, in the transformation from the state (q,p) to the state (Q,P) by the generating function, G$\bigl($q,P,t$\bigr)$, the implicit-explicit nature can be seen in Equations \ref{eq:oliver49} and \ref{eq:oliver50}.
  \item \emph{Explicit Dependence} $\equiv$ A direct relationship between two quantities, e.g. f(t) explicitly depends on $t$ if the variable $t$ exists directly in the function f(t), NOT if a variable in f(t) has a dependence on time.  Meaning, f$\bigl($\textbf{x}(t)$\bigr)$ would not explicitly depend on $t$ UNLESS $t$ existed indpendent of \textbf{x}(t) in the function.
  \item \emph{Implicit Dependence} $\equiv$ An indirect relationship between two quantities, e.g. f$\bigl($\textbf{x}(t)$\bigr)$ implicitly depends on the variable $t$, but \textbf{x} explicitly depends on $t$
\end{enumerate}
\begin{subequations}
  \begin{align}
    \frac{\partial}{\partial q_{i}} G\Bigl(q, P, t\Bigr) & = p_{i} \label{eq:oliver49} \\
    Q_{i} & = \frac{\partial}{\partial P_{i}} G\Bigl(q, P, t\Bigr)  \label{eq:oliver50}
  \end{align}
\end{subequations}
where the new coordinates, Q$\bigl($q,p$\bigr)$, are given \textbf{explicitly} by Equation \ref{eq:oliver50} and the coordinates, P$\bigl($q,p$\bigr)$, are given \textbf{implicitly} by Equation \ref{eq:oliver49}.  The two following examples illustrate some trivial transformations:
\begin{subequations}
  \begin{align}
    G\Bigl(q, P, t\Bigr) & = q_{i} P_{i} \label{eq:oliver51} \\
    F\Bigl(q, Q, t\Bigr) & = q_{i} Q_{i} \label{eq:oliver52}
  \end{align}
\end{subequations}
yield the following identity and inverse-identity transformations: 1) for G$\bigl($q,P,t$\bigr)$ we have the identity transformation given by:
\begin{subequations}
  \begin{align}
    Q_{i} & = q_{i} \label{eq:oliver53} \\
    P_{i} & = p_{i} \label{eq:oliver54} \\
    \mathcal{H}' & = \mathcal{H} \label{eq:oliver55}
  \end{align}
\end{subequations}
and 2) for F$\bigl($q,Q,t$\bigr)$ we have the inverse-identity transformation given by:
\begin{subequations}
  \begin{align}
    Q_{i} & = \quad p_{i} \label{eq:oliver56} \\
    P_{i} & = -q_{i} \label{eq:oliver57} \\
    \mathcal{H}' & = \quad \mathcal{H} \label{eq:oliver58} \text{ .}
  \end{align}
\end{subequations}
A nineteenth-century astronomer, C.E. Delaunay, used the fact that the transformations must ONLY be canonical in nature by attempting to simplify the problem of motion.  In his attempts, he found coordinates that are now referred to as \emph{elementary flow} coordinates, which occur when one of the coordinates, P$_{i}$ or Q$_{i}$, are selected as constants (i.e. assume we chose P$_{i}$ $\equiv$ I$_{i}$, then $\dot{I}_{i}$ = 0 and Q$_{i}$ $\equiv$ $\alpha_{i}$).  Now Hamilton's equations simplify dramatically to:
\begin{subequations}
  \begin{align}
    \dot{\alpha}_{i} & = \quad \frac{\partial \mathcal{H}'}{\partial I_{i}} \label{eq:oliver59} \\
    \dot{I}_{i} & = -\frac{\partial \mathcal{H}'}{\partial \alpha_{i}} = 0 \label{eq:oliver60} 
  \end{align}
\end{subequations}
which simplifies our job of solving Hamilton's equations even more than just having Equation \ref{eq:oliver60} be null.  The reason for the underlying simplicity is due to the coordinate's symplectic union.  This equation also shows us that the Hamiltonian is now only a function of one canonical coordinate, namely $\mathcal{H}$' = $\mathcal{H}$'$\bigl(I\bigr)$.  We may now rewrite the R.H.S. of Equation \ref{eq:oliver61} as a function of only $I$ also:
\begin{equation}
  \label{eq:oliver61}
  \omega_{i}\bigl(I\bigr) \equiv \frac{\partial \mathcal{H}'}{\partial I_{i}} \text{ .}
\end{equation}
This reformulation of the coordinates allows for a remarkably trivial integral form, which might otherwise be seemingly impossible:
\begin{equation}
  \label{eq:oliver62}
  \alpha_{i} = \int dt \Bigl(\frac{\partial \mathcal{H}'}{\partial I_{i}}\Bigr) = \omega_{i} t + \beta_{i}
\end{equation}
where $\beta_{i}$ (i = 1, 2, $\dotsc$, s) are the integration constants.
\begin{enumerate}
  \item The elementary flow ONLY ''flows'' along the $\alpha$-coordinates $\Rightarrow$ its phase velocity has no components in the invariant coordinate, $I$, since $\dot{I}$ = 0.
  \item The integrals of elementary flow depend upon the two constants of integration, $I$ and $\beta$, which make up the two sets of \emph{s} quantities.
  \item We can define the \emph{Phase Vector} (Equation \ref{eq:oliver63}), the phase vector's \emph{Phase Velocity} (Equation \ref{eq:oliver64}), and the \emph{Integration Constants} (Equation \ref{eq:oliver65}).
\end{enumerate}
\begin{subequations}
  \begin{align}
    \Xi         & = \bigl(\alpha, I\bigr) \label{eq:oliver63} \\
    \Omega      & = \bigl(\omega, 0\bigr) \label{eq:oliver64} \\
    \mathcal{I} & = \bigl(\beta,  I\bigr) \label{eq:oliver65}
  \end{align}
\end{subequations}
Thus the elementary flow can be described by:
\begin{equation}
  \label{eq:oliver66}
  \Xi\bigl(t\bigr) = \Omega t + \mathcal{I} \text{ .}
\end{equation}
The elementary flow can be seen to depend upon the constants of flow, $\mathcal{I}$, which are also invariant in coordinate phase space because: \emph{Quantities invariant on the flow in one set of coordinates are invariant on the IMAGE of this flow in all other canonical coordinates} $\Rightarrow$ they are the 2s invariants of motion!  This is important because the elementary phase space coordinates, $\Xi$ = $\bigl(\alpha, I\bigr)$, are NOT connected to their image phase space coordinates, $\xi$ = $\bigl(q, p\bigr)$, in any simple way (typically a VERY ''ugly'' transformation connects them).  However, the invariants are the ''same'' in both phase spaces, linking the two together, satisfying an important conservation law:
\begin{equation}
  \label{eq:oliver67}
  \Delta\bigl(\mathcal{I}\bigr) = 0 \text{ .}
\end{equation}
These invariants also satisfy the condition that its rate of change along the ''flow'' (i.e. its total time derivative) vanishes by:
\begin{equation}
  \label{eq:oliver68}
  \frac{d\mathcal{I}}{dt} = \frac{\partial \mathcal{I}}{\partial t} + \Bigl\{\mathcal{I}, \mathcal{H}\Bigr\} = 0 \text{ .}
\end{equation}
It is often the case where $\mathcal{I}$ $\ne$ $\mathcal{I}\bigl(t\bigr)$, \textbf{explicitly} (i.e. $\partial \mathcal{I}/\partial t = 0$), but only a function of the canonical phase space coordinates, $\mathcal{I}$ $=$ $\mathcal{I}\bigl(q,p\bigr)$.  Thus any quantity which does \textbf{NOT EXPLICITLY} depend upon time satisfies the following:
\begin{equation}
  \label{eq:oliver69}
  \Bigl\{\mathcal{I}, \mathcal{H}\Bigr\} = 0 \text{ ,}
\end{equation}
namely, it's Poisson bracket with the Hamiltonian vanishes.
%%----------------------------------------------------------------------------------------
%%  Subsection: Action Again
%%----------------------------------------------------------------------------------------
\subsection{Action Again}
Since we can say that the Lagrangian is really just the total time derivative of the action, we can also say:
\begin{equation}
  \label{eq:oliver70}
    dS = -\mathcal{H} dt + p_{i} dq_{i} = \frac{\partial S}{\partial t}dt + \frac{\partial S}{\partial q_{i}}dq_{i} \text{ .}
\end{equation}
As one might expect from our previous treatments of such things, we can say:
\begin{subequations}
  \begin{align}
    \mathcal{H}  & = -\frac{\partial S}{\partial t} \label{eq:oliver71} \\
    p_{i}        & = \quad \frac{\partial S}{\partial q_{i}}dq_{i} \label{eq:oliver72} 
  \end{align}
\end{subequations}
and since $\mathcal{H}$ $=$ $\mathcal{H}\bigl(q,p\bigr)$, we can rewrite Equation \ref{eq:oliver71} as:
\begin{equation}
  \label{eq:oliver73}
    \frac{\partial S}{\partial t} + \mathcal{H}\Bigl(q,\frac{\partial S}{\partial q}\Bigr) = 0 \text{ .}
\end{equation}
We now know that S = S$\bigl(q,t\bigr)$ is a solution to the 1$^{st}$-Order partial differential equation in the $s$-position coordinates, $q$, and the time, $t$.  The solution, in general, depends upon $s$ $+$ 1 constants of integration, where one of these constants is purely additive; meaning, if S$\bigl(q,t\bigr)$ is a solution of the Hamilton-Jacobi equation, then S$\bigl(q,t\bigr)$ $+$ $A$ is too (assuming $A$ is an additive constant)\footnote{So, it's not entirely clear why, but the remaining constants must be invariants of motion.}.  We also assume the total energy of our system is constant, meaning, $\mathcal{H}$ $=$ $\mathcal{E}$ $=$ $\partial S/\partial t$, which leads to an action of the form:
\begin{equation}
  \label{eq:oliver74}
    S\Bigl(q,I,t\Bigr) = -\mathcal{E} t + S_{o}\Bigl(q,I\Bigr)
\end{equation}
where S$_{o}\Bigl(q,I\Bigr)$ is the time-independent part of the action.  One should also note that the constant, $\mathcal{E}$, is one of the invariants, $I$ $=$ $\bigl(I_{1},I_{2},\dotsc,I_{s}\bigr)$.  So now we have the new momentum-like coordinates, P $\equiv$ $I$ in G$\bigl(q, P, t\bigr)$ $\equiv$ S$_{o}\Bigl(q,I\Bigr)$.  This leaves the remaining coordinates as:
\begin{subequations}
  \begin{align}
    p_{i}      & = \frac{\partial}{\partial q_{i}} S_{o}\Bigl(q,I\Bigr)\label{eq:oliver75} \\
    \alpha_{i} & = \frac{\partial}{\partial I_{i}} S_{o}\Bigl(q,I\Bigr) \label{eq:oliver76} \\
  \end{align}
\end{subequations}
or they may also be expressed as:
\begin{subequations}
  \begin{align}
    p_{i}      & = \frac{\partial}{\partial q_{i}} S\Bigl(q,I,t\Bigr) \label{eq:oliver77} \\
    \beta_{i} & = \frac{\partial}{\partial I_{i}} S\Bigl(q,I,t\Bigr) \label{eq:oliver78} \\
  \end{align}
\end{subequations}
which leads us to the conclusion that the Hamiltonians in the two sets of coordinates are the same, just of different form, given by:
\begin{equation}
  \label{eq:oliver79}
    \mathcal{H}'\Bigl(I\Bigr) = \mathcal{H}\Bigl(q, p\Bigr) \text{ .}
\end{equation}
We already know that we can write the action as an integral of the canonical coordinates:
\begin{equation}
  \label{eq:oliver80}
    S = \int \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr) 
\end{equation}
but this form is \emph{clearly} not an invariant\footnote{Due the indefinite nature of the integral and the lack of rotational invaraince in the p$_{i}$dq$_{i}$ terms, the integral can't be said to be an invariant of motion.}, so we reconsider this case as a contour integral over a closed contour, $\gamma$, in phase space as:
\begin{equation}
  \label{eq:oliver81}
    S = \oint_{\gamma} \Bigl(p_{i}dq_{i} - \mathcal{H}dt \Bigr)  \text{ .}
\end{equation}
One should note that the contour, $\gamma$, itself is not invariant because it is deformed as it is swept through phase space by the flow, however, the integral over this moving contour can be invariant\footnote{The invariance arises from the integration within a closed boundary.  When one considers the integral at hand, one can see we are really integrating the Lagrangian within a closed boundary.  That means, for this integral to NOT be an invariant of motion, requires a violation of the conservation of energy}.  This closed integral is the Poincar$\acute{e}$ invariant.  Though this invariant exists for all motion, its form is completely opaque unless the canonical coordinates are known functions and one can actually solve the integrals. 
%%----------------------------------------------------------------------------------------
%%  Subsection: Hooke Motion
%%----------------------------------------------------------------------------------------
\subsubsection{Hooke Motion}
As a way to illustrate how these transformations work, we'll consider a few examples.  The first of which is an idea proposed by Robert Hooke, one of Newton's most prominent contemporaries\footnote{Hooke happened to be a rather short man, while Newton was very tall.  Both men did not get along very well, and as a way to mock Hooke, Newton said the famous quote: \emph{If I have seen further it is by standing on ye shoulders of Giants.}} involved a force corresponding to the potential:
\begin{equation}
  \label{eq:oliver82}
    V\bigl(q\bigr) = \frac{\kappa q^{2}}{2} \text{ ,}
\end{equation}
where $\kappa$ is a constant.  It gives rise to a force, denoted by:
\begin{equation}
  \label{eq:oliver83}
    -\frac{\partial}{\partial q}V\bigl(q\bigr) = -\kappa q \equiv \mathcal{F} \text{ ,}
\end{equation}
which really doesn't correspond to any fundamental force in nature, it's only an approximation to the force between two bodies bound by an elastic material (e.g. a spring)\footnote{Oliver goes into a paragraph-long explanation of how the inverse-square law Coulomb force actually averages out to a linear force when dealing with the macroscopic scale of a spring.  This results in enormous cancelations of forces.}.  The Hamiltonian can be written as:  
\begin{equation}
  \label{eq:oliver84}
    \mathcal{H} = \frac{p^{2}}{2 m} + \frac{\kappa q^{2}}{2} \text{ ,}
\end{equation}
corresponding to the Hamiltonian for the simple harmonic oscillator with natural frequency, $\omega_{o}$ $=$ $\sqrt{\kappa/m}$.  The canonical invariant of motion and its elementary flow (like all elementary flows) turns out to be:
\begin{subequations}
  \begin{align}
    I & = \mathcal{H}/\omega_{o} \label{eq:oliver85} \\
    \alpha_{i} & = \omega_{i} + \beta_{i} \label{eq:oliver86} 
  \end{align}
\end{subequations}
which gives us a new Hamiltonian, $\mathcal{H}'\bigl(I\bigr)$ $=$ $\omega_{o} I$, and the coordinates, $\bigl(\alpha, I\bigr)$.  The generating function for this case is the action found in Equation \ref{eq:oliver74}.  This can be found from solving Equation \ref{eq:oliver73} for one degree of freedom, which reduces to:
\begin{equation}
  \label{eq:oliver87}
  \frac{dS_{o}}{dq} = p\bigl(q\bigr)
\end{equation}
After some algebraic manipulation, the integral for S$_{o}$ can be found\footnote{Solve for p(q) in Equation \ref{eq:oliver84} by replacing $\kappa$ with $m\omega_{o}^{2}$, and $\mathcal{H}$ with $\mathcal{H}'$ $=$ I$\omega_{o}$.} to be:
\begin{equation}
  \label{eq:oliver88}
  S_{o}\bigl(q, I\bigr) = \int dq \sqrt{2 m \omega_{o} \Bigl(I - \frac{m \omega_{o} q^{2}}{2}  \Bigr)} \text{ .}
\end{equation}

\clearpage
\appendix
%%----------------------------------------------------------------------------------------
%%  Appendix:  Definitions
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\section{Definitions}  \label{app:Definitions}
%%----------------------------------------------------------------------------------------
%%  Appendix:  Symbols and Parameters
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Symbols and Parameters}  \label{subapp:SymbolsandParameters}
\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Fundamental Constants}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\varepsilon{\scriptstyle_{o}}$ $\equiv$ permittivity of free space [F m$^{-1}$ or s$^{2}$ C$^{2}$ kg$^{-1}$ m$^{-3}$]
    \item  $\mu{\scriptstyle_{o}}$ $\equiv$ permeability of free space [H  m$^{-1}$ or kg m C$^{-2}$]
    \item  $c$ $=$ 1/$\sqrt{ \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} }$ $\equiv$ speed of light in vacuum [m s$^{-1}$]
    \item  $e$ $\equiv$ fundamental charge [C]
    \item  k${\scriptstyle_{B}}$ $\equiv$ Boltzmann constant [J K$^{-1}$ or kg m$^{2}$ s$^{-2}$ K$^{-1}$]
  \end{enumerate}
  \item  \textbf{Particle/Plasma-Related Parameters}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  m${\scriptstyle_{s}}$ $\equiv$ mass of particle species $s$ [kg]
    \item  q${\scriptstyle_{s}}$ $\equiv$ charge of particle species $s$ [C]
    \item  n${\scriptstyle_{s}}$ $\equiv$ number density of particle species $s$ [m$^{-3}$]
    \item  $\rho{\scriptstyle_{s}}$ $=$ m${\scriptstyle_{s}}$ n${\scriptstyle_{s}}$ $\equiv$ mass density of particle species $s$ [kg m$^{-3}$]
    \item  B${\scriptstyle_{o}}$ $\equiv$ quasi-static magnetic field magnitude [T]
    \item  \textbf{j} $\equiv$ electrical current density [e.g., from Ampere's law]
    \item  $\omega{\scriptstyle_{ps}}$ $=$ $\sqrt{ n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}/(m{\scriptstyle_{s}} \varepsilon{\scriptstyle_{o}}) }$ $\equiv$ plasma frequency of particle species $s$ [rad s$^{-1}$]
    \item  $\Omega{\scriptstyle_{cs}}$ $=$ q${\scriptstyle_{s}}$B${\scriptstyle_{o}}$/($\gamma$ m${\scriptstyle_{s}}$) $\equiv$ cyclotron frequency of particle species $s$ [rad s$^{-1}$]\footnote{$\gamma$ $\equiv$ Lorentz frame transformation factor $=$ $\left[ 1 - \left( v/c \right)^{2} \right]^{-1/2}$}
    \item  $\mathbf{V}{\scriptstyle_{s}}$ $\equiv$ bulk flow velocity\footnote{refers to the 1st velocity moment} of species $s$ [m s$^{-1}$]
    \item  T${\scriptstyle_{s}}$ $\equiv$ average temperature\footnote{refers to the 2nd velocity moment in the bulk flow rest frame per unit mass} of particle species $s$ [eV or K]
    \item  V${\scriptstyle_{Ts}}$ $=$ $\sqrt{ (2 k{\scriptstyle_{B}} T{\scriptstyle_{s}})/m{\scriptstyle_{s}} }$ $\equiv$ average thermal speed\footnote{here it is the \emph{most probable speed}, whereas V${\scriptstyle_{Ts}}$/$\sqrt{2}$ is the \emph{root mean square speed}} of particle species $s$ [m s$^{-1}$]
    \item  $\lambda{\scriptstyle_{s}}$ $=$ c/$\omega{\scriptstyle_{ps}}$ $\equiv$ inertial length (or skin depth) of particle species $s$ [m]
    \item  $\lambda{\scriptstyle_{Ds}}$ $=$ $\sqrt{ (\varepsilon{\scriptstyle_{o}} k{\scriptstyle_{B}} T{\scriptstyle_{s}})/(n{\scriptstyle_{s}} q{\scriptstyle_{s}}^{2}) }$ $\equiv$ Debye length of particle species $s$ [m]
    \item  $\rho{\scriptstyle_{cs}}$ $=$ V${\scriptstyle_{Ts}}$/$\omega{\scriptstyle_{ps}}$ $\equiv$ thermal gyroradius of particle species $s$ [m]
    \item  V${\scriptstyle_{A}}$ $=$ $\sqrt{ B{\scriptstyle_{o}}^{2}/( \mu{\scriptstyle_{o}} m{\scriptstyle_{i}} n{\scriptstyle_{i}} ) }$ $\equiv$ Alfv\'{e}n speed [m s$^{-1}$]\footnote{we also refer to an electron Alfv\'{e}n speed, V${\scriptstyle_{Ae}}$, on occasion, but it does not have the same physical significance as V${\scriptstyle_{A}}$}
    \item  C${\scriptstyle_{s}}$ $=$ $\sqrt{ k{\scriptstyle_{B}} \left( Z{\scriptstyle_{i}} \gamma{\scriptstyle_{e}} T{\scriptstyle_{e}} + \gamma{\scriptstyle_{i}} T{\scriptstyle_{i}} \right)/m{\scriptstyle_{i}} }$ $\equiv$ ion sound speed in an electron-ion plasma with differing ratios of specific heats\footnote{typical values are $\gamma{\scriptstyle_{e}}$ $=$ 3 (or 1) and $\gamma{\scriptstyle_{i}}$ $=$ 1}, $\gamma{\scriptstyle_{e}}$ and $\gamma{\scriptstyle_{i}}$, for each species and ion charge state, $Z{\scriptstyle_{i}}$.
    \item  P${\scriptstyle_{s}}$ $=$ n${\scriptstyle_{s}}$ k${\scriptstyle_{B}}$ T${\scriptstyle_{s}}$ $\equiv$ thermal pressure of particle species $s$ [Pa or N m$^{-2}$ or J m$^{-3}$ or kg m$^{-1}$ s$^{-2}$]
  \end{enumerate}
  \item  \textbf{Wave/Fluctuation-Related Parameters}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\omega$ $\equiv$ angular frequency (typically used for waves)
    \item  k $\equiv$ wavenumber [$=$ 2$\pi$/$\lambda$]
    \item  $\lambda$ $\equiv$ wavelength (typically when no subscript is present)
  \end{enumerate}
  \item  \textbf{Useful Relationships}
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  $\omega{\scriptstyle_{pi}}$ $=$ ($c$ $\Omega{\scriptstyle_{ci}}$)/V${\scriptstyle_{A}}$
    \item  $\omega{\scriptstyle_{pe}}$ $=$ ($c$ $\Omega{\scriptstyle_{ce}}$)/V${\scriptstyle_{Ae}}$
  \end{enumerate}
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Terminology and Jargon
%%----------------------------------------------------------------------------------------
\phantomsection   %%  Fix reference link
\subsection{Terminology and Jargon}  \label{subapp:TerminologyandJargon}
\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Phase Front:}  plane of constant phase
  \item  \textbf{V}${\scriptstyle_{ph}}$ $\equiv$ phase velocity
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  the velocity associated with a fixed value of phase $\Rightarrow$ representing an advance of position, \textbf{r}, with t \citep[see page 233 of][]{french71a}
  \end{enumerate}
  \item  \textbf{Dispersive:}  a medium where the phase speed of a wave depends upon the frequency of the wave \citep[see page 398 of][]{griffiths99a}
  \item  \textbf{V}${\scriptstyle_{gr}}$ $\equiv$ group velocity
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  the velocity associated with a modulated envelope, which encloses a group of phase fronts (or short waves)
    \item  so long as the wave source is slowly varying, constructive interference will maximize where the phase is stationary (i.e., where d\textbf{r}/dt $=$ $\partial \omega$/$\partial$\textbf{k}) $\Rightarrow$ the locus of points satisfying this condition define the group velocity \citep[see page 76 of][]{stix92a}
    \item  is perpendicular to a contour of constant $\omega$ in \textbf{k}-space \citep[pages 82-83 of][]{gurnett05}
  \end{enumerate}
  \item  \textbf{Index of Refraction:}  a dimensionless vector that has the direction of \textbf{k} and the magnitude of c/\textbf{V}${\scriptstyle_{ph}}$ (defined as \textbf{n})
  \item  \textbf{Wave Normal Surface:}  the locus of \textbf{V}${\scriptstyle_{ph}}$ azimuthally revolved around \textbf{B}${\scriptstyle_{o}}$ with a 2D cross-section shown as a polar plot of $\omega$/k vs. $\theta$ ($=$ angle between \textbf{k} and \textbf{B}${\scriptstyle_{o}}$)
  \begin{enumerate}
    \setlength{\itemsep}{0pt}    %% tighten up spacing between items
    \setlength{\parskip}{0pt}    %% tighten up spacing between items
    \item  formed by plotting u ($=$ $\omega$/kc $=$ 1/n) vs. $\theta$
    \item  formed by the locus of the tip of the vector \textbf{n}$^{-1}$ $\equiv$ \textbf{n}/n$^{2}$
  \end{enumerate}
\end{enumerate}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Maxwell Equations
%%----------------------------------------------------------------------------------------
\section{Maxwell Equations}  \label{app:maxwell}

\indent  We start with the Maxwell equations:

\begin{subequations}
  \begin{align}
    \nabla \cdot \mathbf{E} & = \frac{ \rho{\scriptstyle_{c}} }{\varepsilon{\scriptstyle_{o}}}  \label{eq:maxwell_0}  \\
    \nabla \cdot \mathbf{B} & = 0  \label{eq:maxwell_1}  \\
    \nabla \times \mathbf{E} + \frac{ \partial \mathbf{B} }{ \partial t } & = 0  \label{eq:maxwell_2}  \\
    \nabla \times \mathbf{B} & = \mu{\scriptstyle_{o}} \mathbf{j} + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t }  \label{eq:maxwell_3}
  \end{align}
\end{subequations}

\noindent  Since we know from vector calculus that the divergence of the curl of a vector is zero, then from Equation \ref{eq:maxwell_1} we can define the vector potential as:

\begin{equation}
  \label{eq:maxwell_4}
  \mathbf{B} = \nabla \times \mathbf{A}
\end{equation}

\noindent  which we then substitute into Equation \ref{eq:maxwell_2} to get:

\begin{equation}
  \label{eq:maxwell_5}
  \nabla \times \left[ \mathbf{E} + \frac{ \partial \mathbf{A} }{ \partial t } \right] = 0 \text{  .}
\end{equation}

\noindent  From vector calculus, we know that the curl of the gradient of a scalar function is zero, thus we can say:

\begin{equation}
  \label{eq:maxwell_6}
  \mathbf{E} = - \nabla \Phi - \frac{ \partial \mathbf{A} }{ \partial t }
\end{equation}

\noindent  where the negative sign is chosen due to the physical relationship between potentials and forces.  The arbitrariness in the choice of $\mathbf{A}$ and $\Phi$ suggest that the following two operations would not affect the electric or magnetic fields:

\begin{subequations}
  \begin{align}
    \mathbf{A} \rightarrow \mathbf{A}' & = \mathbf{A} + \nabla \Lambda  \label{eq:maxwell_7}  \\
    \Phi \rightarrow \Phi' & = \Phi - \frac{ \partial \Lambda }{ \partial t }  \label{eq:maxwell_8}
  \end{align}
\end{subequations}

\noindent  which means we can choose any ($\mathbf{A}$, $\Phi$) such that they satisfy:

\begin{equation}
  \label{eq:maxwell_9}
  \nabla \cdot \mathbf{A} + \frac{1}{c^{2}} \frac{ \partial \Phi }{ \partial t } = 0 \text{  .}
\end{equation}

\noindent  This allows us to rewrite Equations \ref{eq:maxwell_0} and \ref{eq:maxwell_3} as:

\begin{subequations}
  \begin{align}
    \nabla^{2} \Phi - \frac{1}{c^{2}} \frac{ \partial^{2} \Phi }{ \partial t^{2} } & = - \frac{\rho}{\varepsilon{\scriptstyle_{o}}}  \label{eq:maxwell_10} \\
    \nabla^{2} \mathbf{A} - \frac{1}{c^{2}} \frac{ \partial^{2} \mathbf{A} }{ \partial t^{2} } & = - \mu{\scriptstyle_{o}} \mathbf{j}  \label{eq:maxwell_11}  \text{  .}
  \end{align}
\end{subequations}

\noindent  For more information, see Chapter 6 of \citet[][]{jackson98a}.

%%----------------------------------------------------------------------------------------
%%  Appendix:  Poynting's Theorem
%%----------------------------------------------------------------------------------------
\subsection{Poynting's Theorem} \label{subapp:poynting}

\indent  We can define the, if there exists a continuous distribution of charge and current, the total rate of work done by electromagnetic fields in a volume by:

\begin{equation}
  \label{eq:poynting_0}
  \frac{ d W{\scriptstyle_{EM}} }{ dt } = \int_{V} \thickspace d^{3}x \thickspace \mathbf{j} \cdot \mathbf{E}  \text{  .}
\end{equation}

\noindent  This power represents the rate at which electromagnetic field energy is converted into mechanical or thermal energy.  To conserve energy, the increase in mechanical or thermal energy must be balanced by a decrease in electromagnetic field energy.  We can change the form of this equation and represent it in terms of fields only by using Equation \ref{eq:maxwell_3} to replace $\mathbf{j}$, which results in:

\begin{subequations}
  \begin{align}
    \frac{ d W{\scriptstyle_{EM}} }{ dt } & = \int_{V} \thickspace d^{3}x \thickspace \left[ \frac{1}{\mu{\scriptstyle_{o}}} \nabla \times \mathbf{B} - \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t } \right] \cdot \mathbf{E}  \label{eq:poynting_1a}  \\
    & = \int_{V} \thickspace d^{3}x \thickspace \left[ \frac{1}{\mu{\scriptstyle_{o}}} \mathbf{E} \cdot \left( \nabla \times \mathbf{B} \right) - \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right]  \label{eq:poynting_1b} \\
    & = \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \frac{1}{\mu{\scriptstyle_{o}}} \left[ \mathbf{B} \cdot \left( \nabla \times \mathbf{E} \right) - \nabla \cdot \left( \mathbf{E} \times \mathbf{B} \right) \right] - \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right\rbrace  \label{eq:poynting_1c} \\
    & = - \frac{1}{\mu{\scriptstyle_{o}}} \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \left[ \mathbf{B} \cdot \frac{ \partial \mathbf{B} }{ \partial t } + \nabla \cdot \left( \mathbf{E} \times \mathbf{B} \right) \right] + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \frac{ \partial \mathbf{E} }{ \partial t } \right\rbrace  \label{eq:poynting_1d} \\
    & = - \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \nabla \cdot \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) + \frac{ \partial }{ \partial t } \left[ \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } + \frac{ \varepsilon{\scriptstyle_{o}} \left( \mathbf{E} \cdot \mathbf{E} \right) }{ 2 } \right] \right\rbrace  \label{eq:poynting_1e} \\
    - \mathbf{j} \cdot \mathbf{E} & = \nabla \cdot \mathbf{S} + \frac{ \partial }{ \partial t } \left( \mathcal{W}{\scriptstyle_{B}} + \mathcal{W}{\scriptstyle_{E}} \right)  \label{eq:poynting_1f}
%%    & = \int_{V} \thickspace d^{3}x \thickspace \left[  \right]
  \end{align}
\end{subequations}

\noindent  where we have used $\nabla \cdot ( \mathbf{A} \times \mathbf{B})$ $=$ $\mathbf{B} \cdot ( \nabla \times \mathbf{A} )$ - $\mathbf{A} \cdot ( \nabla \times \mathbf{B} )$, Equation \ref{eq:maxwell_2}, and $\mathbf{S}$ is the Poynting flux.  $\mathbf{S}$ is the flow of electromagnetic field energy per unit area per unit time.  Since ($\mathbf{j} \cdot \mathbf{E}$) is a rate of work to convert electromagnetic field energy into a mechanical energy, $E{\scriptstyle_{mech}}$, we can relate this to Newton's 2nd law through the following:

\begin{equation}
  \label{eq:poynting_2}
  \frac{ d \textbf{P}{\scriptstyle_{mech}} }{ dt } = \int_{V} \thickspace d^{3}x \thickspace \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right]
\end{equation}

\noindent  where $\rho{\scriptstyle_{c}}$ is the charge density and \textbf{P}${\scriptstyle_{mech}}$ is the total momenta of all particles in the volume.  Using Equations \ref{eq:maxwell_0} and \ref{eq:maxwell_3}, we can change the integrand to the following:

\begin{subequations}
  \begin{align}
    \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right] & = \varepsilon{\scriptstyle_{o}} \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + \frac{1}{\mu{\scriptstyle_{o}}} \left( \nabla \times \mathbf{B} \right) \times \mathbf{B} - \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t } \times \mathbf{B}  \label{eq:poynting_3a}  \\
    & = \varepsilon{\scriptstyle_{o}} \left[ \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + \mathbf{B} \times \frac{ \partial \mathbf{E} }{ \partial t } - c^{2} \mathbf{B} \times \left( \nabla \times \mathbf{B} \right) \right]  \text{  .}  \label{eq:poynting_3b}
  \end{align}
\end{subequations}

\noindent  We can manipulate this further by using the following:

\begin{subequations}
  \begin{align}
    \mathbf{B} \times \frac{ \partial \mathbf{E} }{ \partial t } & = - \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right) + \mathbf{E} \times \frac{ \partial \mathbf{B} }{ \partial t }  \label{eq:poynting_4a}  \\
    & = - \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right) - \mathbf{E} \times \left( \nabla \times \mathbf{E} \right)  \text{  .}  \label{eq:poynting_4b}
  \end{align}
\end{subequations}

\noindent  Now we can take this result and use $c^{2} \mathbf{B} ( \nabla \cdot \mathbf{B} )$ $=$ 0 to change the terms in brackets to:

\begin{equation}
  \label{eq:poynting_5}
  \begin{split}
    \left[ \rho{\scriptstyle_{c}} \mathbf{E} + \mathbf{j} \times \mathbf{B} \right] & = \\
    & \varepsilon{\scriptstyle_{o}} \left\lbrace \left[ \mathbf{E} \left( \nabla \cdot \mathbf{E} \right) + c^{2} \mathbf{B} \left( \nabla \cdot \mathbf{B} \right) \right] - \left[ \mathbf{E} \times \left( \nabla \times \mathbf{E} \right) + c^{2} \mathbf{B} \times \left( \nabla \times \mathbf{B} \right) \right] \right\rbrace \\
    & - \varepsilon{\scriptstyle_{o}} \frac{ \partial }{ \partial t } \left( \mathbf{E} \times \mathbf{B} \right)  \text{  .}
  \end{split}
\end{equation}

\noindent  We can simplify this slightly by recognizing the following rule about the divergence of a 2nd rank tensor:

\begin{equation}
  \label{eq:poynting_6}
  \left[ \mathbf{A} \left( \nabla \cdot \mathbf{A} \right) - \mathbf{A} \times \left( \nabla \times \mathbf{A} \right) \right]{\scriptstyle_{\alpha}} = \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left[ A{\scriptstyle_{\alpha}} A{\scriptstyle_{\beta}} - \frac{ \mathbf{A} \cdot \mathbf{A} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right]
\end{equation}

\noindent  where we have used tensor notation\footnote{Meaning, $\partial / \partial x^{\beta}$ $\equiv$ $\partial{\scriptstyle_{\beta}}$ $=$ $\left\lbrace \partial/\partial x^{0}, \nabla \right\rbrace$, where $x^{\beta}$ $\equiv$ \emph{contravariant vector} (or rank one tensor) and $x{\scriptstyle_{\beta}}$ $\equiv$ \emph{covariant vector} \citep[e.g., see Chapter 11 of][]{jackson98a}.}, therefore, we can rewrite Equation \ref{eq:poynting_2} as:

\begin{equation}
  \label{eq:poynting_7}
  \begin{split}
    \frac{ d P{\scriptstyle_{mech, \alpha}} }{ dt } & + \frac{ d }{ dt } \int_{V} \thickspace d^{3}x \thickspace \varepsilon{\scriptstyle_{o}} \left( \mathbf{E} \times \mathbf{B} \right){\scriptstyle_{\alpha}} = \\
    & \varepsilon{\scriptstyle_{o}} \int_{V} \thickspace d^{3}x \thickspace \left\lbrace \left[ \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left( E{\scriptstyle_{\alpha}} E{\scriptstyle_{\beta}} - \frac{ \mathbf{E} \cdot \mathbf{E} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right) \right] + \left[ c^{2} \sum_{\beta} \frac{ \partial }{ \partial x^{\beta} } \left( B{\scriptstyle_{\alpha}} B{\scriptstyle_{\beta}} - \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 } \delta{\scriptstyle_{\alpha \beta}} \right) \right] \right\rbrace  \text{  .}
  \end{split}
\end{equation}

\noindent  At this point, we can define the \emph{Maxwell stress tensor}, $T{\scriptstyle_{\alpha \beta}}$, as:

\begin{equation}
  \label{eq:poynting_8}
  T{\scriptstyle_{\alpha \beta}} = \varepsilon{\scriptstyle_{o}} \left[ \left( E{\scriptstyle_{\alpha}} E{\scriptstyle_{\beta}} + B{\scriptstyle_{\alpha}} B{\scriptstyle_{\beta}} \right) - \frac{1}{2} \left( \mathbf{E} \cdot \mathbf{E} + c^{2} \mathbf{B} \cdot \mathbf{B} \right) \delta{\scriptstyle_{\alpha \beta}} \right]
\end{equation}

\noindent  which shows us that Equation \ref{eq:poynting_7} can be rewritten, in component form, as:

\begin{subequations}
  \begin{align}
    \frac{ d }{ dt } \left( \textbf{P}{\scriptstyle_{mech}} + \textbf{P}{\scriptstyle_{EM}} \right){\scriptstyle_{\alpha}} & = \sum_{\beta} \thickspace \int_{V} \thickspace d^{3}x \thickspace \frac{ \partial }{ \partial x^{\beta} } T{\scriptstyle_{\alpha \beta}}  \label{eq:poynting_9a}  \\
    & = \oint_{S} \thickspace dA \thickspace \sum_{\beta} T{\scriptstyle_{\alpha \beta}} \thickspace n{\scriptstyle_{\beta}}  \text{  .}  \label{eq:poynting_9b}
  \end{align}
\end{subequations}

\noindent  We should note that $T{\scriptstyle_{\alpha \beta}}$ is not the rank two antisymmetric field-strength tensor $F^{\alpha \beta}$ $=$ $\partial^{\alpha} A^{\beta}$ - $\partial^{\beta} A^{\alpha}$ (where $A^{\alpha}$ is the 4-vector electromagnetic potentials) and $\partial^{\beta} A^{\lambda}$ $=$ -$F^{\lambda \beta}$ + $\partial^{\lambda} A^{\beta}$, but the two are related through:

\begin{equation}
  \label{eq:poynting_10}
  T^{\alpha \beta} = \frac{1}{4 \pi} \left\lbrace \left[ g^{\alpha \mu} F{\scriptstyle_{\mu \lambda}} F^{\lambda \beta} + \frac{1}{4} g^{\alpha \beta} F{\scriptstyle_{\mu \nu}} F^{\mu \nu} \right] - g^{\alpha \mu} F{\scriptstyle_{\mu \lambda}} \partial^{\lambda} A^{\beta} \right\rbrace  \text{  .}
\end{equation}

\noindent  For more information, see Chapters 6, 11, and 12 of \citet[][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Lorentz Transformation
%%----------------------------------------------------------------------------------------
\subsection{Lorentz Transformation}  \label{subsec:lorentz}

\indent  The general Lorentz transformation for electromagnetic fields is derived from the properties of the second-rank antisymmetric field-strength tensor given by:

\begin{equation}
  \label{eq:lorentz_0}
  F^{\alpha \beta} = \partial^{\alpha} A^{\beta} - \partial^{\beta} A^{\alpha}
\end{equation}

\noindent  where $A^{\alpha}$ [$=$ ($\Phi$,$\mathbf{A}$)] is the scalar and vector potentials.  The covariant form of the inhomogeneous Maxwell's equations are given by:

\begin{equation}
  \label{eq:lorentz_1}
  \partial{\scriptstyle_{\alpha}} F^{\alpha \beta} = \frac{4 \pi}{c} J^{\beta}
\end{equation}

\noindent  where $J^{\alpha}$ [$=$ (c$\rho$,$\mathbf{J}$)] is the 4-vector current density.  The covariant form of the homogeneous Maxwell's equations are given by:

\begin{subequations}
  \begin{align}
    \partial{\scriptstyle_{\alpha}} \mathcal{F}^{\alpha \beta} & = 0  \label{eq:lorentz_2a}  \\
    \mathcal{F}^{\alpha \beta} & = \frac{1}{2} \epsilon^{\alpha \beta \gamma \delta} F{\scriptstyle_{\gamma \delta}}  \label{eq:lorentz_2b}
  \end{align}
\end{subequations}

\noindent  where $\epsilon^{\alpha \beta \gamma \delta}$ is defined on page 556 of \citet{jackson98a}.  Note that the Lorentz force can be written in its covariant form as:

\begin{equation}
  \label{eq:lorentz_3}
  \frac{ d p^{\alpha} }{ d \tau } = m \frac{ d U^{\alpha} }{ d \tau } = \frac{ q }{ c } F^{\alpha \beta} U{\scriptstyle_{\beta}}
\end{equation}

\noindent  which means that the equations of motion are given by:

\begin{subequations}
  \begin{align}
    \frac{ 1 }{ c } \frac{ d \mathcal{E} }{ d \tau } & = \frac{ q }{ c } \mathbf{U} \cdot \mathbf{E}  \label{eq:lorentz_4a}  \\
    \frac{ d \mathbf{p} }{ d \tau } & = \frac{ q }{ c } \left( \frac{ \mathcal{E} }{ m c } \mathbf{E} + \mathbf{U} \times \mathbf{B} \right)  \label{eq:lorentz_4b}
  \end{align}
\end{subequations}

\noindent  where $\mathcal{E}$ is the scalar energy, ($U{\scriptstyle_{o}}$,$\mathbf{U}$) is the 4-vector velocity [$U{\scriptstyle_{o}}$ $=$ $\mathcal{E}$/mc], ($p{\scriptstyle_{o}}$,$\mathbf{p}$) is the 4-vector momentum [$=$ m ($U{\scriptstyle_{o}}$,$\mathbf{U}$)], and $\mathbf{E}$ and $\mathbf{B}$ are the electric and magnetic fields.  Because $\mathbf{E}$ and $\mathbf{B}$ are elements of the second-rank tensor F$^{\alpha \beta}$, their values can be expressed in different reference frames in terms of their values in other reference frames \citep[see page 558 of][]{jackson98a}.  Mathematically, this is expressed as:

\begin{equation}
  \label{eq:lorentz_5}
  F'^{\alpha \beta} = \frac{ \partial x'^{\alpha} }{ \partial x^{\gamma} } \frac{ \partial x'^{\beta} }{ \partial x^{\delta} } F^{\gamma \delta}  \text{  .}
\end{equation}

\noindent  For a general Lorentz transformation, $\mathbf{E}$ and $\mathbf{B}$ are transformed as:

\begin{subequations}
  \begin{align}
    \mathbf{E}' & = \gamma \left( \mathbf{E} + \boldsymbol{\beta} \times \mathbf{B} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{E} \right)  \label{eq:lorentz_6a}  \\
    \mathbf{B}' & = \gamma \left( \mathbf{B} - \boldsymbol{\beta} \times \mathbf{E} \right) - \frac{ \gamma^{2} }{ \gamma + 1 } \boldsymbol{\beta} \left( \boldsymbol{\beta} \cdot \mathbf{B} \right)  \label{eq:lorentz_6b}
  \end{align}
\end{subequations}

\noindent  where $\boldsymbol{\beta}$ $=$ $\mathbf{v}$/c \citep[see page 558 of][]{jackson98a}.  As an aside, if we have $\mathbf{J}$' $=$ $\sigma$ $\mathbf{E}$' (Ohm's law), where primes denote the rest frame, then the covariant generalization of Ohm's law is written as:

\begin{equation}
  \label{eq:lorentz_7}
  J^{\alpha} - \frac{ 1 }{ c^{2} } \left( U{\scriptstyle_{\beta}} J^{\beta} \right) U^{\alpha} = \frac{ \sigma }{ c } F^{\alpha \beta} U{\scriptstyle_{\beta}}
\end{equation}

\noindent  where we have allowed for the possibility of convection currents and conduction currents \citep[see problem 11.16 of][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Wave Properties
%%----------------------------------------------------------------------------------------
\section{Wave Properties} \label{app:waveprops}

\noindent  Before we begin, we should define some terms and parameters/functions that will be used later:

\begin{enumerate}
  \setlength{\itemsep}{0pt}    %% tighten up spacing between items
  \setlength{\parskip}{0pt}    %% tighten up spacing between items
  \item  \textbf{Wave Number:}  $\equiv$ effectively the number of wave crests (i.e., anti-node of local maximum) per unit length $\leftrightharpoons$ ``density'' of waves $\rightarrow$ $\mathbf{k}$ $=$ $\mathbf{k} \left(\omega,\mathbf{x},t\right)$ in general (sometimes denoted by $\boldsymbol{\kappa}$ as in \citet[][]{whitham99a})
  \item  \textbf{Wave Frequency:}  $\equiv$ effectively the number of wave crests crossing position $\mathbf{x}$ per unit time $\leftrightharpoons$ ``flux'' of waves $\rightarrow$ $\omega$ $=$ $\omega\left(\mathbf{k},\mathbf{x},t\right)$ in general
  \item  \textbf{Wave Phase:}  $\equiv$ position on a wave cycle between a crest and a trough (i.e., anti-node of local minimum) $\rightarrow$ $\phi$ $=$ $\phi\left(\mathbf{x},t\right)$ in general
  \item  \textbf{Dispersive Wave:}  $\equiv$ a propagating fluctuation where the wave frequency, $\omega\left(\mathbf{k},\mathbf{x},t\right)$, is nonlinearly dependent upon the wave number, $\mathbf{k} \left(\omega,\mathbf{x},t\right)$ $\Rightarrow$ modes with different $\mathbf{k}$ will propagate at different speeds $\Rightarrow$ modes will spread out spatially $=$ \emph{disperse}.
  \item  \textbf{Dispersion Relation:}  $\equiv$ the mathematical dependence of $\omega$ on $\mathbf{k}$ (or vice versa) $\leftrightharpoons$ mathematical relationship between $\omega$ and $\mathbf{k}$
  \item  \textbf{[Wave] Mode:}  $\equiv$ a general solution to a dispersion relation\footnote{There can be multiple \emph{modes} for one dispersion relation}
\end{enumerate}

\noindent  We can define an elementary solution to periodic wave equations as:

\begin{equation}
  \label{eq:waveprops_0a}
  \psi\left( \mathbf{x}, t \right) = \mathcal{A} \thickspace e^{ {\displaystyle i\left( \boldsymbol{\kappa} \cdot \mathbf{x} - \omega t \right) } }
\end{equation}

\noindent  where $\mathcal{A}$ is the wave amplitude and, in general, can be a function of $\boldsymbol{\kappa}$ and/or $\omega$, but we will assume constant for now.  Let us assume that a \emph{dispersion relation}, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right)$, exists and may be solved for positive real roots.  In general, there will be multiple solutions to the dispersion relation, where each solution is referred to as different \emph{modes}.  The term in the exponent is known as the \emph{wave phase}, given by:

\begin{equation}
  \label{eq:waveprops_1a}
  \phi\left( \mathbf{x}, t \right) = \boldsymbol{\kappa}\left( \omega, \mathbf{x}, t \right) \cdot \mathbf{x}  -  \omega\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) \thickspace t + \phi{\scriptstyle_{o}}
\end{equation}

\noindent  where we have used a general form for the frequency, $\omega$, and wave number, $\boldsymbol{\kappa}$.  Recall that the Fourier transform of an arbitrary function, $f(\mathbf{x},t)$, in four-dimensions is given by:

\begin{equation}
  \label{eq:waveprops_2a}
  f\left( \mathbf{x}, t \right) = \frac{1}{\left( 2 \pi \right)^{2}} \int_{-\infty}^{\infty} \thickspace d^{3}\kappa \thickspace d\omega \thickspace \tilde{f}\left( \boldsymbol{\kappa}, \omega \right) e^{ {\displaystyle i\left( \boldsymbol{\kappa} \cdot \mathbf{x} - \omega t \right) } }
\end{equation}

\noindent  where $\mathbf{x}(t)$ is an arbitrary position on a plane of constant phase at time $t$.  In other words, ``$\mathbf{x}$ represents the point of maximum constructive interference at time $t$ for a wave packet centered, in Fourier space, on $\boldsymbol{\kappa}$ and $\omega$'' \citep[page 71 of][]{stix92a}.  Because $\phi\left(\mathbf{x},t\right)$ results from solutions of the wave equation, its derivatives must satisfy the dispersion relation through the following:

\begin{equation}
  \label{eq:waveprops_3a}
  - \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial t } = \mathcal{W}\left( \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial \mathbf{x} }, \mathbf{x}, t \right)
\end{equation}

\noindent  and we can see from Equation \ref{eq:waveprops_1a} that the following is true:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial \mathbf{x} }  \label{eq:waveprops_4a}  \\
    \omega & = - \frac{ \partial \phi\left( \mathbf{x}, t \right) }{ \partial t }  \text{   .}  \label{eq:waveprops_4b}
  \end{align}
\end{subequations}

\noindent  We also know that $\partial^{2} \phi / \partial \mathbf{x} \partial t$ $=$ $\partial^{2} \phi / \partial t \partial \mathbf{x}$ \citep[page 119 of][]{kulsrud05b}, therefore:

\begin{subequations}
  \begin{align}
    \frac{ \partial^{2} \phi }{ \partial t \partial \mathbf{x} } - \frac{ \partial^{2} \phi }{ \partial \mathbf{x} \partial t } & = 0  \label{eq:waveprops_5a}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } - \frac{ - \partial \omega }{ \partial \mathbf{x} } = 0  \label{eq:waveprops_5b}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \mathbf{x} } = 0  \label{eq:waveprops_5c}  \\
    & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \nabla \omega = 0  \text{   .}  \label{eq:waveprops_5d}
  \end{align}
\end{subequations}

\noindent  One can see that Equation \ref{eq:waveprops_5d} looks similar to the \emph{continuity equation}, so long as $\boldsymbol{\kappa}$ $\leftrightharpoons$ ``density'' of the waves, and $\omega$ $\leftrightharpoons$ ``flux'' of the waves.  \\
\indent  From the above relations, we can see that on \emph{contours} of constant $\phi\left(\mathbf{x},t\right)$, we are sitting on local wave crests (i.e., \emph{phase fronts}) where $\boldsymbol{\kappa}$ is orthogonal to these \emph{contours}.  These phase fronts move parallel to $\boldsymbol{\kappa}$ at a speed, $\mathbf{V}{\scriptstyle_{\phi}}$, known as the \emph{phase velocity}.  The general form for this speed is given by:

\begin{equation}
  \label{eq:waveprops_6a}
  \mathbf{V}{\scriptstyle_{\phi}} \equiv \frac{ \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \kappa } \boldsymbol{\hat{\kappa}}  \text{   .}
\end{equation}

\noindent  If we multiply the 2nd term in Equation \ref{eq:waveprops_5c} by unity, we get:

\begin{subequations}
  \begin{align}
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \mathbf{x} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \boldsymbol{\kappa} } & = 0  \label{eq:waveprops_7a}  \\
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } & = 0  \label{eq:waveprops_7b}  \\
    \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \left( \mathbf{V}{\scriptstyle_{g}} \cdot \nabla \right) \boldsymbol{\kappa} & = 0  \label{eq:waveprops_7c}
  \end{align}
\end{subequations}

\noindent  where $\mathbf{V}{\scriptstyle_{g}}$ is called the \emph{group velocity}, where we note that:

\begin{equation}
  \label{eq:waveprops_8a}
  \frac{ \partial \omega }{ \partial \mathbf{x} } = \frac{ \partial \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \mathbf{x} }
\end{equation}

\noindent  which shows that $\partial \mathcal{W}$/$\partial \boldsymbol{\kappa}$ $=$ $\left( \partial \omega / \partial \boldsymbol{\kappa} \right){\scriptstyle_{\mathbf{x}}}$ $\Rightarrow$ \emph{different $\boldsymbol{\kappa}$'s propagate with velocity $\mathbf{V}{\scriptstyle_{g}}$} \citep[page 376 of][]{whitham99a}.  In other words, \emph{$\mathbf{V}{\scriptstyle_{g}}$ is the propagation velocity for $\kappa$} \citep[page 380 of][]{whitham99a} and \emph{$\mid$$\mathcal{A}$$\mid^{2}$ propagates with velocity $\mathbf{V}{\scriptstyle_{g}}$} \citep[page 379 of][]{whitham99a}.

\indent  We wish to define the term \emph{dispersive} more appropriately, so we require the following constraints:

\begin{subequations}
  \begin{align}
    \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) & = \text{ real and } \neq 0  \label{eq:waveprops_9a}  \\
    \frac{ \partial^{2} \thickspace \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \kappa^{2} } & \neq 0  \label{eq:waveprops_9b}
  \end{align}
\end{subequations}

\noindent  and finally:

\begin{equation}
  \label{eq:waveprops_10a}
  \det \left\lvert \frac{ \partial^{2} \thickspace \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) }{ \partial \kappa{\scriptstyle_{i}} \thickspace \partial \kappa{\scriptstyle_{j}} } \right\rvert \neq 0  \text{   .}
\end{equation}

\noindent  Thus, an observer moving with the phase fronts (crests) moves at $\mathbf{V}{\scriptstyle_{\phi}}$, but they observe the local wave number and frequency to change in time $\Rightarrow$ neighboring phase fronts (crests) move away from the observer in this frame.  In contrast, for an observer moving with $\mathbf{V}{\scriptstyle_{g}}$, they observe constant local wave number and frequency (with respect to time), but phase fronts (crests) continuously move past the observer in this frame \citep[page 377 of][]{whitham99a}.

%%----------------------------------------------------------------------------------------
%%  Appendix:  Inhomogeneous Media
%%----------------------------------------------------------------------------------------
\subsection{Inhomogeneous Media} \label{subapp:inhomogeneous}

\indent  Recall that for an arbitrary function, $\mathcal{F}$ $=$ $\mathcal{F} \left( t, x{\scriptstyle_{1}}, x{\scriptstyle_{2}}, ..., x{\scriptstyle_{n - 1}}, x{\scriptstyle_{n}} \right)$, the \emph{exact derivative} or \emph{total derivative} is given by:

\begin{equation}
  \label{eq:inhomogeneous_0a}
  \frac{ d \mathcal{F} }{ dt } = \frac{ \partial \mathcal{F} }{ \partial t } + \sum_{i = 1}^{n} \frac{ \partial \mathcal{F} }{ \partial x{\scriptstyle_{i}} } \thickspace \frac{ d x{\scriptstyle_{i}} }{ dt }  \text{   .}
\end{equation}

\indent  Let us start with the assumption of a lossless dispersion relation, $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right)$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right)$ $=$ 0, where $\mathcal{W}$ $=$ 0 for all points along a trajectory following $\mathbf{V}{\scriptstyle_{g}}$.  The variation of the dispersion relation, $\delta \mathcal{W}$, is given by:

\begin{equation}
  \label{eq:inhomogeneous_0b}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \omega, \mathbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \cdot \delta \mathbf{x} + \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \omega + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa} = 0
\end{equation}

\noindent  which we require to $=$ 0 as well\footnote{this is actually the constraint that makes $\mathcal{W}$ $\rightarrow$ 0 for all points along the $\mathbf{V}{\scriptstyle_{g}}$-trajectory}.  Now if we let $\tau$ be a measure of distance along this trajectory and we vary the parameters with respect to $\tau$ (e.g., $\delta \boldsymbol{\kappa}$ $\rightarrow$ $d\boldsymbol{\kappa} / d\tau$ $\delta \tau$), then we find the following relations that cause the terms in Equation \ref{eq:inhomogeneous_0b} to cancel accordingly:

\begin{subequations}
  \begin{align}
    \frac{ d \mathbf{x} }{ d \tau } & = \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} }  \label{eq:inhomogeneous_1a}  \\
    \frac{ d \boldsymbol{\kappa} }{ d \tau } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_1b}  \\
    \frac{ d t }{ d \tau } & = - \frac{ \partial \mathcal{W} }{ \partial \omega }  \label{eq:inhomogeneous_1c}  \\
    \frac{ d \omega }{ d \tau } & = \frac{ \partial \mathcal{W} }{ \partial t }  \label{eq:inhomogeneous_1d}  \\
    \intertext{therefore,}
    \delta \mathbf{x} & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \delta \tau  \label{eq:inhomogeneous_1e}  \\
    \delta \boldsymbol{\kappa} & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \delta \tau  \label{eq:inhomogeneous_1f}  \\
    \delta t & \rightarrow \frac{ \partial \mathcal{W} }{ \partial \omega } \delta \tau  \label{eq:inhomogeneous_1g}  \\
    \delta \omega & \rightarrow \frac{ \partial \mathcal{W} }{ \partial t } \delta \tau  \label{eq:inhomogeneous_1h}
  \end{align}
\end{subequations}

\noindent  which shows us that the 2nd and 4th terms and 1st and 3rd terms in Equation \ref{eq:inhomogeneous_0b} cancel, respectively \citep[Chapter 4.7 of][]{stix92a}.  Combining Equations \ref{eq:inhomogeneous_1a} and \ref{eq:inhomogeneous_1c}, we find:

\begin{equation}
  \label{eq:inhomogeneous_1i}
  \frac{ d \mathbf{x} }{ d t } = - \frac{ \partial \mathcal{W}/\partial \boldsymbol{\kappa} }{ \partial \mathcal{W}/\partial \omega } = \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} } \equiv \mathbf{V}{\scriptstyle_{g}}  \text{   .}
\end{equation}

\indent  Now let us consider the dispersion relation, $\omega$ $=$ $\mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right)$, then variation can be shown to be:

\begin{equation}
  \label{eq:inhomogeneous_2a}
  \delta \mathcal{W}\left( \boldsymbol{\kappa}, \mathbf{x}, t \right) = \frac{ \partial \mathcal{W} }{ \partial t } \delta t + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \cdot \delta \mathbf{x} + \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \delta \boldsymbol{\kappa} = 0
\end{equation}

\noindent  which gives us the following relationships:

\begin{subequations}
  \begin{align}
    \frac{ d \mathbf{x} }{ d t } & = \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } = \frac{ \partial \omega }{ \partial \boldsymbol{\kappa} }  \label{eq:inhomogeneous_3a}  \\
    \frac{ d \boldsymbol{\kappa} }{ d t } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } = - \frac{ \partial \omega }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_3b}  \\
    \frac{ d \omega }{ d t } & = \frac{ \partial \mathcal{W} }{ \partial t } = \frac{ \partial \omega }{ \partial t }  \text{   .}  \label{eq:inhomogeneous_3c}
  \end{align}
\end{subequations}

\indent  We can show this by considering that the total derivative of $\boldsymbol{\kappa}$ is given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } dt + \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } \cdot d \mathbf{x}  \notag \\
    \frac{ d \boldsymbol{\kappa} }{ dt } & = \frac{ \partial \boldsymbol{\kappa} }{ \partial t } + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99a}  \\
    & = - {\displaystyle \left( \frac{ \partial \omega }{ \partial \mathbf{x} } \right)_{{\scriptstyle_{t}}} } + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99b}  \\
    & = - \left\{ \frac{ \partial \mathcal{W} }{ \partial \boldsymbol{\kappa} } \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \right\} + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99c}  \\
    & = - \left\{ \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} } + \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \right\} + \mathbf{V}{\scriptstyle_{g}} \cdot \frac{ \partial \boldsymbol{\kappa} }{ \partial \mathbf{x} }  \label{eq:inhomogeneous_99d}  \\
    \intertext{therefore,}
    \frac{ d \boldsymbol{\kappa} }{ dt } & = - \frac{ \partial \mathcal{W} }{ \partial \mathbf{x} } \equiv - \left( \frac{ \partial \omega }{ \partial \mathbf{x} } \right)_{{\scriptstyle_{\boldsymbol{\kappa}}}}  \label{eq:inhomogeneous_99e}
  \end{align}
\end{subequations}

\noindent  where the notation $\left(  \right){\scriptstyle_{\alpha}}$ considers the expression within the parentheses a constant with respect to $\alpha$.  Equation \ref{eq:inhomogeneous_99e} is known as the \emph{wave normal equation}, or sometimes as the \emph{eikonal equation} \citep[Chapter 5.6 of][]{kulsrud05b}.  This equation defines the total rate of change of $\boldsymbol{\kappa}$ for a wave packet while following the wave packet\footnote{this is also the wave packet's \emph{refraction}}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Anisotropic Media
%%----------------------------------------------------------------------------------------
\subsection{Anisotropic Media} \label{subapp:AnisotropicMedia}

In anisotropic media, the angle between $\mathbf{V}{\scriptstyle_{gr}}$ and $\mathbf{V}{\scriptstyle_{ph}}$, $\alpha$, is given by:

\begin{subequations}
  \begin{align}
    \tan{\alpha} & = \frac{{\displaystyle \frac{ 1 }{ k } \left(\frac{ \partial \omega }{ \partial \theta }\right)_{k} }}{{\displaystyle \left(\frac{ \partial \omega }{ \partial k }\right)_{\theta} }}  \label{eq:definition_9a}  \\
    & = \frac{{\displaystyle \frac{ 1 }{ n } \left(\frac{ \partial \omega }{ \partial \theta }\right)_{n} }}{{\displaystyle \left(\frac{ \partial \omega }{ \partial n }\right)_{\theta} }}  \label{eq:definition_9b}  \\
    & = - \frac{ 1 }{ n } \left( \frac{ \partial n }{ \partial \theta } \right)_{\omega}  \text{  .}  \label{eq:definition_9c}
  \end{align}
\end{subequations}

Often times it is useful to write $\mathbf{V}{\scriptstyle_{gr}}$ in terms of $\mathbf{V}{\scriptstyle_{ph}}$.  This can be done in the following way:

\begin{subequations}
  \begin{align}
    \frac{ \partial \omega }{ \partial k } & = \frac{ c }{ n } - \frac{ k c }{ n^{2} } \left(\frac{ \partial n }{ \partial k }\right)  \label{eq:definition_10a}  \\
    & = \frac{ c }{ n } \left[ 1 - \frac{ k }{ n } \left( \frac{ \partial \omega }{ \partial k } \cdot \frac{ \partial n }{ \partial \omega } \right) \right]  \label{eq:definition_10b}  \\
    \frac{ c }{ n } & = \left[ 1 + \frac{ \omega }{ n } \left( \frac{ \partial n }{ \partial \omega } \right) \right] \frac{ \partial \omega }{ \partial k }  \label{eq:definition_10c}  \\
    \frac{ \partial \omega }{ \partial k } & = \frac{{\displaystyle c }}{{\displaystyle n + \omega \frac{ \partial n }{ \partial \omega } }}  \label{eq:definition_10d}  \\
    \frac{ \partial \omega }{ \partial k } & = V{\scriptstyle_{gr}} = \frac{{\displaystyle V{\scriptstyle_{ph}} }}{{\displaystyle \frac{ c }{ V{\scriptstyle_{ph}} } + c \omega \frac{ \partial V{\scriptstyle_{ph}}^{-1} }{ \partial \omega } }}  \label{eq:definition_10e}  \\
    & = \frac{{\displaystyle c }}{{\displaystyle \frac{ c }{ V{\scriptstyle_{ph}} } \left( 1 - \frac{ \omega }{ V{\scriptstyle_{ph}} } \frac{ \partial V{\scriptstyle_{ph}} }{ \partial \omega } \right) }}  \label{eq:definition_10f}
  \end{align}
\end{subequations}

which gives us a simple relation between the group and phase speeds, given by:

\begin{equation}
  \label{eq:definition_11}
  \mathbf{V}{\scriptstyle_{gr}} = \frac{{\displaystyle V{\scriptstyle_{ph}} }}{{\displaystyle \left( 1 - \frac{ \omega }{ V{\scriptstyle_{ph}} } \frac{ \partial V{\scriptstyle_{ph}} }{ \partial \omega } \right) }}  \text{  .}
\end{equation}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Nonlinear Optics
%%----------------------------------------------------------------------------------------
\subsection{Nonlinear Optics} \label{subapp:nonlinoptics}

\indent  To be more general, let us assume that a nonlinear phase, $\vartheta\left(\mathbf{x},t\right)$, contains separable terms where one part represents the phase of the wave if the medium was uniform, stationary, and linear ($\mathbf{k}{\scriptstyle_{o}}$, $\omega{\scriptstyle_{o}}$) and a second part that allows for nonlinear terms ($\boldsymbol{\varkappa}$, $\varpi$).  Under these conditions, we can see that $\omega{\scriptstyle_{o}}$ $=$ $\omega{\scriptstyle_{o}} \left( \mathbf{k}{\scriptstyle_{o}}, 0 \right)$.  Therefore, we can say:

\begin{equation}
  \label{eq:nonlinoptics_0a}
  \vartheta\left( \mathbf{x}, t \right) = \left[ \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} - \omega{\scriptstyle_{o}} \thickspace t \right] + \varphi\left( \mathbf{x}, t \right)
\end{equation}

\noindent  where we now use $\varphi\left(\mathbf{x},t\right)$ to describe the nonlinear part of the wave phase \citep[Chapter 15 of][]{sagdeev88a}.  From this, we can see that the total wave number, $\boldsymbol{\kappa}$, and frequency, $\omega$, are given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \nabla \vartheta\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1a}  \\
    & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) - \nabla \left( \omega{\scriptstyle_{o}} \thickspace t \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1b}  \\
    & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1c}  \\
    \intertext{and}
    \omega & = - \partial{\scriptstyle_{t}} \thickspace \vartheta\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_1d}  \\
    & = - \partial{\scriptstyle_{t}} \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) - \partial{\scriptstyle_{t}} \left( - \omega{\scriptstyle_{o}} \thickspace t \right) - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right) \label{eq:nonlinoptics_1e}  \\
    & = \partial{\scriptstyle_{t}} \left( \omega{\scriptstyle_{o}} \thickspace t \right) - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_1f}
  \end{align}
\end{subequations}

\noindent  If we assume that $\partial{\scriptstyle_{t}}$ and $\nabla$ acting on either $\mathbf{k}{\scriptstyle_{o}}$ or $\omega{\scriptstyle_{o}}$ $\rightarrow$ 0, then we have:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_2a}  \\
    \intertext{and}
    \omega & = \omega{\scriptstyle_{o}} - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_2b}
  \end{align}
\end{subequations}

\noindent  To proceed further, we need to recall some rules for vector calculus.  These rules are:

\begin{subequations}
  \begin{align}
    \nabla \left( \mathbf{A} \cdot \mathbf{B} \right) & = \mathbf{A} \times \left( \nabla \times \mathbf{B} \right) + \mathbf{B} \times \left( \nabla \times \mathbf{A} \right) + \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B} + \left( \mathbf{B} \cdot \nabla \right) \thickspace \mathbf{A}  \label{eq:nonlinoptics_3a}  \\
    \nabla \cdot \left( \mathbf{A} \thickspace \mathbf{B} \right) & = \left( \nabla \cdot \mathbf{A} \right) \thickspace \mathbf{B} + \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3b}  \\
    \mathbf{A} \times \left( \nabla \times \mathbf{B} \right) & = \left( \nabla \thickspace \mathbf{B} \right) \cdot \mathbf{A} - \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3c}  \\
    \nabla \times \left( \mathbf{A} \times \mathbf{B} \right) & = \mathbf{A} \thickspace \left( \nabla \cdot \mathbf{B} \right) - \mathbf{B} \thickspace \left( \nabla \cdot \mathbf{A} \right) + \left( \mathbf{B} \cdot \nabla \right) \thickspace \mathbf{A} - \left( \mathbf{A} \cdot \nabla \right) \thickspace \mathbf{B}  \label{eq:nonlinoptics_3d}  \\
    \intertext{and}
    \nabla \times \mathbf{x} & = 0  \label{eq:nonlinoptics_3e}  \\
    \nabla \cdot \mathbf{x} & = 3  \label{eq:nonlinoptics_3f}  \\
    \nabla \thickspace \mathbf{x} & = \overleftrightarrow{\mathbb{I}}  \label{eq:nonlinoptics_3g}
  \end{align}
\end{subequations}

\noindent  where $\overleftrightarrow{\mathbb{I}}$ is the unit dyad\footnote{often this is called the unit tensor or unit matrix or identity matrix because it satisfies the multiplication identity for rank two tensors}.  Using these relationships, we can show:

\begin{subequations}
  \begin{align}
    \nabla \left( \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x} \right) & = \mathbf{k}{\scriptstyle_{o}} \times \left( \nabla \times \mathbf{x} \right) + \mathbf{x} \times \left( \nabla \times \mathbf{k}{\scriptstyle_{o}} \right) + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} + \left( \mathbf{x} \cdot \nabla \right) \thickspace \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_4a}  \\
    & = 0 + \left\{ \left( \nabla \thickspace \mathbf{k}{\scriptstyle_{o}} \right) \cdot \mathbf{x} - \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} \right\} + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} + \left( \mathbf{x} \cdot \nabla \right) \thickspace \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_4b}  \\
    & = \left( \nabla \thickspace \mathbf{k}{\scriptstyle_{o}} \right) \cdot \mathbf{x} + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x}  \label{eq:nonlinoptics_4c}  \\
    & = 0 + \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x}  \text{   .}  \label{eq:nonlinoptics_4d}
  \end{align}
\end{subequations}

\noindent  The final term can be reduced even further by noticing:

\begin{subequations}
  \begin{align}
    \left( \mathbf{k}{\scriptstyle_{o}} \cdot \nabla \right) \thickspace \mathbf{x} & = \left( \mathbf{k}{\scriptstyle_{ox}} \thickspace \partial{\scriptstyle_{x}} + \mathbf{k}{\scriptstyle_{oy}} \thickspace \partial{\scriptstyle_{y}} + \mathbf{k}{\scriptstyle_{oz}} \thickspace \partial{\scriptstyle_{z}} \right) \thickspace \left\{ x \thickspace \textbf{i} + y \thickspace \textbf{j} + z \thickspace \mathbf{k} \right\}  \label{eq:nonlinoptics_5a}  \\
    & = \left( \mathbf{k}{\scriptstyle_{ox}} \thickspace \partial{\scriptstyle_{x}} \thickspace x \right) \textbf{i} + \left( \mathbf{k}{\scriptstyle_{oy}} \thickspace \partial{\scriptstyle_{y}} \thickspace y \right) \textbf{j} + \left( \mathbf{k}{\scriptstyle_{oz}} \thickspace \partial{\scriptstyle_{z}} \thickspace z \right) \mathbf{k}  \label{eq:nonlinoptics_5b}  \\
    & = \mathbf{k}{\scriptstyle_{o}}  \label{eq:nonlinoptics_5c}
  \end{align}
\end{subequations}

\noindent  which means that the final forms for $\boldsymbol{\kappa}$ and $\omega$ are given by:

\begin{subequations}
  \begin{align}
    \boldsymbol{\kappa} & = \mathbf{k}{\scriptstyle_{o}} + \nabla \varphi\left( \mathbf{x}, t \right)  \label{eq:nonlinoptics_6a}  \\
    \omega & = \omega{\scriptstyle_{o}} - \partial{\scriptstyle_{t}} \thickspace \varphi\left( \mathbf{x}, t \right)  \text{   .} \label{eq:nonlinoptics_6b}
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
%%    \left\{  \right\}
  \end{align}
\end{subequations}

\clearpage
%%----------------------------------------------------------------------------------------
%%  Subsection:  Doppler Effect
%%----------------------------------------------------------------------------------------
\subsection{Doppler Effect}  \label{subapp:doppler}

\noindent  If we assume we are at rest with respect to a fluid moving at a velocity of $\mathbf{V}{\scriptstyle_{sw}}$, then the frequency of a signal convecting with the fluid would be given by:

\begin{equation}
  \label{eq:doppler_0}
  \omega{\scriptstyle_{obs}} = \gamma \left( \omega{\scriptstyle_{o}} + \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{V}{\scriptstyle_{sw}} \right)
\end{equation}

\noindent  where $\omega{\scriptstyle_{obs}}$ is the frequency we observe, $\omega{\scriptstyle_{o}}$ is the actual frequency of the source, $\gamma$ is the relativistic factor, and $\mathbf{k}{\scriptstyle_{o}}$ is the wave vector of the source.  The relationship in Equation \ref{eq:doppler_0} holds because the phase of any signal, $\phi$, is a Lorentz invariant \citep[see page 529 of][]{jackson98a}.  This means:

\begin{equation}
  \label{eq:doppler_1}
  \phi = \omega{\scriptstyle_{obs}} t{\scriptstyle_{obs}} - \mathbf{k}{\scriptstyle_{obs}} \cdot \mathbf{x}{\scriptstyle_{obs}} = \omega{\scriptstyle_{o}} t{\scriptstyle_{o}} - \mathbf{k}{\scriptstyle_{o}} \cdot \mathbf{x}{\scriptstyle_{o}}
\end{equation}

\noindent  where t${\scriptstyle_{j}}$ is the time in the $j$-frame and $\mathbf{x}{\scriptstyle_{j}}$ the position in the $j$-frame.  The wave number is given by:

\begin{equation}
  \label{eq:doppler_2}
  k{\scriptstyle_{j}}\left(\omega{\scriptstyle_{j}}\right) = \frac{ \omega{\scriptstyle_{j}} }{ c } n\left(\omega{\scriptstyle_{j}}\right)
\end{equation}

\noindent  where n$\left(\omega{\scriptstyle_{j}}\right)$ is the wave index of refraction in the $j$-frame.  Thus, we can show for stationary phase ($\partial \thickspace \phi$/$\partial \thickspace \omega$ $=$ 0):

\begin{equation}
  \label{eq:doppler_3}
  c \frac{ d \thickspace k{\scriptstyle_{j}} }{ d \thickspace \omega{\scriptstyle_{j}} } = n\left(\omega{\scriptstyle_{j}}\right) + \omega{\scriptstyle_{j}} \frac{ d \thickspace n\left(\omega{\scriptstyle_{j}}\right) }{ d \thickspace \omega{\scriptstyle_{j}} }  \text{  .}
\end{equation}

\noindent  In general the frequency and wave number are a four vector with the following Lorentz transformations:

\begin{subequations}
  \begin{align}
    \frac{ \omega' }{ c } & = \gamma \left( \frac{ \omega }{ c } - \frac{ \mathbf{v} }{ c } \cdot \mathbf{k} \right)  \label{eq:doppler_4a}  \\
    k{\scriptstyle_{\parallel}}' & = \gamma \left( \frac{ \mathbf{v} \cdot \mathbf{k} }{ \mid \mathbf{v} \mid } - \frac{ v }{ c } \frac{ \omega }{ c } \right)  \label{eq:doppler_4b}  \\
    \mathbf{k}{\scriptstyle_{\perp}}' & = \mathbf{k}{\scriptstyle_{\perp}}  \text{  .}  \label{eq:doppler_4c}
  \end{align}
\end{subequations}

\noindent  For an electromagnetic wave with the angle between $\mathbf{k}$ and $\mathbf{v}$ defined as $\theta$, we have:

\begin{equation}
  \label{eq:doppler_4}
  \tan{\theta'} = \frac{{\displaystyle \sin{\theta} }}{{\displaystyle \gamma \left( \cos{\theta} - \frac{ v }{ c } \right) }}
\end{equation}

\noindent  which shows that there exists a Doppler shift even when $\mathbf{k}$ is orthogonal to $\mathbf{v}$ (i.e. $\theta$ $\rightarrow$ $\pi$/2) \citep[see page 530 of][]{jackson98a}.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Fluid Moment Definitions
%%----------------------------------------------------------------------------------------
\section{Fluid Moment Definitions} \label{app:moments}

\indent  Let us assume we have a function, f${\scriptstyle_{s}}$(\textbf{x},\textbf{v},t), which defines the number of particles of species $s$ in the following way:

\begin{equation}
  \label{eq:app3_0a}
  dN = f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right) \thickspace d^{3}x \thickspace d^{3}v
\end{equation}

\noindent  which tells us that f${\scriptstyle_{s}}$(\textbf{x},\textbf{v},t) is the particle distribution function of species $s$ that defines a probability density in phase space.  We can define moments of the distribution function as expectation values of any dynamical function, g(\textbf{x},\textbf{v}), as:

\begin{equation}
  \label{eq:app3_0b}
  \left< g\left( \textbf{x}, \textbf{v} \right) \right> = \frac{ 1 }{ N } \int d^{3}x \thickspace d^{3}v \thickspace g\left( \textbf{x}, \textbf{v} \right) \thickspace f\left( \textbf{x}, \textbf{v}, t \right)
\end{equation}

\noindent  where $\langle$ $\rangle$ is the average, which can mean ensemble average, arithmetic mean, etc.

\indent  If we define a set of fluid moments with similar format to that of Equations \ref{eq:mean_0c} and \ref{eq:mean_0d}, then we have:

\begin{subequations}
  \begin{align}
    \text{number density:   } n{\scriptstyle_{s}} & = \int d^{3}v \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_0}  \\
    \text{average velocity:   } \textbf{U}{\scriptstyle_{s}} & = \frac{ 1 }{ n{\scriptstyle_{s}} } \int d^{3}v \thickspace \textbf{v}\thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_1}  \\
    \text{kinetic energy density:   } W{\scriptstyle_{s}} & = \frac{ m{\scriptstyle_{s}} }{ 2 } \int d^{3}v \thickspace v^{2} \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_2}  \\
    \text{pressure tensor:   } \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} & = m{\scriptstyle_{s}} \int d^{3}v \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right) \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right) \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_3}  \\
    \text{heat flux tensor:   } \left(\overleftrightarrow{\mathbb{Q}}{\scriptstyle_{s}}\right){\scriptstyle_{i,j,k}} & = m{\scriptstyle_{s}} \int d^{3}v \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{i}} \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{j}} \left( \textbf{v} - \textbf{U}{\scriptstyle_{s}} \right){\scriptstyle_{k}} \thickspace f{\scriptstyle_{s}}\left( \textbf{x}, \textbf{v}, t \right)  \label{eq:app3_3b}
  \end{align}
\end{subequations}

\noindent  where the pressure tensor can be written as:

\begin{equation}
  \label{eq:app3_4}
  \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} = 
  \begin{bmatrix}
     P{\scriptstyle_{xx}} & P{\scriptstyle_{xy}} & P{\scriptstyle_{xz}}  \\
     P{\scriptstyle_{yx}} & P{\scriptstyle_{yy}} & P{\scriptstyle_{yz}}  \\
     P{\scriptstyle_{zx}} & P{\scriptstyle_{zy}} & P{\scriptstyle_{zz}}
  \end{bmatrix}
\end{equation}

\noindent  which can be reduced to a symmetric tensor (consequence of covariance symmetry, see Section \ref{subsubsec:covariance}) with the only off-diagonal elements being P${\scriptstyle_{xy}}$ $=$ P${\scriptstyle_{yx}}$, P${\scriptstyle_{xz}}$ $=$ P${\scriptstyle_{zx}}$, and P${\scriptstyle_{yz}}$ $=$ P${\scriptstyle_{zy}}$.  In a magnetized plasma, the magnetic field direction can often organize the collective particle motion so that the pressure tensor is reduced to a diagonal tensor.  In general, one can separate the pressure tensor into a diagonal part and an off-diagonal part\footnote{which is usually called the stress tensor}.  The general diagonal elements of the pressure tensor are:

\begin{equation}
  \label{eq:app3_5}
  \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} = 
  \begin{bmatrix}
     P{\scriptstyle_{\perp, 1}} &          0                 & 0  \\
              0                 & P{\scriptstyle_{\perp, 2}} & 0  \\
              0                 &          0                 & P{\scriptstyle_{\parallel}}
  \end{bmatrix}
\end{equation}

\noindent  where a gyrotropic assumption will result in P${\scriptstyle_{\perp, 1}}$ $=$ P${\scriptstyle_{\perp, 2}}$.  Thus, a gyrotropic plasma will have:

\begin{subequations}
  \begin{align}
    P{\scriptstyle_{\perp, s}}     & = n{\scriptstyle_{s}} k{\scriptstyle_{B}} T{\scriptstyle_{\perp, s}}  \label{eq:app3_6a}  \\
    P{\scriptstyle_{\parallel, s}} & = n{\scriptstyle_{s}} k{\scriptstyle_{B}} T{\scriptstyle_{\parallel, s}}  \label{eq:app3_6b}
  \end{align}
\end{subequations}

\noindent  and a non-gyrotropic plasma will have:

\begin{subequations}
  \begin{align}
    T{\scriptstyle_{\perp, s}} & = \frac{ 1 }{ 2 n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \left( P{\scriptstyle_{\perp, 1, s}} + P{\scriptstyle_{\perp, 2, s}} \right)  \label{eq:app3_7a}  \\
    T{\scriptstyle_{\parallel, s}} & = \frac{ 1 }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } P{\scriptstyle_{\parallel, s}} \text{  .}  \label{eq:app3_7b}
  \end{align}
\end{subequations}

\noindent  Therefore, if we have the following relationships:

\begin{subequations}
  \begin{align}
    \text{gyrotropic:   } V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 1 }{ 2 } \left( V{\scriptstyle_{T_{s}, \perp}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_8a}  \\
    \text{non-gyrotropic:   } V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 2 }{ 3 m{\scriptstyle_{s}} } Tr\left[ \frac{ \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \right] }  \label{eq:app3_8b}  \\
    & = \sqrt{ \frac{ 2 k{\scriptstyle_{B}} \left< T{\scriptstyle_{s}} \right> }{ m{\scriptstyle_{s}} }  }  \label{eq:app3_8c}
  \end{align}
\end{subequations}

\noindent  where we have used Tr[] as the trace and defined:

\begin{equation}
  \label{eq:app3_9}
  \left< T{\scriptstyle_{s}} \right> = \frac{ 1 }{ 3 } Tr\left[ \frac{ \overleftrightarrow{\mathbb{P}}{\scriptstyle_{s}} }{ n{\scriptstyle_{s}} k{\scriptstyle_{B}} } \right] \text{  .}
\end{equation}

\noindent  The average temperature of particle species $s$ shown in Equation \ref{eq:app3_9} is the one most often used when calculating temperatures from electrostatic plasma analyzers \citep[e.g.,][]{curtis89a}.  The temperature is physically a measure of the average kinetic energy density of particle species $s$, and can be represented as:

\begin{subequations}
  \begin{align}
    T{\scriptstyle_{\perp, s}} & = \frac{ 1 }{ 2 } \left(  T{\scriptstyle_{\perp, 1, s}} + T{\scriptstyle_{\perp, 2, s}} \right)  \label{eq:app3_10a}  \\
    \left< T{\scriptstyle_{s}} \right> & = \frac{ 1 }{ 3 } \left(  T{\scriptstyle_{\perp, 1, s}} + T{\scriptstyle_{\perp, 2, s}} + T{\scriptstyle_{\parallel, s}} \right)  \label{eq:app3_10b}
  \end{align}
\end{subequations}

\noindent  therefore, if we already have V${\scriptstyle_{T_{s}, \perp}}$ and V${\scriptstyle_{T_{s}, \parallel}}$ and we assume T${\scriptstyle_{\perp, 1}}$ $\neq$ T${\scriptstyle_{\perp, 2}}$ (i.e., non-gyrotropic)\footnote{In most cases, it is assumed that the electrons are gyrotropic and ions are non-gyrotropic.  Physically, this is due to the relatively long sample period ($\geq$3 s) of current particle detectors compared to $\Omega{\scriptstyle_{ce}}^{-1}$ for electrons, which causes the resulting measured distribution to appeared \textit{smeared out} in phase space.  Most non-gyrotropic features are lost due to the relatively long sample periods.  For ions, however, $\Omega{\scriptstyle_{cp}}^{-1}$ can be $\sim$1-10 s (for B${\scriptstyle_{o}}$ $\sim$ 1-10 nT).  Therefore, non-gyrotropic features (e.g., gyrophase bunching) can often be observed in ion distributions.}, then we have:

\begin{subequations}
  \begin{align}
    V{\scriptstyle_{T_{s}}} & = \sqrt{ \frac{ 1 }{ 3 } \left( V{\scriptstyle_{T_{s}, \perp, 1}}^{2} + V{\scriptstyle_{T_{s}, \perp, 2}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_11a}  \\
    & = \sqrt{ \frac{ 2 V{\scriptstyle_{T_{s}, \perp}}^{2} }{ 3 } + \frac{ V{\scriptstyle_{T_{s}, \parallel}}^{2} }{ 3 } }  \label{eq:app3_11b}  \\
    & \neq \sqrt{ \frac{ 1 }{ 2 } \left( V{\scriptstyle_{T_{s}, \perp}}^{2} + V{\scriptstyle_{T_{s}, \parallel}}^{2} \right) }  \label{eq:app3_11c}
  \end{align}
\end{subequations}

%%  Heat Flux (3rd moment)
\indent  The heat flux tensor (or kinetic energy flux in the bulk flow reference frame), in its general form, is a 3$\times$3$\times$3-element array, which, without symmetries, would have 27 distinct elements.  However, due to symmetries imposed by math\footnote{similar covariance rules to those used to make the pressure tensor symmetric}, assumptions, and physical aspects of fluids, we can reduce this tensor to only its symmetric components (10 total).  The 10 variations of $Q{\scriptstyle_{l,m,n}}$ are:  $Q{\scriptstyle_{x,x,x}}$, $Q{\scriptstyle_{x,y,y}}$, $Q{\scriptstyle_{x,z,z}}$, $Q{\scriptstyle_{x,x,y}}$, $Q{\scriptstyle_{x,x,z}}$, $Q{\scriptstyle_{x,y,z}}$, $Q{\scriptstyle_{y,y,z}}$, $Q{\scriptstyle_{y,z,z}}$, $Q{\scriptstyle_{y,y,y}}$, and $Q{\scriptstyle_{z,z,z}}$.  The result is a simple rank-2 tensor or 3$\times$3 matrix where the sum of the i$^{th}$ row results in the i$^{th}$ component of the resultant \emph{heat flux vector}.  This is how one typically defines a heat flux in practical applications e.g., the solar wind electron heat flux, given by:

\begin{equation}
  \label{eq:hflux_3}
  \vec{\textbf{q}} = \frac{ m{\scriptstyle_{e}} }{ 2 } \int d^{3}v \thickspace f{\scriptstyle_{e}}(\vec{\textbf{x}},\vec{\textbf{v}},t) \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{i}} \right) \thickspace \left( \textbf{v} - \textbf{U}{\scriptstyle_{i}} \right)^{2}
\end{equation}

\noindent  where m${\scriptstyle_{e}}$ is the electron mass, \textbf{U}${\scriptstyle_{i}}$ the bulk flow velocity, and f${\scriptstyle_{e}}$($\vec{\textbf{v}}$,$\vec{\textbf{x}}$,t) represents a general form of the electron velocity distribution function.

\clearpage
%%----------------------------------------------------------------------------------------
%%  Appendix:  Conservation Relations
%%----------------------------------------------------------------------------------------
\section{Conservation Relations} \label{app:rhequations}

\indent  In the case of a planar shock, we can define the conservation relations called the Rankine-Hugoniot relations across the shock ramp.  If we define $\Delta [X]$ $=$ $\langle X \rangle{\scriptstyle_{dn}}$ - $\langle X \rangle{\scriptstyle_{up}}$, where the subscript up(dn) corresponds to upstream(downstream).  Then we have from \citet{vinas86a, koval08a}:

\begin{subequations}
  \begin{align}
    \Delta \left[ G{\scriptstyle_{n}} \right] \equiv & \Delta \left[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \right] = 0  \label{eq:rhequations_0a} \\
    \Delta \left[ B{\scriptstyle_{n}} \right] \equiv & \Delta \left[ \hat{\mathbf{n}} \cdot \mathbf{B} \right] = 0  \label{eq:rhequations_0b} \\
    \Delta \left[ \textbf{S}{\scriptstyle_{t}} \right] \equiv & \Delta \left[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \mathbf{V}{\scriptstyle_{t}} - \frac{ B{\scriptstyle_{n}} }{ \mu{\scriptstyle_{o}} } \mathbf{B}{\scriptstyle_{t}} \right] = 0  \label{eq:rhequations_0c} \\
    \Delta \left[ \textbf{S}{\scriptstyle_{t}} \right] \equiv & \Delta \left[ \left( \hat{\mathbf{n}} \times \mathbf{V}{\scriptstyle_{t}} \right) B{\scriptstyle_{n}} - \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \left( \hat{\mathbf{n}} \times \mathbf{B}{\scriptstyle_{t}} \right) \right] = 0  \label{eq:rhequations_0d} \\
    \Delta \left[ S{\scriptstyle_{n}} \right] \equiv & \Delta \left[ P + \frac{ \mathbf{B}{\scriptstyle_{t}} \cdot \mathbf{B}{\scriptstyle_{t}} }{ 2 \mu{\scriptstyle_{o}} } + \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right)^{2} \right] = 0  \label{eq:rhequations_0e} \\
    \begin{split}
      \Delta \left[ \varepsilon \right] \equiv & \Delta \Bigg[ \rho \left( V{\scriptstyle_{n}} - V{\scriptstyle_{shn}} \right) \left\{ \frac{1}{2} \left( \mathbf{V}{\scriptstyle_{sw}} - V{\scriptstyle_{shn}} \hat{\mathbf{n}} \right)^{2} + \frac{ \gamma }{ \gamma - 1 } \frac{ P }{ \rho } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ \rho \mu{\scriptstyle_{o}} } \right\} \\
      \qquad & - \frac{ B{\scriptstyle_{n}} \left( \mathbf{V}{\scriptstyle_{sw}} - V{\scriptstyle_{shn}} \hat{\mathbf{n}} \right) \cdot \mathbf{B} }{ \mu{\scriptstyle_{o}} } \Bigg] = 0  \label{eq:rhequations_0f}
    \end{split}
  \end{align}
\end{subequations}

\noindent  where we have defined:

\begin{subequations}
  \begin{align}
    Q{\scriptstyle_{n}} & = \mathbf{Q} \cdot \hat{\mathbf{n}}  \label{eq:rhequations_1aa}  \\
    \mathbf{Q}{\scriptstyle_{t}} & = \left( \hat{\mathbf{n}} \times \mathbf{Q} \right) \times \hat{\mathbf{n}}  \label{eq:rhequations_1bb}  \\
    & = \mathbf{Q} \cdot \left( \mathbb{I} - \hat{\mathbf{n}} \hat{\mathbf{n}} \right)  \label{eq:rhequations_1cc}  \\
    V{\scriptstyle_{shn}} & = \frac{ \Delta \left[ \rho \mathbf{V}{\scriptstyle_{sw}} \right] }{ \Delta \left[ \rho \right] } \cdot \hat{\mathbf{n}}  \label{eq:rhequations_1dd}
  \end{align}
\end{subequations}

\noindent  and $\rho$ is the mass density, P is scalar total (ion plus electron) thermal pressure, and $\gamma$ is the ratio of specific heats or polytrope index.  We note that P $=$ $\hat{\mathbf{n}}$ $\cdot$ $\mathbb{P}$ $\cdot$ $\hat{\mathbf{n}}$ $=$ 1/3 Tr[$\mathbb{P}$] $\sim$ n${\scriptstyle_{o}}$ k${\scriptstyle_{B}}$ (T${\scriptstyle_{e}}$ $+$ T${\scriptstyle_{i}}$).

\noindent  The more general form of the above equations are as follows:

\begin{subequations}
  \begin{align}
    \intertext{Maxwell's equations:}
    & \nabla \cdot \mathbf{E} = \frac{ \rho{\scriptstyle_{c}} }{\varepsilon{\scriptstyle_{o}}}  \label{eq:genconseq_0a}  \\
    & \nabla \cdot \mathbf{B} = 0  \label{eq:genconseq_0b}  \\
    & \nabla \times \mathbf{E} + \frac{ \partial \mathbf{B} }{ \partial t } = 0  \label{eq:genconseq_0c}  \\
    & \nabla \times \mathbf{B} = \mu{\scriptstyle_{o}} \mathbf{j} + \mu{\scriptstyle_{o}} \varepsilon{\scriptstyle_{o}} \frac{ \partial \mathbf{E} }{ \partial t }  \label{eq:genconseq_0d} \\
    \intertext{mass flux continuity equation:}
    & \frac{ \partial \rho }{ \partial t } + \nabla \left( \rho \mathbf{V} \right) = 0  \label{eq:genconseq_0e} \\
    \intertext{charge flux continuity equation:}
    & \frac{ \partial \rho{\scriptstyle_{c}} }{ \partial t } = - \nabla  \cdot \left[ e \left( n{\scriptstyle_{i}} \mathbf{V}{\scriptstyle_{i}} - n{\scriptstyle_{e}} \mathbf{V}{\scriptstyle_{e}} \right) \right] = - \nabla  \cdot \mathbf{j} \label{eq:genconseq_0f} \\
    \intertext{momentum flux continuity equation:}
    & \frac{ \partial }{ \partial t } \left[ \rho \mathbf{V} + \frac{ 1 }{ c^{2} } \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) \right] + \nabla  \cdot \left[ \rho \mathbf{V} \mathbf{V} + \mathbb{P} + \left( \frac{ \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \mathbf{E} }{ 2 } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } \right) \mathbb{I} - \varepsilon{\scriptstyle_{o}} \mathbf{E} \mathbf{E} - \frac{  \mathbf{B} \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right] = 0 \label{eq:genconseq_0g} \\
    \intertext{energy flux continuity equation:}
    & \frac{ \partial }{ \partial t } \left[ \frac{ 1 }{ 2 } \rho \mathbf{V} \cdot \mathbf{V} + \frac{ 3 }{ 2 } P + \left( \frac{ \varepsilon{\scriptstyle_{o}} \mathbf{E} \cdot \mathbf{E} }{ 2 } + \frac{ \mathbf{B} \cdot \mathbf{B} }{ 2 \mu{\scriptstyle_{o}} } \right) \right] + \nabla  \cdot \left[ \left( \frac{ 1 }{ 2 } \rho \mathbf{V} \cdot \mathbf{V} + \frac{ 3 }{ 2 } P \right) \mathbf{V} + \mathbb{P} \cdot \mathbf{V} + \mathbf{q} + \left( \frac{ \mathbf{E} \times \mathbf{B} }{ \mu{\scriptstyle_{o}} } \right) \right] = 0 \label{eq:genconseq_0h} \\
    \intertext{and generalized Ohm's law:}
    & \frac{ \partial \mathbf{j} }{ \partial t } + \nabla  \cdot \sum_{\alpha} \left( q{\scriptstyle_{\alpha}} n{\scriptstyle_{\alpha}} \mathbf{V}{\scriptstyle_{\alpha}} \mathbf{V}{\scriptstyle_{\alpha}} + \frac{ q{\scriptstyle_{\alpha}} }{ m{\scriptstyle_{\alpha}} } \mathbb{P}{\scriptstyle_{\alpha}} \right) = \sum_{\alpha} \frac{ q{\scriptstyle_{\alpha}}^{2} n{\scriptstyle_{\alpha}} }{ m{\scriptstyle_{\alpha}} } \left( \mathbf{E} + \mathbf{V}{\scriptstyle_{\alpha}} \times \mathbf{B} \right) \label{eq:genconseq_0i}
%%    \frac{  }{  }
%%    \left\langle  \right\rangle
%%    \left[  \right]
%%    \left(  \right)
  \end{align}
\end{subequations}

\clearpage
\setcounter{secnumdepth}{-1}  %%  --> Shut off section counters
%%----------------------------------------------------------------------------------------
%%  Bibliography
%%----------------------------------------------------------------------------------------
\renewcommand{\bibsep}{0pt}  %%  Tighten spacing between references
\addtocontents{toc}{}         %%  Tighten spacing in Table of Contents
\phantomsection               %%  Fix reference link
\section{\emph{References}} \label{sec:References}
\renewcommand{\refname}{}     %%  Shut off default section title for bibliography
\bibliography{/Users/lbwilson/Desktop/Lynn_B_Wilson_III/LaTeX/Bibliographies/my_bib_maker}

%%########################################################################################
%%%%######################################################################################
%%%%%%####################################################################################
%%%%%%%%  End the document
%%%%%%####################################################################################
%%%%######################################################################################
%%########################################################################################
\end{document}